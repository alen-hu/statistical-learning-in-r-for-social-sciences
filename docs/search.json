[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Learning in R for Social Sciences",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "introduction_to_statistical_learning.html",
    "href": "introduction_to_statistical_learning.html",
    "title": "1  Introduction to Statistical Learning",
    "section": "",
    "text": "1.1 Statistical Learning Formula\nStatistical learning refers to a broad set of approaches and techniques for estimating the function that connects independent variables to an dependent variable. At its core, statistical learning is concerned with understanding the relationship between variables and using that understanding either to make predictions about future observations or to gain insight into how different factors influence an outcome.\nThe fundamental idea of statistical learning can be expressed through a simple formula:\n\\(Y = f(X) + \\epsilon\\)\nThis formula tells us that any outcome we wish to study or predict can be understood as the result of some systematic relationship between independent and dependent variables, plus some random variation that we cannot fully explain or control. The goal of statistical learning is to estimate the function \\(f\\) based on observed data, so that we can either predict Y for new observations or understand how changes in X are associated with changes in Y.\nLet’s now explain each component of this formula in detail.\nTo make these concepts concrete, let me illustrate them with the example. Consider a researcher studying income inequality and social mobility. The researcher might want to understand what determines a person’s income in adulthood. The dependent variable Y would be adult income, measured in monetary units. The predictors X might encompass the person’s own educational credentials, their occupation, the region where they live, their parents’ socioeconomic status, their race and gender, and the number of years of work experience they have accumulated. The function \\(f\\) would capture the systematic relationships between these characteristics and income, revealing how the labor market rewards different attributes and how social background continues to influence economic outcomes across generations. The error term \\(\\epsilon\\) would account for all the variation in income that these measured factors cannot explain. This residual variation might stem from unmeasured differences in job performance, luck in finding particularly good or bad employment matches, health shocks that affect earning capacity, or discrimination that varies in ways not captured by the measured variables.\nWe can write this relationship as: \\(Y\\ =\\ f(X_1,\\ X_2,\\ X_3,\\ X_4,\\ X_5,\\ X_6,\\ X_7)\\ +\\ ϵ\\)\nIn this formula, Y represents adult income measured in monetary units such as annual earnings in euros. This is the response we are trying to understand or predict. The predictors are defined as follows.\nThe function \\(f\\) captures the systematic relationship between all these predictors and adult income. This function describes how the labor market values different combinations of education, occupation, location, background, and demographic characteristics. The precise form of \\(f\\) is unknown to us and must be estimated from data. It might be relatively simple, such as a linear combination of the predictors, or it might be quite complex, involving interactions between variables and nonlinear relationships.\nThe error term \\(\\epsilon\\) represents all the variation in adult income that cannot be explained by the seven predictors we have included. This encompasses unmeasured factors such as individual differences in productivity, motivation, and interpersonal skills, as well as random events like fortunate or unfortunate timing in job searches, health events that affect earning capacity, and idiosyncratic experiences of discrimination or favoritism in the workplace, and so on.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Learning</span>"
    ]
  },
  {
    "objectID": "introduction_to_statistical_learning.html#statistical-learning-formula",
    "href": "introduction_to_statistical_learning.html#statistical-learning-formula",
    "title": "1  Introduction to Statistical Learning",
    "section": "",
    "text": "The dependent variable or response, denoted by Y, represents the response that we are trying to understand, explain, or predict. This is the variable whose variation we want to account for using other available information. It is called dependent precisely because its values depend on, or are influenced by, other variables in the system we are studying.\nThe independent variable or predictor, denoted by X, represents the input information that we use to explain or predict the outcome. In most realistic situations, we have multiple predictors rather than just one, so X typically represents a collection of variables written as \\(X = (X_1, X_2, ..., X_p)\\), where \\(X_p\\) indicates the total number of predictors. The key characteristic of predictors is that they provide information that helps us understand or anticipate the values of the dependent variable.\nThe function \\(f\\) represents the systematic relationship between the dependent variable and the indipendent variable. This function captures all the information that the independent variables collectively provide about the dependent variable. In other words, \\(f\\) describes the pattern or rule that connects predictors to response in a consistent, reproducible way. The crucial point is that in real-world applications, the true form of \\(f\\) is almost always unknown to us. We never directly observe this function; instead, we must estimate it based on the data we have collected. The entire enterprise of statistical learning revolves around developing methods to estimate \\(f\\) as accurately as possible.\nThe error term, denoted by \\(\\epsilon\\), represents the random component of the relationship between dependent and independent variables. This term captures all the variation in Y that cannot be explained by the \\(X_p\\). The error term is assumed to be independent of X and to have a mean of zero, which means that on average, the errors cancel out and do not systematically bias our predictions in one direction or another. The error term exists for several important reasons.\n\nFirst, there may be variables that influence dependent variable but that we have not measured or included in our analysis.\nSecond, even if we could measure every relevant variable, there might be inherent randomness or unpredictability in the phenomenon we are studying.\nThird, our measurements themselves may contain some degree of imprecision or noise.\n\n\n\n\n\n\n\\(X_1\\) represents the person’s educational credentials, which might be measured as years of schooling completed or as the highest degree obtained.\n\\(X_2\\) represents occupation, which could be coded as occupational prestige scores or as categorical indicators for different types of jobs.\n\\(X_3\\) represents the geographic region where the person lives and works, capturing spatial variation in labor markets and cost of living.\n\\(X_4\\) represents parents’ socioeconomic status, which might be measured through parental income, parental education, or a composite index combining multiple indicators of family background.\n\\(X_5\\) represents race, coded as categorical indicators for different racial or ethnic groups.\n\\(X_6\\) represents gender, typically coded as a binary or categorical variable.\n\\(X_7\\) represents years of work experience, measuring how long the person has been participating in the labor force.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Learning</span>"
    ]
  },
  {
    "objectID": "introduction_to_statistical_learning.html#relationship-between-dependent-and-independent-variable",
    "href": "introduction_to_statistical_learning.html#relationship-between-dependent-and-independent-variable",
    "title": "1  Introduction to Statistical Learning",
    "section": "1.2 Relationship between Dependent and Independent Variable",
    "text": "1.2 Relationship between Dependent and Independent Variable\nThe function \\(f\\) is the central object of interest in statistical learning. It represents the systematic relationship between the independent variable and the dependent variable, capturing all the information that the independent variables provide about the dependent variable. When we say that \\(Y = f(X) + \\epsilon\\), we are asserting that the response can be decomposed into two parts: a predictable component \\(f(X)\\) that depends on the values of the predictors, and an unpredictable component \\(\\epsilon\\) that represents random variation. The function \\(f\\) is what connects the world of responses to the world of predictors in a consistent, reproducible manner.\nUnderstanding the nature of \\(f\\) is crucial because it embodies the underlying pattern that governs how changes in the independent variable translate into changes in the dependent variable. If we knew \\(f\\) perfectly, we would understand exactly how each predictor influences the response, how predictors interact with one another, and what response to expect for any given combination of predictor values. However, in virtually all real-world applications, \\(f\\) is unknown. We never observe \\(f\\) directly; we only observe data points consisting of predictor values and corresponding responses. The entire purpose of statistical learning is to use these observed data points to construct an estimate of \\(f\\), which we denote as \\(\\hat{f}\\). The reasons we might want to estimate \\(f\\) fall into two broad categories: prediction and inference. These two goals are conceptually distinct, and they often lead us to prefer different types of statistical learning methods.\nPrediction is concerned with accurately anticipating the value of Y for new observations where we know the predictors X but do not yet know the response of the predictors. In prediction tasks, we treat \\(\\hat{f}\\) as a kind of black box. We do not necessarily care about the internal workings of our estimated function or about which specific predictors matter most. What we care about is whether our estimate \\(\\hat{f}\\) produces accurate predictions when applied to new data. The quality of predictions depends on two sources of error:\n\nThe first is reducible error, which arises because our estimate \\(\\hat{f}\\) is imperfect and does not exactly match the true \\(f\\). We can potentially reduce this error by using better statistical learning methods or by collecting more data.\nThe second is irreducible error, which corresponds to the variance of \\(\\epsilon\\). Even if we had a perfect estimate of \\(f\\), our predictions would still contain some error because Y is inherently influenced by random factors that cannot be predicted from X alone.\n\nInference, by contrast, is concerned with understanding the relationship between the predictors and the outcome. When our goal is inference, we cannot treat \\(\\hat{f}\\) as a black box because we need to know its exact form. We want to answer questions such as which predictors are associated with the response, what is the direction and magnitude of each predictor’s effect, and whether the relationships are linear or more complex. Inference requires that our estimate \\(\\hat{f}\\) be interpretable, meaning that we can examine it and draw substantive conclusions about how the world works.\nIn practice, many research projects involve elements of both prediction and inference. A researcher studying income might want to understand the determinants of earnings while also developing a model that can predict incomes for new individuals. However, there is often tension between these goals because the methods that produce the most accurate predictions are not always the most interpretable, and the most interpretable methods do not always produce the best predictions.\n\n1.2.1 Parametric vs non-parametric methods\nHaving established why we want to estimate \\(f\\), let us now turn to the question of how we estimate \\(f\\). Statistical learning methods for estimating \\(f\\) can be broadly divided into two categories: parametric methods and non-parametric methods. These two approaches differ fundamentally in the assumptions they make about the form of \\(f\\) and in the way they use data to construct an estimate.\nParametric methods proceed in two steps. In the first step, we make an assumption about the functional form of \\(f\\). That is, we specify in advance what kind of mathematical relationship we believe connects the predictors to the outcome. The most common assumption is that \\(f\\) is linear, meaning that we assume the relationship can be written as \\(f(X) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p\\). This linear model asserts that the response is a weighted sum of the predictors, where the weights \\(\\beta_1, \\beta_2, ..., \\beta_p\\) are unknown coefficients that quantify the contribution of each predictor, and \\(\\beta_0\\) is an intercept term representing the expected response when all predictors equal zero. By assuming a linear form, we have dramatically simplified the problem. Instead of having to estimate an arbitrary, potentially very complex function \\(f\\), we only need to estimate the intercept and the p coefficients. In the second step of the parametric approach, we use the observed data to fit or train the model. This means finding values of the parameters that make the model match the data as closely as possible. For the linear model, the most common fitting procedure is ordinary least squares, which chooses the parameter values that minimize the sum of squared differences between the observed responses and the responses predicted by the model. Once we have estimated the parameters, our estimate \\(\\hat{f}\\) is fully specified, and we can use it for prediction or inference.\nThe parametric approach has several important advantages. Because we have reduced the problem to estimating a fixed number of parameters, parametric methods are computationally efficient and can work well even with relatively small samples. Furthermore, the resulting models are typically easy to interpret. In a linear model, each coefficient tells us how much the expected responses changes when the corresponding predictor increases by one unit, holding all other predictors constant. This interpretability makes parametric models particularly valuable for inference. However, parametric methods also have a significant limitation. The assumption we make about the form of \\(f\\) may be wrong. If the true relationship between the predictors and the response is nonlinear or involves complex interactions, a linear model will fail to capture these features and will provide a poor approximation to \\(f\\). We can try to address this problem by using more flexible parametric models that include polynomial terms, interaction effects, or other elaborations of the basic linear form. But as we make our parametric model more flexible, we must estimate more parameters, which requires more data and increases the risk of a phenomenon called overfitting. Overfitting occurs when a model fits the training data very well but performs poorly on new data because it has captured random noise rather than genuine patterns. The model essentially memorizes the idiosyncrasies of the particular sample rather than learning the underlying relationship.\nNon-parametric methods take a fundamentally different approach. Instead of assuming a specific functional form for \\(f\\), non-parametric methods seek an estimate that gets close to the data points without imposing strong prior assumptions about the shape of the relationship. The idea is to let the data speak for themselves and to allow \\(\\hat{f}\\) to take whatever form best fits the observed patterns. One example of a non-parametric method is the thin-plate spline, which estimates \\(f\\) as a smooth surface that passes near the observed data points. The analyst does not specify in advance that \\(f\\) should be linear or quadratic or any other particular form. Instead, the method finds a smooth function that fits the data well, subject to some constraint on how wiggly or rough the function is allowed to be. Another example is the k-nearest neighbors method, which predicts the outcome for a new observation by averaging the outcomes of the k training observations that are most similar to it in terms of the predictor values.\nThe main advantage of non-parametric methods is their flexibility. Because they do not assume a particular form for \\(f\\), they can potentially capture a much wider range of relationships, including highly nonlinear patterns and complex interactions that would be missed by a simple parametric model. If the true \\(f\\) has an unusual or complicated shape, a non-parametric method has a better chance of approximating it accurately. However, non-parametric methods also have important disadvantages. Because they do not reduce the problem to estimating a small number of parameters, they typically require much larger samples to produce accurate estimates. The flexibility that allows non-parametric methods to fit complex patterns also makes them prone to overfitting, especially when sample sizes are limited. Furthermore, the estimates produced by non-parametric methods are often difficult to interpret. A non-parametric prediction does not come with coefficients that tell us how each predictor contributes to the outcome. This lack of interpretability makes non-parametric methods less useful for inference, even when they excel at prediction.\nThe choice between parametric and non-parametric methods involves a fundamental trade-off. Parametric methods impose structure on the problem, which makes estimation easier and results more interpretable, but at the cost of potentially misspecifying the true form of \\(f\\). Non-parametric methods avoid this misspecification risk by staying flexible, but they require more data and produce less interpretable results. In practice, the best choice depends on the goals of the analysis, the amount of data available, and how much prior knowledge we have about the likely form of the relationship.\nThis brings us to a closely related issue: the trade-off between prediction accuracy and model interpretability. In statistical learning, there is often an inverse relationship between how flexible a method is and how interpretable its results are. Methods that impose strong restrictions on the form of \\(f\\) tend to be highly interpretable but may not fit complex patterns very well. Methods that are highly flexible can capture intricate relationships but produce results that are difficult for humans to understand.\nAt one end of the spectrum, we have highly restrictive methods like linear regression and its close relatives. Linear regression assumes that \\(f\\) is a linear combination of the predictors, which is a very strong restriction. This restriction means that linear regression can only produce straight lines in one dimension, flat planes in two dimensions, and hyperplanes in higher dimensions. The advantage is that the results are extremely interpretable. Each coefficient has a clear meaning: it tells us the expected change in Y associated with a one-unit increase in the corresponding predictor, holding other predictors constant. We can examine the coefficients and immediately understand which predictors matter, how large their effects are, and in which direction they operate. For inference purposes, this interpretability is invaluable.\nMoving along the spectrum toward greater flexibility, we encounter methods like generalized additive models, which relax the linearity assumption by allowing each predictor to have a potentially nonlinear effect on the response, while still maintaining an additive structure. These models are more flexible than linear regression and can capture curved relationships, but they remain reasonably interpretable because we can plot and examine the estimated effect of each predictor separately. Further along the spectrum, we find decision trees, which partition the predictor space into regions and assign a predicted response to each region. Trees are moderately flexible and can capture interactions and nonlinearities, but they remain somewhat interpretable because we can visualize the tree structure and see which predictors are used to make splits and at what values. These methods can approximate extremely complex functions and often achieve superior predictive accuracy on difficult problems. However, their results are very hard to interpret.\nOne might think that we should always prefer the most flexible method available, reasoning that greater flexibility means better ability to capture the true \\(f\\). Surprisingly, this is not the case. More flexible methods are not always better, even when our sole goal is prediction. The reason is overfitting. A highly flexible method can fit the training data very closely, including the random noise in that particular sample. When we apply the model to new data, the noise patterns will be different, and the overfitted model will perform poorly. In many situations, a simpler, more restrictive model that does not fit the training data as closely will actually generalize better to new observations.\nThis phenomenon is especially important when sample sizes are limited. With a small sample, there is not enough information to reliably estimate a complex, flexible model, and the risk of overfitting is high. In such cases, imposing structure through a parametric model can actually improve predictive performance by preventing the model from chasing noise. As sample sizes grow larger, we can afford to use more flexible methods because there is enough information to distinguish genuine patterns from random variation.\n\n\n1.2.2 Supervised vs unsupervised learning\nTo complete the overview of the foundational concepts in statistical learning, we need to understand additional distinction between supervised and unsupervised learning that help us categorize different types of learning problems.\nSupervised learning refers to situations where for each observation in our dataset, we have both predictor measurements and a corresponding response measurement. The term supervised reflects the idea that the learning process is guided or supervised by the known dependent variables. We observe what actually happened for each case in our training data, and we use this information to learn the relationship between independent variables and dependent variable. All the methods we have discussed so far fall into the supervised learning category when applied to problems where outcomes are observed. The fundamental goal of supervised learning is to build a model that can predict the response for new observations based on their predictor values, or to understand how the predictors relate to the response. In social sciences research, most studies involve supervised learning because we typically have data on both the explanatory variables and the outcome of interest. For example, when we study the relationship between education and income, we observe both variables for the individuals in our sample, which allows us to estimate how education influences earnings.\nUnsupervised learning describes a fundamentally different situation where we observe predictor measurements for each observation but have no corresponding response variable. Without a dependent variable to predict or explain, we cannot fit a regression model or train a classifier. Instead, unsupervised learning seeks to discover patterns, structures, or groupings within the data itself. The most common unsupervised learning task is cluster analysis, which attempts to identify subgroups of observations that are similar to one another. For instance, a researcher might have survey data containing many variables about people’s attitudes, behaviors, and demographic characteristics, but no predefined categorization of people into types. Cluster analysis could reveal that the respondents naturally fall into distinct groups based on their patterns of responses, perhaps identifying clusters that correspond to different lifestyles, political orientations, or consumption patterns. The key feature of unsupervised learning is that there is no correct answer to supervise the learning process. We are not trying to predict a known outcome but rather to uncover hidden structure in the data. This makes unsupervised learning more exploratory and somewhat more subjective than supervised learning, since there is no objective criterion like prediction accuracy to evaluate whether we have found the right structure.\n\nTable 1.1 Summary table of statistical learning methods\n\n\n\n\n\n\n\n\n\nMethod\nUnsupervised / Supervised\nParametric / Non-parametric\nFlexibility\nInterpretability\n\n\n\n\nLinear Regression\nSupervised\nParametric\nLow\nHigh\n\n\nRidge Regression\nSupervised\nParametric\nLow\nHigh\n\n\nLasso\nSupervised\nParametric\nLow\nHigh\n\n\nLogistic Regression\nSupervised\nParametric\nLow\nHigh\n\n\nGeneralized Additive Models\nSupervised\nParametric (additive structure)\nMedium\nMedium-High\n\n\nDecision Trees\nSupervised\nNon-parametric\nMedium\nMedium\n\n\nBagging\nSupervised\nNon-parametric\nHigh\nLow\n\n\nRandom Forests\nSupervised\nNon-parametric\nHigh\nLow\n\n\nBoosting\nSupervised\nNon-parametric\nHigh\nLow\n\n\nLinear Support Vector Machines\nSupervised\nParametric\nLow-Medium\nMedium\n\n\nNonlinear Support Vector Machines\nSupervised\nNon-parametric\nHigh\nLow\n\n\nK-Nearest Neighbors\nSupervised\nNon-parametric\nHigh\nLow\n\n\nNeural Networks\nSupervised\nNon-parametric\nVery High\nVery Low\n\n\nDeep Learning\nSupervised\nNon-parametric\nVery High\nVery Low\n\n\nK-Means Clustering\nUnsupervised\nNon-parametric\nMedium\nMedium\n\n\nHierarchical Clustering\nUnsupervised\nNon-parametric\nMedium\nMedium-High\n\n\nPrincipal Component Analysis\nUnsupervised\nParametric\nLow\nMedium-High\n\n\nFactor Analysis\nUnsupervised\nParametric\nLow\nHigh",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Learning</span>"
    ]
  },
  {
    "objectID": "introduction_to_statistical_learning.html#assessing-model-accuracy",
    "href": "introduction_to_statistical_learning.html#assessing-model-accuracy",
    "title": "1  Introduction to Statistical Learning",
    "section": "1.3 Assessing Model Accuracy",
    "text": "1.3 Assessing Model Accuracy\nA fundamental question in statistical learning can be expressed as a: how do we know which method or model is the best one to use for a given dataset? This might seem like a simple question at first, but it is actually one of the most challenging aspects of statistical learning in practice.\nWhen we build a statistical learning model, we need a way to evaluate how well it actually works. In other words, we need to measure how close the model’s predictions are to the real values we observe in the data. This is what we mean by measuring the quality of fit. Without such a measure, we would have no principled way of comparing different models or deciding whether a particular approach is adequate for our research question.\n\n1.3.1 The regression setting\nIn the regression setting, where the response variable is quantitative, the most commonly used measure of fit is the mean squared error (MSE). The mean squared error is calculated by taking each observation in the dataset, computing the difference between the actual observed value and the value that the model predicts, squaring that difference, and then averaging all of these squared differences across every observation. Formally, MSE is expressed as:\n\\(MSE = \\frac{1}{n} \\times \\sum_{i=1}^n(y_i - \\hat{f}(x_i))^2\\)\nThe logic behind this measure is straightforward. If our model’s predictions are very close to the true observed values, the differences will be small, the squared differences will be even smaller, and the average of all those squared differences will be a small number. On the other hand, if the model produces predictions that are far from the actual values for at least some observations, the squared differences will be large, pulling the MSE upward. Squaring the differences serves two purposes: it ensures that positive and negative errors do not cancel each other out, and it penalizes larger errors more heavily than smaller ones.\nTo make this concrete, consider our example of predicting adult income. Suppose the researcher has collected data on a sample of one thousand individuals, recording each person’s educational credentials, occupation, region, parents’ socioeconomic status, race, gender, and years of work experience, along with their actual adult income. The researcher then estimates the function \\(f\\) using some statistical learning method - perhaps a linear regression model - to produce a predicted income \\(\\hat{f}(x_i)\\) for each person in the dataset. For one individual, the model might predict an annual income of 38,000 euros while the person actually earns 42,000 euros, yielding a difference of 4,000 euros. For another individual, the model might predict 55,000 euros while the person earns 53,000 euros, giving a difference of 2,000 euros. The mean squared error takes all of these individual discrepancies, squares each one, and averages them across the entire sample. The resulting number gives us a single summary of how well the model’s predictions match reality.\nThe MSE we just described is computed using the same data that were used to build the model. This is called the training MSE, because it measures how well the model fits the training data - the observations the model has already seen and learned from. At first glance, it might seem perfectly reasonable to use the training MSE to evaluate a model. After all, if a model fits the data well, that should mean it is a good model. However, this reasoning is flawed. In most practical situations, we do not actually care how well the model fits the data it was trained on. What we really care about is how well the model will perform on new data that it has never seen before. This new, unseen data is called test data, and the MSE computed on test data is called the test MSE.\nTo understand why this distinction matters so profoundly, let us return to our income inequality example. Suppose the researcher has built a model using data from a survey conducted in 2018, which includes information on one thousand individuals and their incomes. The model fits these one thousand observations well, producing a low training MSE. But the real purpose of the model is not to predict the incomes of these specific one thousand people whose incomes the researcher already knows. The real purpose is to predict incomes for new individuals - perhaps people surveyed in 2020, or individuals from a different but comparable population - based on their educational credentials, occupation, region, family background, race, gender, and work experience. The question that truly matters is whether the model will produce accurate predictions for these new cases, not whether it accurately reproduces the incomes of the people it was trained on.\nThis is the fundamental insight: the training MSE measures something that is not of primary interest, while the test MSE measures something that is. A model that performs beautifully on its training data might perform poorly on new data, and a model with a somewhat higher training MSE might actually generalize better to unseen observations.\nMany statistical learning methods are designed, either directly or indirectly, to minimize the training MSE. They adjust their estimates and coefficients specifically to fit the training observations as closely as possible. As a result, the training MSE can be driven very low - sometimes all the way to zero - but this does not mean that the model has learned the true underlying patterns in the data. Instead, the model may have started to learn the noise in the training data, the random fluctuations and idiosyncratic features that are specific to that particular sample and will not appear again in new data. In our income example, imagine that the researcher uses a highly flexible model that can adapt to very fine details in the data. This model might learn that in the specific 2018 sample, there was one individual from a particular small region who had low education but unusually high income, perhaps due to an inheritance or a lucky business venture. A very flexible model might adjust its predictions to accommodate this particular case, effectively learning the specific circumstances of this one person rather than the general relationship between education and income. When the model is then applied to new individuals, this kind of overly specific learning will not help and may actually hurt prediction accuracy, because the idiosyncratic patterns of the training data do not generalize to the broader population.\nThe chapter illustrates this problem using the concept of model flexibility. A model’s flexibility refers to how closely it can conform to the patterns in the training data. At one end of the spectrum, a simple linear regression is relatively inflexible - it fits a straight line (or a flat hyperplane in multiple dimensions) through the data. At the other end, highly flexible methods like smoothing splines or very complex nonlinear models can bend and curve to follow almost every individual data point. The key finding, which the chapter demonstrates through several examples, is that as model flexibility increases, the training MSE will always decrease - because a more flexible model can always conform more closely to the training data. However, the test MSE does not simply decrease along with the training MSE. Instead, the test MSE initially decreases as the model becomes flexible enough to capture the real underlying patterns, but at some point it reaches a minimum and then begins to increase again. This produces the characteristic U-shaped curve that appears throughout the book.\nIn the income inequality context, a linear regression model assumes that the relationship between each predictor and income is a straight line. This might miss important nonlinearities - for example, the return to education might increase sharply once a person obtains a university degree, rather than rising smoothly with each additional year of schooling. A somewhat more flexible model could capture this nonlinearity and would likely produce better predictions on new data, yielding a lower test MSE. However, if the researcher keeps increasing flexibility - allowing the model to capture finer and finer details of the training data - at some point the model starts picking up noise rather than signal. It might learn that in this particular sample, people with exactly fourteen years of education and exactly eight years of work experience who live in one specific region have unusually high incomes, when in reality this pattern is just a coincidence in the sample. At this point, the test MSE starts rising again, even as the training MSE continues to fall.\nThe phenomenon, where a model fits the training data too closely and as a result performs poorly on new data, is called overfitting. It occurs when a statistical learning method works too hard to find patterns in the training data and ends up capturing patterns that are caused by random chance rather than by genuine features of the underlying relationship. When overfitting occurs, the training MSE is very low but the test MSE is high, because the spurious patterns the model learned from the training data do not exist in the test data.\nTo understand overfitting in our example, think of it this way. The true relationship between the seven predictors and adult income has a certain level of complexity. Education, occupation, region, family background, race, gender, and work experience all influence income in systematic ways, but those systematic influences operate at a general level - they describe broad patterns that hold across many individuals. A good model captures these broad, stable patterns. An overfit model goes beyond these patterns and starts memorizing the specific incomes of specific individuals in the training sample, including all the random variation that makes each person’s income slightly different from what the general pattern would predict. Since this random variation is specific to the training sample and will not replicate in new data, the overfit model ends up making worse predictions when applied to new observations.\nRegardless of whether overfitting has occurred, the training MSE will almost always be smaller than the test MSE. This is simply because most methods are designed to minimize the training MSE, so they will naturally fit the training data better than any data they have not seen. Overfitting refers specifically to the situation where additional flexibility leads to a worse test MSE - that is, where a less flexible model would actually have produced better predictions on new data.\nIn practice, the researcher can usually compute the training MSE quite easily, since it only requires the data used to fit the model. However, estimating the test MSE is considerably more difficult because test data may not be available. If the researcher studying income inequality has only one dataset, there is no separate pool of unseen observations on which to evaluate the model. One important solution to this problem is cross-validation which provides a way to estimate the test MSE using only the training data by cleverly splitting the data into parts and alternating which part serves as the training set and which serves as the test set. This allows the researcher to approximate how well the model would perform on genuinely new data without actually needing a separate test dataset.\nSummarizing the above, evaluating a model’s quality requires looking beyond how well it fits the data it was trained on. The true measure of a model’s value is its ability to make accurate predictions for observations it has never encountered. This principle applies whether we are predicting any phenomenon in the social sciences. Understanding the distinction between training performance and test performance, and recognizing the danger of overfitting, are essential foundations for the study of statistical learning.\n\n1.3.1.1 The Bias-Variance Trade-Off\nIn the previous section, we established that when we evaluate a statistical learning model, what truly matters is the test MSE - how well the model predicts outcomes for new, previously unseen observations. We also observed that as model flexibility increases, the test MSE tends to follow a characteristic U-shape: it initially decreases, reaches a minimum at some optimal level of flexibility, and then begins to increase again. The bias-variance trade-off is the theoretical explanation for why this U-shape occurs. It is one of the most important concepts in all of statistical learning, and understanding it deeply will help us make better decisions about which models to use and how flexible those models should be.\nExpected test MSE at any given point can always be broken down into the sum of three distinct quantities:\n\nthe variance of the model’s prediction,\nthe squared bias of the model’s prediction, and\nthe variance of the irreducible error.\n\nThis decomposition is expressed formally as:\n\\[\nE(y_0 - \\hat{f}(x_0))^2 = Var(\\hat{f}(x_0)) + [Bias(\\hat{f}(x_0))]^2 + Var(\\epsilon)\n\\]\nThe term on the left side of this equation is the expected test MSE at a particular point \\(x_0\\)​. The word “expected” here has a specific meaning: it refers to the average test MSE we would obtain if we were to repeat the entire process of collecting training data and fitting the model many times over. Each time we collect a new training dataset and fit a model, we would get a slightly different estimate \\(\\hat{f}\\), and therefore a slightly different prediction error at \\(x_0\\)​. The expected test MSE is the average of all these prediction errors across all possible training datasets we might have drawn. This decomposition tells us something profound. It says that the prediction error at any point is not a single monolithic quantity but rather the sum of three fundamentally different sources of error. To build good models, we need to understand each of these three components and how they relate to each other.\n1. Understanding Variance\nThe first component is the variance of \\(\\hat{f}(x_0)\\). Variance, in this context, refers to how much the model’s prediction at the point \\(x_0\\)​ would change if we estimated the model using a different training dataset. Remember that the training data are a sample drawn from a larger population, and if we were to draw a different sample, we would get different data points and therefore a different estimated function \\(\\hat{f}\\)​. If a method has high variance, it means that small changes in the training data lead to large changes in the estimated function and therefore in the predictions the model produces. If a method has low variance, the predictions remain relatively stable regardless of which particular training dataset is used.\nTo understand this in the context of our income inequality example, imagine that the researcher conducts the same study multiple times, each time drawing a new random sample of one thousand individuals from the population. Each sample will contain slightly different people with slightly different combinations of education, occupation, region, family background, race, gender, work experience, and income. Now suppose the researcher fits the same type of model to each of these different samples. If the method has low variance, the predicted income for a person with, say, a university degree, a professional occupation, living in an urban area, from a middle-class family, who is a white male with ten years of work experience would be roughly similar regardless of which particular sample the model was trained on. The predictions would be stable and consistent across different training datasets. However, if the method has high variance, the predicted income for this same hypothetical person could change dramatically depending on which sample happened to be drawn. One sample might produce a prediction of 45,000 euros while another sample, drawn from the same population, might produce a prediction of 52,000 euros, and yet another might yield 39,000 euros. This instability in predictions is what we mean by high variance.\nThe crucial insight is that more flexible methods tend to have higher variance. The reason is intuitive. A highly flexible model can conform closely to the specific patterns in whatever training data it receives. This means it is highly sensitive to the particular observations in the training set. If one influential individual is replaced by another, the flexible model might change its predictions substantially because it was fitting so closely to each data point. In our income example, a very flexible model might learn intricate patterns specific to the particular one thousand people in the sample - perhaps noticing that in this specific dataset, people from a certain small region with a certain combination of education and experience earn unusually high incomes. If a different sample were drawn, this particular pattern would likely not reappear, and the model’s predictions would shift accordingly.\nIn contrast, a simple linear regression model has low variance because it is constrained to fit a straight-line relationship. Changing a few observations in the training data will only shift the line slightly. The predictions are stable because the model’s rigid structure prevents it from responding dramatically to the idiosyncrasies of any particular sample. Whether the researcher uses sample A or sample B, a linear model will produce roughly similar predictions, because it can only capture broad, linear trends that tend to be consistent across samples.\n2. Understanding Bias\nThe second component of the expected test MSE is the squared bias of \\(\\hat{f}(x_0)\\). Bias refers to the error that arises from approximating a real-world phenomenon, which may be very complex, with a simplified model. It measures the difference between the average prediction of our model (averaged over all possible training datasets) and the true value of the function \\(f\\) at the point \\(x_0\\)​. In other words, bias captures how far off our model is, on average, from the truth - not because of random fluctuations in the training data, but because the model itself is structurally incapable of capturing the true relationship.\nReturning to the income inequality example, suppose that the true relationship between the seven predictors and adult income is genuinely complex. Perhaps the return to education is nonlinear, with relatively modest income gains for each additional year of schooling at lower levels but a sharp jump when a person completes a university degree. Perhaps there are important interactions between predictors - for instance, the effect of work experience on income might differ substantially depending on occupation, with experience mattering a great deal in some professions and very little in others. Perhaps the relationship between parental socioeconomic status and adult income is mediated in complex ways by education and region, creating patterns that cannot be captured by a simple additive model.\nIf the researcher uses a simple linear regression to model this complex reality, the model assumes that each predictor has a constant, additive effect on income. It cannot capture the sharp jump at university degree completion, it cannot represent the interaction between experience and occupation, and it cannot model the complex mediating role of family background. No matter how much training data the researcher collects, the linear model will systematically miss these features of the true relationship. This systematic error is bias. The model is biased because its structure is too simple to represent the truth. Even if the researcher could average the predictions across infinitely many training samples, the average prediction of the linear model would still differ from the true \\(f\\) because the model is fundamentally incapable of representing the true relationship.\nMore flexible methods tend to have lower bias because they can adapt to a wider range of possible shapes for \\(f\\). A flexible model that allows for nonlinear effects and interactions would be better able to capture the true complexity of how education, occupation, family background, and other factors jointly determine income. If the model’s structure is rich enough, it can, in principle, approximate the true function \\(f\\) very closely, resulting in low or even negligible bias.\nThe reason this concept is called a trade-off is that bias and variance tend to move in opposite directions as model flexibility changes. Simple, inflexible methods like linear regression have high bias because they impose strong assumptions about the form of \\(f\\) that may not be accurate. But they have low variance because their rigid structure makes them resistant to fluctuations in the training data. Flexible methods, on the other hand, have low bias because they can adapt to complex patterns in the data, but they have high variance because they are sensitive to the specific observations in the training set.\nThe expected test MSE is the sum of variance, squared bias, and the irreducible error. The irreducible error, \\(Var(\\epsilon)\\), is a constant that does not depend on the model at all - it represents the inherent unpredictability in the response that no model, no matter how sophisticated, can eliminate. In our income example, the irreducible error encompasses all the factors that influence income but are not captured by the seven predictors: individual personality traits, chance events, unmeasured forms of discrimination, health shocks, and countless other sources of variation. Since the irreducible error is fixed, the test MSE can only be reduced by managing the other two components: variance and bias.\nAs we increase flexibility, bias decreases. The model becomes better able to capture the true patterns in the data, and the systematic error introduced by an overly simplistic model structure diminishes. At the same time, variance increases. The model becomes more sensitive to the particular training data, and its predictions become less stable across different samples. The test MSE depends on the combined effect of these two opposing forces.\nAt low levels of flexibility, the model has high bias and low variance. The bias dominates the test MSE, and increasing flexibility helps because the reduction in bias is larger than the increase in variance. This is why the test MSE initially decreases as flexibility grows. In our income example, moving from a very rigid model - say, one that predicts the same average income for everyone regardless of their characteristics - to a basic linear regression would substantially reduce bias by allowing the model to capture at least the broad linear relationships between predictors and income. The test MSE would drop considerably because the improvement from lower bias far outweighs the modest increase in variance.\nAt moderate levels of flexibility, the model has found a good balance. It is flexible enough to capture the important patterns in the data but not so flexible that it is chasing noise. This is the point where the test MSE reaches its minimum, representing the best achievable predictive performance for the given set of predictors and the given type of model.\nAt high levels of flexibility, something changes. The bias has already been reduced to a low level, so further increases in flexibility yield only marginal improvements in bias. But variance continues to climb as the model becomes increasingly sensitive to the training data. Now the increase in variance outweighs the decrease in bias, and the test MSE begins to rise. The model is overfitting - it has become so flexible that it is learning noise rather than signal, and its predictions on new data suffer as a result.\nThis is why the test MSE exhibits the U-shape we discussed earlier. The left side of the U corresponds to high-bias, low-variance models that are too simple. The right side corresponds to low-bias, high-variance models that are too complex. The bottom of the U is the sweet spot where bias and variance are optimally balanced.\nLet us trace through this trade-off more concretely using the income inequality study. Suppose the researcher considers a range of models of increasing flexibility. At the simplest extreme, imagine a model that simply predicts the overall average income in the training sample for every individual, regardless of their education, occupation, or any other characteristic. This model has essentially zero variance - it will produce nearly the same prediction no matter which training sample is drawn, because sample means are very stable. But it has enormous bias, because it completely ignores all the systematic relationships between the predictors and income. The test MSE would be very high, driven almost entirely by bias. Next, consider a standard linear regression that includes all seven predictors. This model can capture the average linear effect of education, occupation, region, family background, race, gender, and work experience on income. Compared to the naive average, it has substantially lower bias because it acknowledges that these factors matter and estimates their effects. Its variance is still relatively low because the linear structure constrains the model considerably. The test MSE would be much lower than that of the naive model. Now consider a more flexible model that allows for nonlinear effects - perhaps using polynomial terms or splines to model the relationship between years of education and income, or including interaction terms that allow the effect of work experience to vary by occupation. This model can capture important features of the true relationship that linear regression misses, such as the disproportionate returns to completing a university degree or the different career trajectories across occupations. The bias decreases further. The variance increases somewhat because the model now has more parameters to estimate and is more sensitive to the particular sample. If the true relationship genuinely contains these nonlinearities and interactions, the reduction in bias will outweigh the increase in variance, and the test MSE will decrease. However, if the researcher continues to increase flexibility - adding higher-order polynomial terms, three-way and four-way interactions between predictors, and extremely localized fits - the model begins to adapt to features of the training data that are not part of the true underlying relationship. Perhaps in this specific sample, there happen to be three individuals from a particular region who all have unusually high incomes, and the very flexible model adjusts its predictions for that region upward to accommodate these three people. In a new sample, this pattern would not recur, and the model’s predictions for that region would be systematically too high. The variance is now large, the bias gains from additional flexibility are negligible, and the test MSE starts climbing.\n3. The Irreducible Error as a Floor\nAn important implication of the bias-variance decomposition is that there is a floor below which the test MSE can never fall, no matter how good our model is. This floor is the irreducible error, \\(Var(\\epsilon)\\). In the income inequality example, even if we had a perfect model that captured every systematic relationship between the seven predictors and income, there would still be variation in income that these predictors cannot explain. Two individuals who are identical in terms of education, occupation, region, family background, race, gender, and work experience will still have different incomes, because of all the unmeasured factors that influence earnings. No model built from these seven predictors can predict this residual variation, and so the test MSE can never be reduced below this level.\nThis is an important reminder for social science researchers. The irreducible error is not a failure of the model - it is a reflection of the inherent complexity of social phenomena. Human income is influenced by a vast number of factors, and no finite set of predictors can account for all of them. The goal of statistical learning is not to eliminate all prediction error but to reduce the reducible portion of the error - the part driven by bias and variance - as much as possible.\nThe bias-variance trade-off has profound practical implications for anyone building statistical models in the social sciences. It tells us that the most complex model is not necessarily the best model. A researcher who uses an extremely flexible machine learning method to predict income might achieve a very low training MSE, but if the model has high variance, its predictions on new data could be poor. Conversely, a researcher who sticks with simple linear regression out of tradition might be leaving predictive accuracy on the table if the true relationships are genuinely nonlinear. The trade-off also explains why different datasets may require different levels of model flexibility. If the true relationship between predictors and the response is approximately linear - perhaps because the predictors have been carefully chosen and transformed - then a simple model will have low bias to begin with, and increasing flexibility will mainly add variance without much benefit. If, on the other hand, the true relationship is highly nonlinear and involves complex interactions, a simple model will have high bias, and the researcher needs to use a more flexible approach to capture the important patterns, accepting some increase in variance as the cost of reducing bias.\nIn real-life situations where the true \\(f\\) is unknown - which is essentially always the case in practice - we cannot directly compute the bias, the variance, or even the test MSE. We cannot look at the decomposition and decide exactly where the optimal flexibility lies. Nevertheless, keeping the bias-variance trade-off in mind helps guide our thinking. It reminds us to be skeptical of models that fit the training data too perfectly, to consider whether our model might be too simple or too complex for the phenomenon at hand, and to use techniques like cross-validation to empirically estimate the test MSE and find a good balance between bias and variance.\n\n\n\n1.3.2 The Classification Setting\nUp to this point, our discussion of model accuracy has focused entirely on the regression setting, where the response variable is quantitative. However, many research questions in the social sciences involve response variables that are qualitative rather than quantitative. A qualitative response variable takes on values that represent discrete categories or classes rather than numerical quantities. The classification setting deals with precisely this kind of problem: predicting which category an observation belongs to, rather than predicting a numerical value.\nThe concepts we have already covered - the distinction between training and test performance, the danger of overfitting, and the bias-variance trade-off - all carry over to the classification setting. However, the specific measures we use to evaluate model performance need to be adapted, because it no longer makes sense to talk about squared differences between predicted and actual values when the response is a category rather than a number.\nTo make the classification setting concrete within our sociological example, let us modify the research question slightly. Instead of predicting how much a person earns, suppose the researcher is now interested in predicting whether a person will end up in a state of economic vulnerability or not. The researcher might define economic vulnerability as earning below a certain threshold - say, below 60 percent of the median national income, which is a commonly used measure of relative poverty risk in European social policy research. The response variable Y is now qualitative: for each individual, it takes one of two values, either “economically vulnerable” or “not economically vulnerable”. The predictors remain the same seven variables we have been working with: educational credentials (\\(X_1\\)​), occupation (\\(X_2\\)​), geographic region (\\(X_3\\)​), parents’ socioeconomic status (\\(X_4\\)), race (\\(X_5\\)​), gender (\\(X_6\\)​), and years of work experience (\\(X_7\\)​). The research question is no longer about predicting the exact income a person will earn, but about classifying each individual into one of two categories based on their characteristics. This is a classification problem, and it requires different tools for measuring how well our model performs.\nIn the regression setting, we measured model performance using the mean squared error, which quantifies how far the predicted numerical values are from the actual numerical values. In the classification setting, the natural analogue is the error rate, which simply measures the proportion of observations that are incorrectly classified. The training error rate is computed by applying the model to the training data and counting the fraction of cases where the predicted class does not match the true class. Formally, this is expressed as:\n\\[\n\\frac{1}{n} \\sum_{i=1}^{n} I(y_i \\neq \\hat{y}_i)\n\\]\nIn this formula, \\(\\hat{y}_i\\)​ is the class label that the model predicts for the \\(i\\)-th observation, and \\(I(y_i \\neq \\hat{y}_i)\\) is an indicator function that equals one whenever the prediction is wrong and zero whenever the prediction is correct. By summing these indicators across all observations and dividing by the total number of observations, we get the fraction of misclassifications - the training error rate.\nIn our example, suppose the researcher trains a classification model on data from one thousand individuals. The model predicts, for each person, whether they are economically vulnerable or not. If the model correctly classifies 920 of the 1,000 individuals and misclassifies 80, the training error rate is 80 divided by 1,000, which equals 0.08, or 8 percent. This means the model gets it wrong for 8 percent of the people in the training sample.\nHowever, just as in the regression setting, the training error rate is not what we truly care about. What matters is the test error rate - the proportion of misclassifications when the model is applied to new observations that were not part of the training data. The test error rate is given by:\n\\[\nAve(I(y_0 \\neq \\hat{y}_0))\n\\]\nThis measures the average misclassification rate across test observations. A good classifier is one that achieves the smallest possible test error rate, meaning it correctly classifies the highest proportion of new, unseen individuals.\nIn our context, the researcher wants a model that can accurately predict economic vulnerability for future individuals - people who were not in the original training sample. Perhaps the model will be used to identify individuals at risk of poverty in a new survey wave, or to target social policy interventions toward those most likely to be economically vulnerable. The value of the model lies not in how well it classifies the one thousand people whose outcomes are already known, but in how well it classifies new individuals whose outcomes the researcher does not yet know.\n\n1.3.2.1 The Bayes Classifier\nThe Bayes classifier represents the best possible classification rule - the one that produces the lowest possible test error rate. Understanding the Bayes classifier is important not because we can ever actually use it in practice, but because it provides a benchmark against which all real classification methods can be evaluated.\nThe Bayes classifier works on a deceptively simple principle: for each observation, assign it to the class that is most probable given its predictor values. Formally, for a test observation with predictor vector \\(x_0\\)​, the Bayes classifier assigns the observation to the class \\(j\\) for which the conditional probability \\(Pr(Y = j \\mid X = x_0)\\) is largest.\nTo understand what this means in our example, consider a specific individual - a woman with a university degree, working in a service occupation, living in a rural region, from a lower-middle-class family background, who is white and has five years of work experience. The Bayes classifier asks: given this particular combination of characteristics, what is the probability that this person is economically vulnerable, and what is the probability that she is not? If the probability of being economically vulnerable given her specific profile is 0.25, and the probability of not being economically vulnerable is 0.75, then the Bayes classifier assigns her to the “not economically vulnerable” category, because that is the more probable outcome for someone with her characteristics.\nIn a two-class problem like ours - where the response is either “economically vulnerable” or “not economically vulnerable” - the Bayes classifier reduces to a simple rule: classify the individual as economically vulnerable if the probability of economic vulnerability given their predictor values exceeds 0.5, and classify them as not economically vulnerable otherwise. The boundary in predictor space where the probability of each class is exactly equal — where \\(Pr(Y = vulnerable \\mid X = x_0) = 0.5\\) - is called the Bayes decision boundary. On one side of this boundary, individuals are classified as vulnerable; on the other side, they are classified as not vulnerable.\nThe Bayes classifier produces the lowest possible test error rate, called the Bayes error rate. This rate is given by:\n\\[\n1 - E\\left(\\max_j \\Pr(Y = j \\mid X)\\right)\n\\]\nThe Bayes error rate is greater than zero whenever there is any overlap between the classes in the population - that is, whenever there exist regions of the predictor space where neither class has a probability of one. In our example, this overlap is substantial. Even among people with very similar educational credentials, occupations, and family backgrounds, some will be economically vulnerable and others will not, because of all the unmeasured factors that influence income. No classifier, no matter how sophisticated, can perfectly separate the two groups based on the seven predictors alone. The Bayes error rate represents this fundamental limit on classification accuracy, and it is directly analogous to the irreducible error in the regression setting.\nThe reason the Bayes classifier cannot be used in practice is that it requires perfect knowledge of the conditional probabilities \\(Pr(Y = j \\mid X = x_0)\\) for every possible combination of predictor values. In the real world, we never know these probabilities. We only have sample data from which we can try to estimate these probabilities. The Bayes classifier therefore serves as a theoretical gold standard - an ideal that real methods try to approximate but can never fully achieve.\n\n\n1.3.2.2 K-Nearest Neighbors\nSince the Bayes classifier is unattainable in practice, we need real methods that can approximate it using available data. One such method is the K-nearest neighbors classifier, commonly abbreviated as KNN. The KNN classifier is a conceptually simple approach that directly attempts to estimate the conditional probabilities that the Bayes classifier relies on, and then classifies each observation to the class with the highest estimated probability.\nThe KNN classifier works as follows. Given a positive integer K and a new observation \\(x_0\\)​ that we want to classify, the algorithm first identifies the K training observations that are closest to \\(x_0\\)​ in the predictor space. This set of K nearest neighbors is denoted \\(\\mathcal{N}_0\\)​. The classifier then estimates the conditional probability for each class as the proportion of those K neighbors that belong to that class:\n\\[\n\\Pr(Y = j \\mid X = x_0) = \\frac{1}{K} \\sum_{i \\in \\mathcal{N}_0} I(y_i = j)\n\\]\nFinally, KNN assigns the test observation \\(x_0\\)​ to the class with the largest estimated probability.\nTo see how this works in our income inequality example, suppose the researcher wants to predict whether a new individual - let us call her Person A - is economically vulnerable or not. Person A has a specific set of characteristics: a vocational degree, a clerical occupation, living in a mid-sized city, from a working-class family, who is a white woman with three years of work experience. The KNN classifier, with, say, \\(K = 5\\), would search through the entire training dataset of one thousand individuals and find the five people whose combination of education, occupation, region, family background, race, gender, and work experience is most similar to Person A’s profile. Perhaps among these five nearest neighbors, three are not economically vulnerable and two are economically vulnerable. The estimated probability of being not vulnerable is then 3/5, or 0.6, and the estimated probability of being vulnerable is 2/5, or 0.4. Since 0.6 is greater than 0.5, KNN classifies Person A as not economically vulnerable.\nThe intuition behind KNN is that people with similar characteristics tend to have similar outcomes. If most of the people in the training data who resemble Person A are not economically vulnerable, then it is reasonable to predict that Person A is also not economically vulnerable. This is a form of learning from analogy - the algorithm classifies new cases by looking at the outcomes of the most similar known cases.\nThe value of K - the number of neighbors considered - is a crucial parameter that profoundly affects the behavior of the KNN classifier. The choice of K determines where the classifier falls on the flexibility spectrum and therefore directly influences the bias-variance trade-off. When K is very small, say \\(K = 1\\), the classifier is extremely flexible. It classifies each new observation based on the single most similar training observation. This means the decision boundary - the line separating the region where the model predicts vulnerability from the region where it predicts non-vulnerability - is highly irregular, twisting and turning to accommodate the class label of every individual training observation. With \\(K = 1\\), the training error rate is actually zero, because each training observation is its own nearest neighbor, so the model always correctly classifies every observation in the training set. However, this impressive training performance is misleading. The model has effectively memorized the training data, including all of its noise and idiosyncrasies. When applied to new data, many of these intricate local patterns will not hold up, and the test error rate will be considerably higher than zero.\nIn our example, using \\(K = 1\\) would mean that the classification of a new individual depends entirely on which single person in the training data happens to have the most similar profile. If that nearest neighbor happens to be an unusual case - perhaps someone who is economically vulnerable despite having relatively favorable characteristics, due to some unmeasured factor like a health crisis - the model would make an incorrect prediction. With \\(K = 1\\), the classifier has very low bias because it imposes almost no assumptions about the shape of the true decision boundary, but it has very high variance because the prediction for any new observation can change dramatically depending on which particular training observations happen to be closest.\nWhen K is very large, the classifier becomes much less flexible. With a large K, the model averages over many training observations to make each prediction, which smooths out the local fluctuations and produces a decision boundary that is much more stable. However, if K is too large, the classifier becomes overly rigid. In the extreme case where K equals the total number of training observations, the classifier would simply predict the majority class for every new observation, ignoring the predictor values entirely. This would have very low variance - the prediction would be the same regardless of which training data were used - but very high bias, because it ignores all the information contained in the predictors.\nIn our example, using a very large K, say \\(K = 100\\), would mean that the prediction for each new individual is based on the outcomes of the 100 most similar people in the training data. This large neighborhood includes people who may not actually be very similar to the individual being classified, and the resulting prediction is essentially an average over a broad swath of the population. The decision boundary becomes very smooth, almost linear, and the model loses its ability to capture local patterns in the data - such as the fact that certain specific combinations of low education, unstable occupation, and disadvantaged family background are particularly strong predictors of economic vulnerability.\nFinall, neither extreme or very large K tends to produce good test error rates. With \\(K = 1\\), the classifier overfits by being too responsive to individual data points. With very large K, the classifier underfits by being too insensitive to meaningful patterns. The best test performance is typically achieved at an intermediate value of K that balances the competing demands of bias and variance. In the simulated example presented in the chapter, \\(K = 10\\) produced a test error rate very close to the theoretical minimum set by the Bayes error rate, illustrating that a well-chosen KNN classifier can approximate the unattainable Bayes classifier remarkably well.\nJust as in the regression setting, the test error rate in the classification setting follows the characteristic U-shape as model flexibility varies. For KNN, flexibility is inversely related to K: small values of K correspond to high flexibility, and large values of K correspond to low flexibility. To make the analogy with the regression plots clearer, the chapter plots the error rates as a function of \\(1/K\\), so that moving to the right on the horizontal axis corresponds to increasing flexibility.\nAs \\(1/K\\) increases from near zero toward one - that is, as K decreases from very large values toward one - the training error rate steadily declines, eventually reaching zero at \\(K = 1\\). This mirrors what we saw in the regression setting: more flexible models always fit the training data better. However, the test error rate does not follow the training error rate downward. Instead, it decreases initially as the classifier becomes flexible enough to capture the important patterns separating the two classes, reaches a minimum at some intermediate level of flexibility, and then increases as the classifier becomes so flexible that it starts overfitting to noise in the training data.\nIn our economic vulnerability example, this means that the researcher would find that a moderately flexible KNN classifier - one that considers a reasonable number of neighbors rather than too few or too many - produces the most accurate predictions for new individuals. Using too few neighbors leads to erratic predictions driven by the particular circumstances of a handful of similar individuals in the training data. Using too many neighbors washes out the meaningful differences between people with different risk profiles, producing predictions that are too uniform.\nThe classification setting reinforces the same fundamental lessons we learned in the regression setting. First, training performance is an unreliable guide to how well a model will perform on new data. A classifier that achieves a very low training error rate may be overfitting, memorizing the training data rather than learning generalizable patterns. Second, the bias-variance trade-off applies to classification just as it applies to regression. Simple classifiers have high bias and low variance, flexible classifiers have low bias and high variance, and the best test performance lies somewhere in between. Third, there exists a theoretical limit on how well any classifier can perform - the Bayes error rate - that is determined by the inherent overlap between the classes in the population and by the information content of the available predictors.\nFor research on economic vulnerability, this means that no model built from the seven predictors we have considered can perfectly classify every individual. Some people with seemingly favorable characteristics will nonetheless be economically vulnerable, and some with seemingly unfavorable characteristics will not be. The irreducible error reflects the complexity of social life - the fact that economic outcomes are shaped by a multitude of factors, many of which cannot be captured in any feasible set of measured variables. The goal of the researcher is not to eliminate this irreducible uncertainty but to build a classifier that comes as close as possible to the Bayes ideal, capturing the genuine patterns in the data without being misled by noise.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistical Learning</span>"
    ]
  },
  {
    "objectID": "02_00_linear_regression.html",
    "href": "02_00_linear_regression.html",
    "title": "Linear Regression Model",
    "section": "",
    "text": "NoteDataset: student_performance\n\n\n\n\n\nThe student_performance dataset is a large educational dataset that seeks to capture the many different factors that can influence how well a student performs on their final exam. The fundamental research question behind this dataset is: what predicts student academic achievement, and how strongly does each factor contribute? This dataset is publicly available on the Kaggle platform, where it was published under the title (Student Performance Factors) [https://www.kaggle.com/datasets/lainguyn123/student-performance-factors]. The dataset contains 6,607 observations, where each observation represents one individual student. For each student, 20 variables have been recorded. One of these variables is the outcome we want to predict (dependent variable), and the remaining 19 are potential predictors (independent variables). Let us walk you through each of them:\n\n“hours_studied” records how many hours per week each student dedicates to studying,\n“attendance” represents the percentage of classes each student attended during the course,\n“parental_involvement” describes the degree to which a student’s parents are involved in their education within the three levels (Low, Medium, and High),\n“access_to_resources” indicates the level of access a student has to educational resources such as textbooks, computers, or online materials within the three levels (Low, Medium, and High),\n“extracurricular_activities” records whether the student participates in extracurricular activities (TRUE or FALSE),\n“sleep_hours” records the average number of hours of sleep the student gets per night,\n“previous_scores” captures the student’s scores from prior academic assessments,\n“motivation_level” captures the student’s self-reported level of motivation within the three categories (Low, Medium, and High),\n“internet_access” indicates whether the student has access to the internet (TRUE or FALSE),\n“tutoring_sessions” records the number of tutoring sessions the student attended,\n“family_income” records the economic status of the student’s family, categorized as Low, Medium, or High,\n“teacher_quality” describes the perceived quality of the student’s teachers, categorized as Low, Medium, or High,\n“school_type” indicates whether the student attends a public or private school,\n“peer_influence” describes the nature of the influence the student’s peer group has on their academic behavior, and it has three levels (Negative, Neutral, and Positive),\n“physical_activity” measures how many hours per week the student spends on physical activities,\n“learning_disabilities” records whether the student has been diagnosed with any learning disabilities (TRUE or FALSE),\n“parental_education_level” captures the highest level of education achieved by the student’s parents, with categories including High School, College, and Postgraduate.\n“distance_from_home” indicates how far the student lives from their school, categorized as Near, Moderate, or Far,\n“gender” records the student’s gender as either Male or Female,\n“exam_score” records the score that each student achieved on their final exam.\n\n\n# Pregled podataka\nglimpse(student_performance)\n\nRows: 6,607\nColumns: 20\n$ hours_studied              &lt;dbl&gt; 23, 19, 24, 29, 19, 19, 29, 25, 17, 23, 17,…\n$ attendance                 &lt;dbl&gt; 84, 64, 98, 89, 92, 88, 84, 78, 94, 98, 80,…\n$ parental_involvement       &lt;chr&gt; \"Low\", \"Low\", \"Medium\", \"Low\", \"Medium\", \"M…\n$ access_to_resources        &lt;chr&gt; \"High\", \"Medium\", \"Medium\", \"Medium\", \"Medi…\n$ extracurricular_activities &lt;lgl&gt; FALSE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE,…\n$ sleep_hours                &lt;dbl&gt; 7, 8, 7, 8, 6, 8, 7, 6, 6, 8, 8, 6, 8, 8, 8…\n$ previous_scores            &lt;dbl&gt; 73, 59, 91, 98, 65, 89, 68, 50, 80, 71, 88,…\n$ motivation_level           &lt;chr&gt; \"Low\", \"Low\", \"Medium\", \"Medium\", \"Medium\",…\n$ internet_access            &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T…\n$ tutoring_sessions          &lt;dbl&gt; 0, 2, 2, 1, 3, 3, 1, 1, 0, 0, 4, 2, 2, 2, 1…\n$ family_income              &lt;chr&gt; \"Low\", \"Medium\", \"Medium\", \"Medium\", \"Mediu…\n$ teacher_quality            &lt;chr&gt; \"Medium\", \"Medium\", \"Medium\", \"Medium\", \"Hi…\n$ school_type                &lt;chr&gt; \"Public\", \"Public\", \"Public\", \"Public\", \"Pu…\n$ peer_influence             &lt;chr&gt; \"Positive\", \"Negative\", \"Neutral\", \"Negativ…\n$ physical_activity          &lt;dbl&gt; 3, 4, 4, 4, 4, 3, 2, 2, 1, 5, 4, 2, 4, 3, 4…\n$ learning_disabilities      &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, F…\n$ parental_education_level   &lt;chr&gt; \"High School\", \"College\", \"Postgraduate\", \"…\n$ distance_from_home         &lt;chr&gt; \"Near\", \"Moderate\", \"Near\", \"Moderate\", \"Ne…\n$ gender                     &lt;chr&gt; \"Male\", \"Female\", \"Male\", \"Male\", \"Female\",…\n$ exam_score                 &lt;dbl&gt; 67, 61, 74, 71, 70, 71, 67, 66, 69, 72, 68,…\n\n\n\n\n\nLinear regression is one of the oldest, most fundamental, and most widely used methods in statistics and statistical learning. Despite its simplicity, it remains an essential tool for anyone working with data, especially in the social sciences. The core idea behind linear regression is straightforward: we want to understand and quantify the relationship between one or more explanatory variables and a single outcome variable. More specifically, linear regression assumes that the relationship between the dependent and the independent variable can be described, at least approximately, by a straight line - or in the case of multiple predictors, by a flat surface (a hyperplane) in higher-dimensional space.\nThe purpose of linear regression is twofold. First, it serves an explanatory purpose: it helps us understand which factors are associated with the outcome and how strongly each factor contributes, while holding the other factors constant. Second, it serves a predictive purpose: once we have estimated the relationship, we can use it to predict the outcome for new observations based on their predictor values. In our student performance example, the central question is: what factors influence a student’s exam score, and by how much? Linear regression allows us to answer questions such as “does studying more hours lead to higher exam scores?” and “how much additional score can a student expect to gain for each additional hour of study?”",
    "crumbs": [
      "Linear Regression Model"
    ]
  },
  {
    "objectID": "02_01_quantitative_linear_regression.html",
    "href": "02_01_quantitative_linear_regression.html",
    "title": "2  Quntitative Linear Regression",
    "section": "",
    "text": "2.1 Estimating the Coefficients of Parameters\nThe simplest form of linear regression involves just one independent variable. This is called simple linear regression. Mathematically, it takes the form:\n\\(Y \\approx \\beta_0 + \\beta_1X\\)\nIn this equation, \\(Y\\) is the dependent variable - the outcome we want to predict. In our example, \\(Y\\) is, for example, the variable exam_score. \\(X\\) is the independent variable - the factor we believe is related to the outcome. For instance, \\(X\\) could be hours_studied. The symbol \\(\\beta_0\\) is called the intercept. It represents the expected value of \\(Y\\) when \\(X\\) equals zero. In our context, it would represent the expected exam score for a hypothetical student who studies zero hours. The symbol \\(\\beta_1\\) is called the slope. It represents the average change in \\(Y\\) that is associated with a one-unit increase in \\(X\\). In our example, it tells us how many additional points on the exam a student can expect to gain for each additional hour of studying. Together, \\(\\beta_0\\) and \\(\\beta_1\\) are called the model coefficients or parameters.\nOf course, in real life we do not know the true values of \\(\\beta_0\\) and \\(\\beta_1\\). We must estimate them from the data. Once we have estimated these coefficients - and we denote the estimates with a hat symbol as \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) - we can write our prediction equation as: \\(y = \\hat{\\beta_0} + \\hat{\\beta_1}x\\). Here, \\(\\hat{y}\\) is the predicted value of the response for a given value \\(x\\) of the predictor. The hat symbol always indicates that we are dealing with an estimate rather than a true and known quantity.\nIn practice, a single predictor is rarely sufficient to explain all the variation in the response. A student’s exam score is not determined by study hours alone - attendance, prior academic performance, tutoring, and many other factors play a role. Multiple linear regression extends the simple model to accommodate several predictors simultaneously. The general formula is:\n\\(Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p + \\epsilon\\)\nIn this equation, \\(X_1\\), \\(X_2\\), \\(X_3\\), …, \\(X_p\\) represent \\(p\\) different independent variables, and \\(\\beta_1\\), \\(\\beta_2\\), …, \\(\\beta_p\\) are their corresponding slope coefficients. Each coefficient \\(\\beta_j\\) represents the average change in \\(Y\\) associated with a one-unit increase in the predictor \\(\\beta_j\\), while holding all other predictors constant which distinguishes multiple regression from simply running many separate simple regressions. The term \\(\\epsilon\\) represents the error term - it captures everything that our model does not explain, including the influence of unmeasured variables, measurement error, and the inherent randomness in human behavior.\nIn our student performance example, a multiple linear regression model might look like this:\n\\(\\text{exam_score} = \\beta_0 + \\beta_1 \\times \\text{hours_studied} + \\beta_2 \\times \\text{attendance} + \\beta_3 \\times \\text{previous_scores} + \\beta_4 \\times \\text{sleep_hours} + \\beta_5 \\times \\text{tutoring_sessions} + \\beta_6 \\times \\text{physical_activity} + \\epsilon\\)\nThis model allows us to estimate the unique contribution of each predictor to the exam score. For instance, \\(\\beta_1\\) tells us the expected change in exam score for each additional hour of study, after accounting for the effects of attendance, previous scores, sleep, tutoring, and physical activity. This is fundamentally different from simple linear regression, where \\(\\beta_1\\) would capture the total association between study hours and exam scores without adjusting for any other factor.\nThe key question is how do we actually find the best values for our coefficient estimates \\(\\hat{\\beta}_0\\), \\(\\hat{\\beta}_1\\), …, \\(\\hat{\\beta}_p\\). The answer lies in the least squares method, which is the most common approach for fitting a linear regression model. The basic idea is intuitive: we want our predicted values \\(\\hat{y}_i\\) to be as close as possible to the actual observed values \\(y_0\\) for every observation in our dataset.\nFor each observation \\(i\\), the difference between the observed value and the predicted value is called the residual, denoted \\(e_i = y_i - \\hat{y}_i\\). The residual tells us how much our model’s prediction misses the actual outcome for that particular student. Some residuals will be positive (when the model underpredicts) and some will be negative (when the model overpredicts).\nSimple Linear Regression: Exam Score ~ Hours Studied\nTo get an overall measure of how well the model fits all the data, we cannot simply add up the residuals, because the positive and negative ones would cancel each other out. Instead, we square each residual and then sum them all up. This quantity is called the residual sum of squares (RSS):\n\\(RSS = e_1^2 + e_2^2 + ... + e_n^2 = \\sum(y_i - \\hat{y}_i)^2\\)\nThe least squares method chooses the coefficient estimates \\(\\hat{\\beta}_0\\), \\(\\hat{\\beta}_1\\), …, \\(\\hat{\\beta}_p\\) that minimize this RSS. In other words, the least squares approach finds the line (in simple regression) or the hyperplane (in multiple regression) that makes the total squared prediction error as small as possible. This is a well-defined mathematical optimization problem, and the solution can be computed using calculus. For simple linear regression, the formulas for the minimizers have a closed-form expression:\n\\(\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\hat{x}\\)\n\\(\\hat{\\beta}_1 = \\frac{\\sum_{i = 1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i = 1}^n(x_i - \\bar{x})^2}\\)\nHere, \\(\\bar{x}\\) and \\(\\bar{y}\\) are the sample means of the predictor and the response, respectively. The formula for \\(\\hat{\\beta}_1\\) has an intuitive interpretation: it measures the degree to which \\(X\\) and \\(Y\\) vary together (the numerator captures their joint variation) relative to the total variation in \\(X\\) (the denominator). For multiple linear regression, the coefficient estimates are computed using matrix algebra.\nBy this, hte least squares method provides a principled, objective way to estimate the model parameters. It does not require any subjective judgment about what the “best” line should look like - the method simply finds the line that minimizes the total squared distance between the observed data points and the fitted line.\nTo fit a simple linear regression model in R, we use the lm() function. The syntax follows the pattern lm(response ~ predictor, data = dataset). The tilde symbol (~) can be read as “is modeled as a function of”. The lm() fits the model by computing the least squares coefficient estimates, and the output shows the estimated parameters of a simple linear regression model.\nThe summary() function then provides a detailed output that includes the estimated coefficients, their standard errors, t-statistics, p-values, the residual standard error, and the \\(R^2\\) statistic. The confint() function computes the 95% confidence intervals for each coefficient estimate, which tell us the range of plausible values for the true population parameters.\nFor our simple linear regression of exam_score onto hours_studied, we write:\nsimple_model &lt;- lm(\n    exam_score ~ hours_studied,\n    data = student_performance\n)\n\nsummary(simple_model)\n\n\nCall:\nlm(formula = exam_score ~ hours_studied, data = student_performance)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.532 -2.243 -0.111  2.046 33.493 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   61.456984   0.149196  411.92   &lt;2e-16 ***\nhours_studied  0.289291   0.007154   40.44   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.483 on 6605 degrees of freedom\nMultiple R-squared:  0.1984,    Adjusted R-squared:  0.1983 \nF-statistic:  1635 on 1 and 6605 DF,  p-value: &lt; 2.2e-16\nsummary(simple_model)$coefficients[, \"Estimate\"]\n\n  (Intercept) hours_studied \n   61.4569836     0.2892906\nThe intercept is estimated at 61.4570. This means that when hours_studied equals zero, the model predicts an exam score of approximately 61.46 points. In substantive terms, a hypothetical student who does not study at all would be expected to score about 61.5 on the exam, according to this model. This makes intuitive sense - students would still have some baseline level of knowledge from attending classes, even without additional study outside the classroom. The slope for hours_studied is estimated at 0.289. This is the key coefficient for our research question. It tells us that for each additional hour of study per week, a student’s exam score is expected to increase by approximately 0.29 points, on average. So a student who studies 10 hours more per week than another student would be expected to score about 2.89 points higher on the exam.\nFor the multiple linear regression model, we simply add more predictors to the right side of the formula, separated by the + sign:\nmultiple_model &lt;- lm(\n    exam_score ~ hours_studied + attendance + previous_scores + sleep_hours + tutoring_sessions + physical_activity,\n    data = student_performance\n)\n\nsummary(multiple_model)\n\n\nCall:\nlm(formula = exam_score ~ hours_studied + attendance + previous_scores + \n    sleep_hours + tutoring_sessions + physical_activity, data = student_performance)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.4391 -1.1316 -0.1619  0.8435 30.9951 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       40.927132   0.338284 120.985  &lt; 2e-16 ***\nhours_studied      0.291579   0.005069  57.517  &lt; 2e-16 ***\nattendance         0.197978   0.002631  75.262  &lt; 2e-16 ***\nprevious_scores    0.048123   0.002110  22.809  &lt; 2e-16 ***\nsleep_hours       -0.018022   0.020686  -0.871    0.384    \ntutoring_sessions  0.493505   0.024679  19.997  &lt; 2e-16 ***\nphysical_activity  0.143997   0.029449   4.890 1.03e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.467 on 6600 degrees of freedom\nMultiple R-squared:  0.5982,    Adjusted R-squared:  0.5979 \nF-statistic:  1638 on 6 and 6600 DF,  p-value: &lt; 2.2e-16\nThis tells R to fit a model that predicts exam_score using all six quantitative predictors simultaneously. The least squares method will estimate a separate slope coefficient for each predictor, along with a single intercept, by minimizing the total RSS across all 6,607 observations.\nsummary(multiple_model)$coefficients[, \"Estimate\"]\n\n      (Intercept)     hours_studied        attendance   previous_scores \n      40.92713226        0.29157938        0.19797753        0.04812256 \n      sleep_hours tutoring_sessions physical_activity \n      -0.01802166        0.49350511        0.14399695\nThe estimated model can be written as:\n\\(\\text{exam_score} \\approx 40.93 + 0.292 \\times \\text{hours_studied} + 0.198 \\times \\text{attendance} + 0.048 \\times \\text{previous_scores} + 0.018 \\times \\text{sleep_hours} + 0.494 \\times \\text{tutoring_sessions} + 0.144 \\times \\text{pphysical_activity}\\)\nThe intercept is now estimated at 40.927. This is substantially lower than in the simple model (61.46), which makes sense. In the simple model, the intercept represented the predicted score when only hours_studied was zero. In the multiple model, the intercept represents the predicted score when all six predictors are on their average value. Such a student is of course entirely hypothetical and unrealistic, which is why we should not over-interpret the intercept in multiple regression. Its main role is mathematical - it anchors the regression plane in the right position.\nThe coefficient for hours_studied is 0.292, which is remarkably similar to its value in the simple regression (0.289). This tells us something important: the relationship between study hours and exam scores is robust - it persists even after we account for the effects of attendance, prior scores, sleep, tutoring, and physical activity. In substantive terms, holding all other factors constant, each additional hour of study per week is associated with an increase of about 0.29 points on the exam.\nThe coefficient for attendance is 0.198, meaning that each additional percentage point of class attendance is associated with about 0.20 additional points on the exam, after controlling for the other variables. To put this in perspective, a student who attends 90% of classes versus one who attends 70% of classes would be expected to score about 3.96 points higher, all else being equal. The coefficient for previous_scores is 0.048. This means that for each additional point a student earned on their previous assessments, their exam score is expected to increase by about 0.048 points, holding other factors constant. A student whose prior scores are 20 points higher than another student’s would be expected to score only about 0.96 points higher on this exam. This suggests that while past performance does predict future performance, its incremental contribution is small once study habits and attendance are already accounted for. The coefficient for sleep_hours is -0.018. The negative sign suggests that more sleep is associated with slightly lower exam scores, meaning that each additional hour of sleep per night is associated with a decrease of about 0.02 points on the exam, after controlling for the other variables. The coefficient for tutoring_sessions is 0.494, the largest individual slope coefficient in the model. Each additional tutoring session is associated with about half a point increase on the exam. The coefficient for physical_activity is 0.144, meaning that each additional hour per week of physical activity is associated with about 0.14 additional exam points, after controlling for the other variables.",
    "crumbs": [
      "Linear Regression Model",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quntitative Linear Regression</span>"
    ]
  },
  {
    "objectID": "02_01_quantitative_linear_regression.html#accuracy-of-the-coefficient-estimates",
    "href": "02_01_quantitative_linear_regression.html#accuracy-of-the-coefficient-estimates",
    "title": "2  Quntitative Linear Regression",
    "section": "2.2 Accuracy of the Coefficient Estimates",
    "text": "2.2 Accuracy of the Coefficient Estimates\nIn the previous section, we estimated the coefficients of our linear regression models using the least squares method. We found, for instance, that the estimated slope for hours_studied was 0.289 in the simple model and 0.292 in the multiple model. A natural and critically important issue that arises here concerns the accuracy of these estimates. If a different sample of 6,607 students were collected, the coefficient estimates might not remain identical but could vary due to sampling variability. This raises the broader problem of statistical uncertainty: the observed relationship between study hours and exam scores may reflect a genuine positive association in the population, yet it is also possible that the estimated effect is partly driven by random variation specific to the analyzed sample. Consequently, the reliability and inferential validity of the estimated coefficients depend on the extent to which sampling error can be ruled out as a substantive explanation of the observed effect.\nThese questions lie at the heart of statistical inference, and the tools we use to answer them - standard errors, confidence intervals, t-statistics, and p-values - are among the most important concepts in all of applied statistics. To understand these tools, we must first understand the concept of the error term and the distinction between the population regression line and our estimated regression line.\nWhen we write the linear regression model as \\(Y = \\beta_0 + \\beta_1X + \\epsilon\\), we are making a statement about the true, underlying relationship between \\(X\\) and \\(Y\\) in the entire population - not just in our particular sample. The coefficients \\(\\beta_0\\) and \\(\\beta_1\\) are the true population parameters, which we will never know exactly. The term \\(\\epsilon\\) is the error term, and it represents everything that our model fails to capture. There are several reasons why the error term exists. First, the true relationship between the predictor and the response may not be perfectly linear - there may be curvature or other patterns that a straight line cannot capture. Second, there may be other variables that influence the response but are not included in our model. Third, there is always some inherent randomness and measurement error in any data we collect. The error term absorbs all of these sources of discrepancy between what our model predicts and what actually happens.\nIn our student_performance example, even if we knew the exact true values of \\(\\beta_0\\) and \\(\\beta_1\\) for the relationship between hours_studied and exam_score in the entire population of all students, we still could not predict any individual student’s exam score perfectly. Some students who study 20 hours per week will score higher than the regression line predicts, and others will score lower. These deviations are captured by \\(\\epsilon\\). We typically assume that the error term has a mean of zero (meaning the model does not systematically overpredict or underpredict), that the errors for different observations are independent of each other, and that the errors have a constant variance \\(\\sigma^2\\) across all values of \\(X\\).\nThe true population regression line, \\(Y = \\beta_0 + \\beta_1X\\), represents the best linear summary of the relationship between \\(X\\) and \\(Y\\) in the entire population. Our estimated regression line, \\(\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x\\), is our best approximation of this population line based on the data we have. The key insight is that \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are estimates of the true parameters, computed from one particular sample. If we were to draw a different random sample of 6,607 students, we would get slightly different estimates. This variability across samples is what motivates the need for standard errors, confidence intervals, and hypothesis tests - they allow us to quantify how much uncertainty surrounds our estimates.\n\n2.2.1 Standard Errors\nThe standard error of a coefficient estimate measures how much that estimate would vary if we repeatedly drew new samples from the same population and re-estimated the model each time. In other words, it quantifies the precision of our estimate. A small standard error means that our estimate is very precise - different samples would give us very similar coefficient values. A large standard error means that our estimate is imprecise - it might change substantially from sample to sample.\nThe standard error of the coefficients or parameters in linear regression is given by the formulas:\n\\(SE(\\hat{\\beta}_0) = \\sigma^2 [\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i =1}^n(x_i - \\bar{x})^2}]\\)\n\\(SE(\\hat{\\beta}_1) = \\frac{\\sigma}{\\sum_{i=1}^n(x_i - \\bar{x})^2}\\)\nThis formulas reveals two important things. First, the standard error depends on \\(\\sigma\\), the standard deviation of the error term. If there is a lot of noise in the data (large \\(\\sigma\\)), then the estimated slope will be less precise, because the true signal is harder to detect amid the noise. Second, the standard error depends on the spread of the predictor values. When the \\(x_i\\) values are more spread out (that is, when \\(\\sum_{i=1}^n(x_i - \\bar{x})^2\\) is large), the standard error is smaller. Intuitively, this makes sense: if we observe students who study anywhere from 1 to 44 hours per week, we have a much better basis for estimating the slope than if all students studied between 19 and 21 hours. A wider range of predictor values gives us more “leverage” to pin down the relationship. In practice, the true value of \\(\\sigma\\) is unknown and must be estimated from the data. This estimate is the Residual Standard Error.\n\nsummary(simple_model)$coefficients[, \"Std. Error\"]\n\n  (Intercept) hours_studied \n  0.149196057   0.007154262 \n\n\nLooking at our simple regression, the standard error for the hours_studied coefficient is 0.00715. This is very small relative to the coefficient estimate of 0.289, which tells us that our estimate is highly precise. The reason for this high precision is our large sample size combined with a good spread in study hours.\n\nsummary(multiple_model)$coefficients[, \"Std. Error\"]\n\n      (Intercept)     hours_studied        attendance   previous_scores \n      0.338284044       0.005069489       0.002630507       0.002109816 \n      sleep_hours tutoring_sessions physical_activity \n      0.020685503       0.024678804       0.029448624 \n\n\nIn the multiple regression model, the standard error for hours_studied is even smaller at 0.00507. This decrease occurs because the multiple model has a lower RSE, which means there is less unexplained noise once we account for the additional predictors - and less noise translates directly into more precise coefficient estimates. The standard errors for the other coefficients in the multiple model tell a similar story. attendance has a standard error of 0.00263, previous_scores has 0.00211, sleep_hours has 0.0207, tutoring_sessions has 0.0247, and physical_activity has 0.0294. Notice that sleep_hours has a relatively large standard error compared to its coefficient estimate (-0.018), which foreshadows the fact that this coefficient will not be statistically significant - the estimate is so imprecise relative to its magnitude that we cannot confidently distinguish it from zero.\nA confidence interval provides a range of plausible values for the true population parameter. The 95% confidence interval for a regression coefficient is constructed using the formula:\n\\(\\hat{\\beta}_j \\pm 2 \\times SE(\\hat{\\beta}_j)\\)\nMore precisely, the multiplier is not exactly 2 but rather the 97.5th percentile of the t-distribution with \\(n - p - 1\\) degrees of freedom, where \\(n\\) is the sample size and \\(p\\) is the number of predictors. For large samples like ours, this value is very close to 1.96, which is approximately 2. The interpretation of a 95% confidence interval is as follows: if we were to repeat the study many times, drawing a new random sample each time and computing a 95% confidence interval from each sample, then 95% of those intervals would contain the true population parameter. It is important to note that this does not mean there is a 95% probability that the true parameter lies within our specific interval - the true parameter is a fixed value, not a random quantity. Rather, the 95% refers to the long-run reliability of the procedure.\n\nconfint(simple_model)\n\n                  2.5 %     97.5 %\n(Intercept)   61.164511 61.7494561\nhours_studied  0.275266  0.3033153\n\n\nIn the simple regression, the 95% confidence interval for the hours_studied coefficient is [0.275, 0.303]. This interval is narrow, reflecting the high precision of our estimate, and it lies entirely above zero. We can therefore state with 95% confidence that the true effect of one additional hour of study falls somewhere between 0.275 and 0.303 points on the exam.\n\nconfint(multiple_model)\n\n                        2.5 %      97.5 %\n(Intercept)       40.26398610 41.59027841\nhours_studied      0.28164155  0.30151722\nattendance         0.19282088  0.20313417\nprevious_scores    0.04398664  0.05225848\nsleep_hours       -0.05857194  0.02252862\ntutoring_sessions  0.44512667  0.54188355\nphysical_activity  0.08626812  0.20172578\n\n\nIn the multiple regression, the confidence intervals are similarly informative. For attendance, the interval is [0.193, 0.203], meaning we are 95% confident that each additional percentage point of attendance is associated with between 0.19 and 0.20 additional exam points, after controlling for the other predictors. For tutoring_sessions, the interval is [0.445, 0.542], and for physical_activity it is [0.086, 0.202] - all comfortably above zero. The critical case is sleep_hours, whose confidence interval is [-0.059, 0.023]. Because this interval spans from a negative value to a positive value, crossing zero in the middle, we cannot determine whether the true effect of sleep hours on exam scores is positive, negative, or simply zero.\n\n\n2.2.2 Hypothesis Testing and the t-Statistic\nHypothesis testing provides a formal framework for determining whether the relationship we observe in our sample is likely to reflect a real relationship in the population, or whether it could plausibly be due to random chance. In linear regression, the most common hypothesis test for each coefficient is:\nThe null hypothesis (\\(H_0\\)): \\(\\beta_j = 0\\), meaning there is no relationship between \\(X_j\\) and \\(Y\\).\nThe alternative hypothesis (\\(H_a\\)): \\(\\beta_j \\neq 0\\), meaning there is some relationship between \\(X_j\\) and \\(Y\\).\nIf the null hypothesis is true and the predictor truly has no effect on the response, then the true slope is zero, and any non-zero slope we estimate from our sample is simply due to random noise. The t-statistic allows us to assess how likely this scenario is.\nThe t-statistic is computed as:\n\\(t = \\frac{\\hat{\\beta}_j}{SE(\\hat{\\beta}_j)}\\)\nThis is simply the coefficient estimate divided by its standard error. It measures how many standard errors the coefficient estimate is away from zero. A t-statistic close to zero means that the coefficient estimate is small relative to its uncertainty, which is consistent with the null hypothesis. A t-statistic far from zero (either very positive or very negative) means that the coefficient estimate is large relative to its uncertainty, which provides evidence against the null hypothesis.\nUnder the null hypothesis, the t-statistic follows a t-distribution with \\(n - p - 1\\) degrees of freedom, where \\(n\\) is the number of observations and \\(p\\) is the number of predictors. For large samples, the t-distribution is virtually identical to the standard normal distribution, so a t-statistic beyond roughly \\(\\pm2\\) is generally considered statistically significant at the 5% level, and beyond roughly \\(\\pm2,75\\) at the 1% level.\n\nsummary(simple_model)$coefficients[, \"t value\"]\n\n  (Intercept) hours_studied \n    411.92096      40.43612 \n\n\nIn the simple regression model, the t-statistic for hours_studied is 40.44. This means the coefficient estimate is more than 40 standard errors away from zero. To put this in perspective, if study hours truly had no effect on exam scores, observing a t-statistic this large would be essentially impossible - it would be like flipping a fair coin and getting heads 40 times in a row, except far less likely even than that. This gives us overwhelming evidence that the relationship between study hours and exam scores is real.\n\nsummary(multiple_model)$coefficients[, \"t value\"]\n\n      (Intercept)     hours_studied        attendance   previous_scores \n      120.9845189        57.5165220        75.2621118        22.8088935 \n      sleep_hours tutoring_sessions physical_activity \n       -0.8712218        19.9971244         4.8897684 \n\n\nIn the multiple regression model, the t-statistics reveal a clear hierarchy of evidence. attendance has the largest t-statistic at 75.26, making it the most precisely estimated and most strongly significant predictor. hours_studied follows with t = 57.52, then previous_scores at 22.81, tutoring_sessions at 20.00, and physical_activity at 4.89. All of these are far beyond any conventional significance threshold. The exception, as we have seen, is sleep_hours, with a t-statistic of only -0.871. This value is well within the range we would expect to see even if the true coefficient were zero - it is less than one standard error away from zero, which is entirely unremarkable.\n\n\n2.2.3 The p-Value\nThe p-value is the probability of observing a t-statistic as extreme as the one we actually computed, assuming that the null hypothesis is true. In other words, it answers the question: if there were truly no relationship between this predictor and the response, how surprising would our observed result be?\nA small p-value (typically below 0.05) indicates that the observed result would be very surprising under the null hypothesis, and we therefore reject the null hypothesis in favor of the alternative. A large p-value indicates that the observed result is not particularly surprising under the null hypothesis, and we therefore fail to reject it. It is important to emphasize that “failing to reject” is not the same as “accepting” the null hypothesis - it simply means we do not have enough evidence to conclude that a relationship exists.\n\nsummary(simple_model)$coefficients[, \"Pr(&gt;|t|)\"]\n\n  (Intercept) hours_studied \n 0.000000e+00 1.286349e-319 \n\n\nIn our simple regression output, the p-value for hours_studied is less than \\(2 \\times 10^{-16}\\), which R displays as “&lt;2e-16”. This is the smallest p-value that R can represent numerically, and it is so close to zero that for all practical purposes it means the probability of observing our results by chance alone is essentially zero.\n\nsummary(multiple_model)$coefficients[, \"Pr(&gt;|t|)\"]\n\n      (Intercept)     hours_studied        attendance   previous_scores \n     0.000000e+00      0.000000e+00      0.000000e+00     6.622656e-111 \n      sleep_hours tutoring_sessions physical_activity \n     3.836647e-01      2.030741e-86      1.033431e-06 \n\n\nIn the multiple regression, the p-values for hours_studied, attendance, previous_scores, and tutoring_sessions are all less than \\(2 \\times 10^{-16}\\), and the p-value for physical_activity is approximately \\(1.03 \\times 10^{-6}\\) (or about one in a million). All of these are far below any conventional significance threshold, providing overwhelming evidence that these predictors are genuinely related to exam scores.\nThe p-value for sleep_hours, however, is 0.384. This means that if sleep hours truly had no effect on exam scores, there would be about a 38.4% probability of observing a coefficient estimate as far from zero as the one we found. In other words, our observed result is entirely unsurprising under the null hypothesis - it is the kind of result we would expect to see by random chance alone roughly 38 times out of 100. We therefore have no grounds to reject the null hypothesis for sleep_hours, and we conclude that this variable does not have a statistically significant linear relationship with exam scores in this model.",
    "crumbs": [
      "Linear Regression Model",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quntitative Linear Regression</span>"
    ]
  },
  {
    "objectID": "02_01_quantitative_linear_regression.html#model-fit",
    "href": "02_01_quantitative_linear_regression.html#model-fit",
    "title": "2  Quntitative Linear Regression",
    "section": "2.3 Model Fit",
    "text": "2.3 Model Fit\nIn the previous section, we focused on assessing the accuracy of individual coefficient estimates - asking whether each specific predictor is significantly related to the response. Now we shift our perspective and ask how well the model as a whole fits the data. In other words, we have to examine how good it is at capturing the actual patterns in student exam scores.\nTo answer these questions, we rely on three complementary statistics that appear at the bottom of every regression summary in R: the residual standard error (RSE), the \\(R^2\\) statistic, and the F-statistic. Each of these measures provides a different lens through which to evaluate the overall quality of the model, and together they give us a well-rounded picture of model performance.\n\n2.3.1 Residual Standard Error\nThe Residual Standard Error (RSE) is perhaps the most intuitive measure of model accuracy, because it is expressed in the same units as the dependent variable. It estimates the standard deviation of the error term - that is, it tells us the typical size of the prediction errors our model makes. The RSE is computed using the formula:\n\\(RSE = \\sqrt{\\frac{RSS}{n - p - 1}}\\)\nIn this formula, RSS is the residual sum of squares, \\(n\\) is the number of observations, and \\(p\\) is the number of predictors. The denominator uses \\(n - p - 1\\) rather than simply \\(n\\) because we have used up \\(p + 1\\) degrees of freedom in estimating the intercept and the \\(p\\) slope coefficients. This correction ensures that the RSE is an unbiased estimate of the true error standard deviation \\(\\sigma\\). The concept of degrees of freedom can be understood intuitively: each parameter we estimate “uses up” one piece of information from the data, leaving fewer independent pieces of information available to estimate the error variance.\n\nsummary(simple_model)$sigma\n\n[1] 3.483406\n\n\nIn our simple regression model, the RSE is 3.483. This means that, on average, the actual exam scores deviate from the scores predicted by the model by approximately 3.48 points. Given that the mean exam score in our dataset is 67.24, we can express this as a percentage error of about 5.18% (3.483 / 67.24 × 100). Whether this level of error is acceptable depends entirely on the context of the research. From a pure prediction standpoint, it also tells us that our simple model leaves a lot of room for improvement - knowing only how many hours a student studies does not allow us to predict their exam score with great precision.\n\nsummary(multiple_model)$sigma\n\n[1] 2.467039\n\n\nIn our multiple regression model, the RSE drops to 2.467. This represents a substantial improvement over the simple model - the average prediction error has decreased by about 29%, from 3.48 to 2.47 points. The percentage error relative to the mean is now approximately 3.67% (2.467 / 67.24 × 100). This decrease makes intuitive sense: by adding attendance, previous scores, tutoring sessions, and physical activity to the model, we have incorporated additional information that helps explain why some students score higher than others. The model now captures more of the systematic patterns in the data, leaving less to the error term. It is worth noting, however, that the RSE can never reach zero unless our model perfectly predicts every single observation, which is essentially impossible with real-world data involving human behavior. There will always be some irreducible error that no model can eliminate.\nOne important limitation of the RSE is that it is measured in the units of the response variable, which makes it difficult to compare across different studies or datasets with different response scales. If another researcher studied a test scored on a scale of 0 to 500, their RSE would naturally be much larger in absolute terms, even if their model were proportionally just as accurate as ours. This limitation motivates the need for a scale-independent measure of model fit, which is exactly what the \\(R^2\\) statistic provides.\n\n\n2.3.2 \\(R^2\\)\nThe \\(R^2\\) statistic, also known as the coefficient of determination, is one of the most commonly reported measures of model fit in applied research. Unlike the RSE, \\(R^2\\) is a proportion that always takes a value between 0 and 1, making it easy to interpret and compare across studies regardless of the scale of the response variable. \\(R^2\\) answers a question on what fraction of the total variation in the response variable is explained by the model. To understand \\(R^2\\), we need to consider two quantities.\nThe first is the Total Sum of Squares (TSS), defined as:\n\\(TSS = \\sum(y_i - \\hat{y})^2\\)\nThis measures the total variability in the response variable before any regression is performed. It is simply the sum of the squared deviations of each observed exam score from the overall mean exam score. In our dataset, this captures the full extent to which students’ exam scores differ from one another. Some of this variation is systematic and some of it is random noise.\nThe second quantity is the Residual Sum of Squares (RSS), which we have already encountered:\n\\(RSS = \\sum(y_i - \\hat{y}_i)^2\\)\nThis measures the variability that remains unexplained after fitting the regression model. It is the sum of the squared residuals - the squared differences between the actual exam scores and the scores predicted by the model.\nThe \\(R^2\\) statistic is then defined as:\n\\(R^2 = \\frac{(TSS - RSS)}{TSS} = 1 - \\frac{RSS}{TSS}\\)\nThe numerator, TSS - RSS, represents the amount of variability in the response that is explained by the regression - it is the reduction in prediction error achieved by using the model instead of simply predicting the mean for every student. Dividing by TSS converts this into a proportion.\nWhen \\(R^2\\) is close to 1, it means that the model explains nearly all of the variation in the response. In such a case, the predicted values \\(\\hat{y}_i\\) are very close to the actual values \\(y_i\\), and the model provides an excellent fit. When \\(R^2\\) is close to 0, the model explains very little of the variation, and using the model is hardly better than simply predicting the mean exam score for every student.\n\nsummary(simple_model)$r.squared\n\n[1] 0.1984301\n\n\nIn our simple regression model, \\(R^2\\) is 0.1984. This tells us that hours_studied alone explains approximately 19.84% of the total variation in exam scores. In other words, about one-fifth of the differences in exam scores among students can be attributed to differences in how many hours they study. This is a meaningful finding - it confirms that study time matters - but it also reveals that roughly 80% of the variation is driven by other factors not captured in this simple model.\n\nsummary(multiple_model)$r.squared\n\n[1] 0.5982496\n\n\nIn our multiple regression model, \\(R^2\\) jumps to 0.5982. Now the model explains approximately 59.82% of the variation in exam scores. This is a dramatic improvement - by adding attendance, previous scores, tutoring sessions, and physical activity as predictors alongside study hours, we have nearly tripled the proportion of explained variance. The remaining approximately 40% of the variation is still unexplained, presumably due to factors that are not included as quantitative predictors in this model - things like motivation level, family income, teacher quality, peer influence, and other qualitative variables in our dataset that we have not yet incorporated, as well as entirely unmeasured factors like test anxiety, the specific content of the exam, or simple luck.\nWhat constitutes a “good” \\(R^2\\) depends heavily on the field of study. In the physical sciences, where experiments can be tightly controlled and measurement is very precise, \\(R^2\\) values above 0.95 are common and expected. In the social sciences and education research, where human behavior is inherently noisy and influenced by a vast number of interacting factors, \\(R^2\\) values between 0.30 and 0.60 are often considered quite good for observational studies. Our multiple model’s \\(R^2\\) of 0.598 is therefore quite respectable for educational data - it suggests that we have identified a set of predictors that genuinely capture a large portion of what drives student performance.\nIt is critical to understand one important caveat about \\(R^2\\): it will always increase (or at least never decrease) when more predictors are added to the model, even if those predictors are completely unrelated to the response. This happens because adding any variable, even a random one, gives the model more flexibility to fit the training data, and the RSS can only go down or stay the same - it can never go up. This means that a high \\(R^2\\) does not necessarily indicate a good model; it could simply reflect the fact that many predictors have been thrown in. To guard against this problem, the Adjusted \\(R^2\\) was developed. The Adjusted \\(R^2\\) modifies the standard \\(R^2\\) by imposing a penalty for each additional predictor. If adding a new predictor does not reduce the RSS enough to offset the penalty for the lost degree of freedom, the Adjusted \\(R^2\\) will actually decrease, signaling that the added predictor is not contributing meaningfully.\n\nsummary(simple_model)$adj.r.squared\n\n[1] 0.1983088\n\nsummary(multiple_model)$adj.r.squared\n\n[1] 0.5978844\n\n\nIn our simple model, the Adjusted \\(R^2\\) is 0.1983, virtually identical to \\(R^2\\) because there is only one predictor and the penalty is negligible. In our multiple model, the Adjusted \\(R^2\\) is 0.5979, also nearly identical to the regular \\(R^2\\) of 0.5982. The fact that the Adjusted \\(R^2\\) barely differs from \\(R^2\\) in the multiple model tells us that all six predictors (or at least most of them) are genuinely contributing to the model’s explanatory power - the improvement in fit is not merely an artifact of adding more variables.\nIt is also worth noting the connection between \\(R^2\\) and correlation. In simple linear regression, \\(R^2\\) is exactly equal to the square of the Pearson correlation coefficient r between \\(X\\) and \\(Y\\). In our case, \\(R^2\\) = 0.1984 for the simple model, so the correlation between hours_studied and exam_score is \\(r = \\sqrt{0.1984} \\approx 0.445\\). In multiple regression, this simple relationship no longer holds (because there are multiple predictors), but \\(R^2\\) can be shown to equal the squared correlation between the observed values \\(y_i\\) and the fitted values \\(\\hat{y}_i\\). This provides a nice intuitive interpretation: \\(R^2\\) tells us how closely the model’s predictions track the actual outcomes.\n\n\n2.3.3 F-statistic\nWhile the t-statistic and its associated p-value allow us to test whether each individual predictor is significantly related to the response, the F-statistic addresses if the model as a whole is useful. Specifically, the F-statistic tests the null hypothesis that all slope coefficients in the model are simultaneously equal to zero \\(H_0: \\beta_1 = \\beta_2 = ... = \\beta_p = 0\\) against the alternative hypothesis \\(H_a\\): at least one \\(\\beta_j\\) is non-zero.\nIf the null hypothesis is true, then none of the predictors have any relationship with the response, and the model is no better than simply predicting the mean for every observation. The F-statistic is computed as:\n\\(F = \\frac{\\frac{TSS - RSS}{p}}{\\frac{RSS}{n - p -1}}\\)\nThe numerator measures how much of the total variance the model explains, divided by the number of predictors \\(p\\). The denominator measures how much variance remains unexplained, divided by the residual degrees of freedom. If the model is no better than chance, the numerator and denominator should be roughly equal, producing an F-statistic close to 1. If the model captures real patterns in the data, the numerator will be much larger than the denominator, producing a large F-statistic.\nOne might reasonably ask: why do we need the F-statistic at all when we already have individual t-tests for each coefficient? The answer lies in the multiple testing problem. When we have many predictors, each with its own t-test, the probability of finding at least one “significant” result by pure chance increases dramatically. For example, if we tested 100 completely useless predictors at the 5% significance level, we would expect about 5 of them to appear significant purely by chance. The F-statistic avoids this problem because it is a single, omnibus test that accounts for the total number of predictors. It maintains the correct overall error rate regardless of how many predictors are in the model. So the proper approach is to first check the F-statistic to determine whether the model as a whole is significant, and only then examine the individual t-statistics to identify which specific predictors are contributing.\n\nsummary(simple_model)$fstatistic\n\n  value   numdf   dendf \n1635.08    1.00 6605.00 \n\n\nIn our simple regression model, the F-statistic is 1,635 with 1 and 6,605 degrees of freedom, and the associated p-value is less than \\(2.2 \\times 10^{16}\\). Since we have only one predictor in the simple model, the F-test is equivalent to the t-test for that predictor. In fact, the F-statistic in simple regression is exactly the square of the t-statistic: \\(40.44^2 \\approx 1,635\\). The overwhelming magnitude of this F-statistic and its essentially zero p-value tell us that the model is highly significant - hours_studied is unquestionably related to exam_score.\n\nsummary(multiple_model)$fstatistic\n\n   value    numdf    dendf \n1638.019    6.000 6600.000 \n\n\nIn our multiple regression model, the F-statistic is 1,638 with 6 and 6,600 degrees of freedom, and the p-value is again less than \\(2.2 \\times 10^{-16}\\). This tests whether at least one of the six predictors is related to exam scores. The result decisively rejects the null hypothesis - the model as a whole is highly significant, and at least one (and as we saw from the individual t-tests, five out of six) predictors are genuinely related to student performance. The fact that the F-statistic is 1,638 rather than close to 1 tells us that the model explains vastly more variance than we would expect by chance alone.\nIt is worth pausing to note an interesting detail. Even though sleep_hours was not individually significant (p = 0.384), the overall F-test is still overwhelmingly significant. This is entirely consistent: the F-test only requires that at least one predictor be related to the response, and the other five predictors more than satisfy this requirement. The F-test does not tell us which predictors are significant - that is the job of the individual t-tests. But it does tell us that the model as a whole is capturing real patterns.",
    "crumbs": [
      "Linear Regression Model",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quntitative Linear Regression</span>"
    ]
  },
  {
    "objectID": "02_01_quantitative_linear_regression.html#interaction-terms",
    "href": "02_01_quantitative_linear_regression.html#interaction-terms",
    "title": "2  Quntitative Linear Regression",
    "section": "2.4 Interaction Terms",
    "text": "2.4 Interaction Terms\nUp to this point, all the linear regression models we have discussed have assumed something that may seem natural but is in fact a very strong assumption: that the effect of each predictor on the response is independent of the values of the other predictors. This is called the additivity assumption, and it is built into the standard multiple linear regression model. When we write our model as \\(exam_score = \\beta_0 + \\beta_1 \\times hours_studied + \\beta_2 \\times attendance + \\epsilon\\), we are implicitly saying that the effect of studying one additional hour is always the same - roughly 0.29 additional points - regardless of whether a student has 60% attendance or 100% attendance. Similarly, we are saying that the effect of one additional percentage point of attendance is always the same, regardless of how many hours the student studies. The model treats each predictor’s contribution as completely separate and simply adds them together, which is where the term “additive” comes from.\nBut is this assumption realistic? Consider the following scenario. A student who attends nearly all classes has been exposed to lectures, discussions, and in-class explanations throughout the course. When this student sits down to study at home, each hour of studying is highly productive, because the student is reinforcing and deepening material they have already encountered in the classroom. Now consider a student who has very low attendance and has missed most of the lectures. When this student tries to study, each hour of studying may be less productive, because the student must learn the material from scratch rather than building on what was covered in class. In this scenario, the effect of study hours on exam scores depends on attendance - studying is more effective for students who also attend class regularly. This is exactly the kind of phenomenon that the additive model cannot capture, and it is what we call an interaction effect. In statistics, an interaction effect occurs when the relationship between one predictor and the response changes depending on the value of another predictor.\nTo incorporate an interaction effect into a linear regression model, we create a new predictor variable that is the product of two existing predictors. Standard additive model with two predictors assumes that the effect of \\(X_1\\) on Y is \\(\\beta_1\\), regardless of the value of \\(X_2\\). To relax this assumption, we add a third term - the interaction term - which is simply the product \\(X_1 \\times X_2\\):\n\\(Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3(X_1 \\times X_2) + \\epsilon\\)\nThe key to understanding how this works is to rearrange the equation algebraically. We can rewrite it as:\n\\(Y = \\beta_0 + (\\beta_1 + \\beta_3X_2)X_1 + \\beta_2X_2 + \\epsilon\\)\nWritten in this form, it becomes clear that the effective slope of \\(X_1\\) is no longer a fixed constant \\(\\beta_1\\) - it is now \\((\\beta_1 + \\beta_3X_2)\\), which depends on the value of \\(X_2\\). In other words, the effect of \\(X_1\\) on Y changes as \\(X_2\\) changes. If \\(\\beta_3\\) is positive, then higher values of \\(X_2\\) amplify the effect of \\(X_1\\). If \\(\\beta_3\\) is negative, then higher values of \\(X_2\\) diminish the effect of \\(X_1\\). And if \\(\\beta_3\\) is zero, then the interaction is absent and we are back to the simple additive model.\nThe same rearrangement works in the other direction. We can also write the model as:\n\\(Y = \\beta_0 + \\beta_1X_1 + (\\beta_2 + \\beta_3X_1)X_2 + \\epsilon\\)\nThis shows that the effect of \\(X_2\\) on Y is \\((\\beta_2 + \\beta_3X_1)\\), which depends on \\(X_1\\). The interaction is symmetric: if the effect of study hours depends on attendance, then equally the effect of attendance depends on study hours. The interaction coefficient \\(\\beta_3\\) captures this mutual dependence.\nIn our Student Performance example, an interaction between Hours_Studied and Attendance would be expressed as:\nexam_score = \\(\\beta_0\\) + \\(\\beta_1\\) \\(\\times\\) hours_studied + \\(\\beta_2\\) \\(\\times\\) attendance + \\(\\beta_3\\) \\(\\times\\) (hours_studied \\(\\times\\) attendance) + \\(\\epsilon\\)\nThe coefficient \\(\\beta_3\\) would tell us how the effectiveness of studying changes as attendance increases (or equivalently, how the effectiveness of attendance changes as study hours increase). If \\(\\beta_3\\) is positive and significant, it would confirm our intuition that studying and attending class work together synergistically - each one makes the other more effective.\nIn R, there are two convenient ways to specify interaction terms. The syntax hours_studied:attendance includes only the interaction term itself, while the syntax hours_studied * attendance is a shorthand that automatically includes both the individual predictors (called main effects) and their interaction. In other words, hours_studied * attendance is equivalent to writing hours_studied + attendance + hours_studied:attendance.\nWe have now fitted three models that allow us to progressively examine whether an interaction exists between “hours_studied” and “attendance” in predicting “exam_score”. The results tell a clear and instructive story - one that is just as valuable for what it does not find as for what it does find.\n\nadditive_model &lt;- lm(\n    exam_score ~ hours_studied + attendance,\n    data = student_performance\n)\n\nsummary(additive_model)\n\n\nCall:\nlm(formula = exam_score ~ hours_studied + attendance, data = student_performance)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.0523 -1.3295 -0.1674  1.0310 31.5633 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   45.604170   0.252314  180.74   &lt;2e-16 ***\nhours_studied  0.293058   0.005413   54.14   &lt;2e-16 ***\nattendance     0.197275   0.002808   70.25   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.635 on 6604 degrees of freedom\nMultiple R-squared:  0.5413,    Adjusted R-squared:  0.5411 \nF-statistic:  3896 on 2 and 6604 DF,  p-value: &lt; 2.2e-16\n\n\nOur additive model serves as the baseline. It estimates the following equation:\nexam_score ≈ 45.60 + 0.293 \\(\\times\\) hours_studied + 0.197 \\(\\times\\) attendance\nIn this model, the effect of each additional hour of study is always 0.293 points, regardless of how often the student attends class. And the effect of each additional percentage point of attendance is always 0.197 points, regardless of how many hours the student studies. The two predictors operate independently - their contributions are simply added together. The model explains 54.13% of the variance in exam scores (\\(R^2\\) = 0.5413), with a residual standard error of 2.635. This additive interpretation may or may not reflect reality. It is possible that studying and attending class reinforce each other - that the benefit of studying is amplified when a student has also been attending lectures regularly. The interaction model tests this possibility directly.\n\ninteraction_model &lt;- lm(\n    exam_score ~ hours_studied * attendance,\n    data = student_performance\n)\nsummary(interaction_model)\n\n\nCall:\nlm(formula = exam_score ~ hours_studied * attendance, data = student_performance)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.0118 -1.3290 -0.1687  1.0450 31.5866 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              4.693e+01  7.788e-01  60.253  &lt; 2e-16 ***\nhours_studied            2.266e-01  3.742e-02   6.055 1.49e-09 ***\nattendance               1.808e-01  9.624e-03  18.781  &lt; 2e-16 ***\nhours_studied:attendance 8.308e-04  4.628e-04   1.795   0.0727 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.635 on 6603 degrees of freedom\nMultiple R-squared:  0.5415,    Adjusted R-squared:  0.5413 \nF-statistic:  2599 on 3 and 6603 DF,  p-value: &lt; 2.2e-16\n\nconfint(interaction_model)\n\n                                 2.5 %       97.5 %\n(Intercept)               4.539998e+01 48.453506206\nhours_studied             1.532255e-01  0.299954598\nattendance                1.618848e-01  0.199618365\nhours_studied:attendance -7.655732e-05  0.001738105\n\n\nThe interaction model adds the product of Hours_Studied and Attendance as a new predictor. The estimated equation is:\nexam_score \\(\\approx\\) 46.93 + 0.227 \\(\\times\\) hours_studied + 0.181 \\(\\times\\) attendance + 0.000831 \\(\\times\\) (hours_studied \\(\\times\\) attendance)\nLet us carefully examine each coefficient and what it means in the context of this model, because the interpretation of coefficients changes fundamentally when an interaction term is present. The intercept of 46.93 represents the predicted exam score when both “hours_studied” and “attendance” are zero. As always, this is a mathematical anchor point for the model rather than a substantively meaningful quantity, since no real student has zero hours of study and zero percent attendance. The coefficient for “hours_studied” is now 0.227, which is noticeably different from its value in the additive model (0.293). However, the interpretation of this coefficient has changed. In the additive model, 0.293 represented the effect of study hours at any level of attendance.\nIn the interaction model, 0.227 represents the effect of study hours specifically when “attendance” equals zero. This is because when we rearrange the interaction model as exam_score = 46.93 + (0.227 + 0.000831 \\(\\times\\) attendance) \\(\\times\\) hours_studied + 0.181 \\(\\times\\) attendance we can see that the effective slope for “hours_studied” is (0.227 + 0.000831 \\(\\times\\) attendance). When “attendance” is zero, this reduces to 0.227. When “attendance” is at its mean of about 80, the effective slope becomes 0.227 + 0.000831 × 80 = 0.227 + 0.066 = 0.293 - which is almost exactly the slope we found in the additive model. This is reassuring and makes intuitive sense: the additive model’s coefficient represents a kind of average effect across all attendance levels, and that average coincides with the effective slope at the mean level of attendance.\nSimilarly, the coefficient for “attendance” is 0.181, which represents the effect of attendance when “hours_studied” equals zero. The effective slope for “attendance” at the mean study hours of about 20 is 0.181 + 0.000831 \\(\\times\\) 20 = 0.181 + 0.017 = 0.198 - again, essentially the same as in the additive model.\nThe interaction coefficient itself is 0.000831. This is the critical number. It tells us how the effect of one predictor changes for each one-unit increase in the other. Specifically, for each additional percentage point of attendance, the effect of one hour of study increases by 0.000831 points. Conversely, for each additional hour of study, the effect of one percentage point of attendance increases by 0.000831 points. In concrete terms, this means that a student with 90% attendance gains 0.227 + 0.000831 \\(\\times\\) 90 = 0.302 points per hour of study, while a student with 70% attendance gains 0.227 + 0.000831 \\(\\times\\) 70 = 0.285 points per hour of study. The difference is 0.017 points per hour - a very small amount.\nNow, the crucial question is whether this interaction effect is statistically significant. The t-statistic for the interaction term is 1.795, and the p-value is 0.0727. This p-value is above the conventional 0.05 significance threshold, although it is below 0.10. The 95% confidence interval for the interaction coefficient is [-0.0000766, 0.001738], which includes zero. This means that at the 5% significance level, we cannot reject the null hypothesis that the interaction coefficient is zero. The evidence for an interaction is suggestive but not strong enough to meet the conventional standard of statistical significance.\nLooking at the model-level statistics reinforces this conclusion. The R² of the interaction model is 0.5415, compared to 0.5413 for the additive model. The increase is only 0.0002 - an almost imperceptible improvement. The residual standard error remains at 2.635, completely unchanged. Adding the interaction term has contributed virtually nothing to the model’s explanatory power.\n\nfull_interaction_model &lt;- lm(\n    exam_score ~ hours_studied * attendance + previous_scores + tutoring_sessions + physical_activity,\n    data = student_performance\n)\nsummary(full_interaction_model)\n\n\nCall:\nlm(formula = exam_score ~ hours_studied * attendance + previous_scores + \n    tutoring_sessions + physical_activity, data = student_performance)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.4580 -1.1344 -0.1632  0.8570 31.0587 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              4.181e+01  7.570e-01  55.238  &lt; 2e-16 ***\nhours_studied            2.406e-01  3.504e-02   6.867 7.16e-12 ***\nattendance               1.854e-01  9.014e-03  20.563  &lt; 2e-16 ***\nprevious_scores          4.810e-02  2.109e-03  22.804  &lt; 2e-16 ***\ntutoring_sessions        4.938e-01  2.467e-02  20.014  &lt; 2e-16 ***\nphysical_activity        1.436e-01  2.945e-02   4.875 1.11e-06 ***\nhours_studied:attendance 6.363e-04  4.334e-04   1.468    0.142    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.467 on 6600 degrees of freedom\nMultiple R-squared:  0.5983,    Adjusted R-squared:  0.598 \nF-statistic:  1639 on 6 and 6600 DF,  p-value: &lt; 2.2e-16\n\nconfint(full_interaction_model)\n\n                                 2.5 %       97.5 %\n(Intercept)              40.3298287765 43.297645958\nhours_studied             0.1719352716  0.309320955\nattendance                0.1676866849  0.203027537\nprevious_scores           0.0439690360  0.052239444\ntutoring_sessions         0.4454663363  0.542206078\nphysical_activity         0.0858242758  0.201276450\nhours_studied:attendance -0.0002133552  0.001485881\n\n\nOur third model includes the interaction between Hours_Studied and Attendance alongside three additional predictors: “previous_scores”, “tutoring_sessions”, and “physical_activity”. The estimated equation is:\nexam_score \\(\\approx\\) 41.81 + 0.241 \\(\\times\\) hours_studied + 0.185 \\(\\times\\) attendance + 0.048 \\(\\times\\) previous_scores + 0.494 \\(\\times\\) tutoring_sessions + 0.144 \\(\\times\\) physical_activity + 0.000636 \\(\\times\\) (hours_studied \\(\\times\\) attendance)\nIn this richer model, the interaction coefficient has shrunk further to 0.000636, the t-statistic has decreased to 1.468, and the p-value has increased to 0.142. The 95% confidence interval is [-0.000213, 0.001486], which now spans zero even more broadly than before. The interaction effect is clearly not statistically significant in this model, and its magnitude is even smaller than in Model 2.\nThe \\(R^2\\) of this full interaction model is 0.5983, compared to 0.5982 for the multiple regression model without the interaction term that we fitted earlier. The difference is 0.0001 — the interaction term adds essentially no explanatory power beyond what is already captured by the main effects and the other predictors. The residual standard error is 2.467, identical to the model without the interaction.\nThe anova() function at the end performs a formal comparison between the additive model and the interaction model. It tests whether the addition of the interaction term leads to a statistically significant improvement in model fit, using an F-test that compares the RSS of the two models. If the interaction term significantly reduces the RSS, the F-statistic will be large and the p-value will be small, indicating that the interaction is a meaningful addition to the model.\n\nanova(additive_model, interaction_model)\n\nAnalysis of Variance Table\n\nModel 1: exam_score ~ hours_studied + attendance\nModel 2: exam_score ~ hours_studied * attendance\n  Res.Df   RSS Df Sum of Sq      F  Pr(&gt;F)  \n1   6604 45868                              \n2   6603 45846  1    22.369 3.2217 0.07271 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe ANOVA output provides a formal statistical test that directly compares our two nested models: Model 1, the additive model with only “hours_studied” and “attendance” as separate predictors, and Model 2, the interaction model that adds the hours_studied \\(\\times\\) “attendance” interaction term. The word “nested” is important here - it means that Model 1 is a special case of Model 2, obtained by setting the interaction coefficient to zero. The ANOVA test asks whether allowing that coefficient to be non-zero produces a meaningfully better fit to the data.\nThe output shows us the residual degrees of freedom and the residual sum of squares (RSS) for each model. Model 1 has 6,604 residual degrees of freedom and an RSS of 45,868. Model 2 has 6,603 residual degrees of freedom and an RSS of 45,846. The difference in RSS between the two models is 22.369, which represents the amount of additional variance in exam scores that is explained by including the interaction term. The column labeled “Df” shows that the difference is 1 degree of freedom, confirming that exactly one additional parameter was added.\nThe F-statistic for this comparison is 3.2217. Recall from our earlier discussion that the F-statistic is constructed by comparing the improvement in fit (the reduction in RSS) to the amount of variance that remains unexplained. Specifically, it takes the reduction in RSS per additional parameter (22.369 / 1 = 22.369) and divides it by the residual mean square of the fuller model (45,846 / 6,603 \\(\\approx\\) 6.943). The resulting F-value of 3.22 tells us that the interaction term reduced the RSS by about 3.22 times more than what a single random, useless predictor would be expected to reduce it. This is a modest improvement - certainly not trivial, but not overwhelming either.\nThe p-value is 0.07271, which is shown with a single dot (.) next to it in R’s significance coding system, indicating that it falls between 0.05 and 0.10. This p-value means that if the true interaction coefficient were zero (that is, if there were truly no interaction between studying and attendance in the population), there would be approximately a 7.3% probability of observing an improvement in fit as large as or larger than the one we found. At the conventional 5% significance level, we do not reject the null hypothesis - the interaction term does not provide a statistically significant improvement in model fit. The evidence is suggestive, sitting in that ambiguous zone between 0.05 and 0.10, but it does not meet the standard threshold for statistical significance.\nIt is worth noting how perfectly consistent this ANOVA result is with the individual coefficient test for the interaction term that we saw in Model 2’s summary output. The t-statistic for the interaction coefficient was 1.795, and its p-value was 0.0727 - virtually identical to the ANOVA p-value. This is not a coincidence. When we compare two models that differ by exactly one predictor, the ANOVA F-statistic is exactly the square of the t-statistic for that predictor (\\(1.795^2\\) \\(\\approx\\) 3.222), and the p-values are identical. The two tests are mathematically equivalent in this case. The ANOVA approach becomes particularly valuable when we want to compare models that differ by more than one predictor - for instance, if we wanted to test whether a whole set of interaction terms simultaneously improves the model, we could not do this with individual t-tests, but we could do it with a single ANOVA F-test.\nTo put the practical magnitude of this result in perspective, the interaction term reduced the RSS from 45,868 to 45,846 - a reduction of just 22.369 out of a total RSS of 45,868, which amounts to a 0.049% reduction in unexplained variance. The \\(R^2\\) increased from 0.5413 to 0.5415, a gain of 0.0002. These numbers confirm what the p-value already suggested: even if the interaction were real, its practical importance would be negligible. The additive model, which is simpler and easier to interpret, captures the relationships between study hours, attendance, and exam scores just as effectively as the interaction model does. For this reason, both on statistical grounds and on grounds of parsimony, we would choose to proceed with the additive model and conclude that “hours_studied” and “attendance” contribute independently to student performance in this dataset.\nThe decision to include interaction terms should be guided by both theory and evidence. From a theoretical standpoint, we should include interactions when we have a substantive reason to believe that the effect of one predictor depends on the value of another. In educational research, for example, we might hypothesize that the effect of tutoring depends on motivation level (highly motivated students may benefit more from tutoring), or that the effect of parental involvement depends on family income (parental involvement may matter more in lower-income families where other resources are scarce). These are theoretically grounded hypotheses that deserve to be tested.\nFrom an empirical standpoint, we should look for evidence in the data - specifically, a significant interaction coefficient with a meaningful magnitude. In our case, the evidence does not support the interaction between “hours_studied” and “attendance”: the coefficient is tiny, the p-value exceeds 0.05, the confidence interval includes zero, and the \\(R^2\\) improvement is negligible. The additive model is therefore preferred on grounds of parsimony - it is simpler, easier to interpret, and fits the data just as well. Including a non-significant interaction term would add unnecessary complexity to the model without any compensating benefit in explanatory power or predictive accuracy.",
    "crumbs": [
      "Linear Regression Model",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quntitative Linear Regression</span>"
    ]
  },
  {
    "objectID": "02_01_quantitative_linear_regression.html#hierarchical-linear-regression",
    "href": "02_01_quantitative_linear_regression.html#hierarchical-linear-regression",
    "title": "2  Quntitative Linear Regression",
    "section": "2.5 Hierarchical Linear Regression",
    "text": "2.5 Hierarchical Linear Regression\nIn all the models we have fitted so far, we have made decisions about which predictors to include based on theoretical reasoning and then examined the results as a single, complete model. But in social science research, we are often interested in something more nuanced than simply knowing which predictors are significant. We want to understand how different groups of factors contribute to the outcome, and specifically, whether adding a new group of factors improves our ability to explain the response above and beyond what was already explained by previously entered factors. This is the core logic behind hierarchical regression, also known as sequential regression or blockwise regression.\nHierarchical regression or sequential regression is not a fundamentally different statistical technique from the multiple linear regression we have already discussed. It uses exactly the same least squares estimation, the same coefficient estimates, the same standard errors, and the same t-statistics. What makes it distinctive is the strategy for entering predictors into the model. Rather than entering all predictors at once, the researcher builds the model in a series of deliberate steps - called blocks or stages - adding one group of theoretically related predictors at each step. After each block is added, the researcher examines how the model’s explanatory power changes, paying particular attention to the change in \\(R^2\\) (denoted \\(\\Delta R^2\\)). This approach allows us to ask questions like: how much additional variance in exam scores do study habits explain, after we have already accounted for students’ baseline academic ability? Or: does the school environment contribute anything meaningful once we already know about students’ personal characteristics and study behavior?\nThe order in which blocks are entered is not arbitrary - it should be guided by theory, prior research, or the specific research questions being investigated. Typically, researchers enter more fundamental, stable, or demographic variables first, and then add variables that are more proximal, malleable, or of primary theoretical interest in subsequent blocks. The rationale is that by entering background variables first, we establish a baseline, and then we can see whether the variables we are most interested in contribute explanatory power beyond what those background factors already provide.\nFor our student_performance dataset, a theoretically motivated hierarchical regression might proceed as follows. In the first block, we would enter a variable that captures students’ baseline academic ability - “previous_scores”. This is the most fundamental predictor, as it reflects everything a student brings to the table before the current course even begins: their prior knowledge, their learning capacity, and their historical academic trajectory. By entering this first, we establish how much of the variation in exam scores is explained simply by pre-existing differences in ability.\nIn the second block, we would add variables related to study behavior and engagement - “hours_studied” and “attendance”. These represent the deliberate efforts a student makes during the course. The key question at this stage is: do study habits and class attendance explain additional variance in exam scores above and beyond what is already explained by baseline ability? If \\(\\Delta R^2\\) is large and significant at this step, it tells us that what students do during the course matters over and above what they could do coming in.\nIn the third block, we would add variables related to academic support - “tutoring_sessions”. This captures the additional help a student receives outside of regular classes and personal study. The question here is whether external academic support adds anything once we already know about baseline ability and personal effort.\nIn the fourth and final block, we would add variables related to lifestyle and well-being - “sleep_hours” and “physical_activity”. These are factors that are somewhat more distal from academic performance, and we are interested in whether they contribute any additional explanatory power after the more directly academic factors have already been accounted for.\nIn R, hierarchical regression is implemented by fitting a series of separate lm() models, each adding a new block of predictors to the previous model. We then compare the models using the anova() function, which performs an F-test for the significance of the improvement at each step, and we manually compute the change in \\(R^2\\) between steps.\n\nblock1 &lt;- lm(\n    exam_score ~ previous_scores,\n    data = student_performance\n)\n\nblock2 &lt;- lm(\n    exam_score ~ previous_scores + hours_studied + attendance,\n    data = student_performance\n)\n\nblock3 &lt;- lm(\n    exam_score ~ previous_scores + hours_studied + attendance + tutoring_sessions,\n    data = student_performance\n)\n\nblock4 &lt;- lm(\n    exam_score ~ previous_scores + hours_studied + attendance + tutoring_sessions + sleep_hours + physical_activity,\n    data = student_performance\n)\n\nanova(block1, block2, block3, block4)\n\nAnalysis of Variance Table\n\nModel 1: exam_score ~ previous_scores\nModel 2: exam_score ~ previous_scores + hours_studied + attendance\nModel 3: exam_score ~ previous_scores + hours_studied + attendance + tutoring_sessions\nModel 4: exam_score ~ previous_scores + hours_studied + attendance + tutoring_sessions + \n    sleep_hours + physical_activity\n  Res.Df   RSS Df Sum of Sq        F    Pr(&gt;F)    \n1   6605 96921                                    \n2   6603 42779  2     54143 4447.924 &lt; 2.2e-16 ***\n3   6602 40320  1      2459  404.023 &lt; 2.2e-16 ***\n4   6600 40169  2       150   12.338 4.484e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(block1)$r.squared\n\n[1] 0.03065269\n\nsummary(block2)$r.squared\n\n[1] 0.5721542\n\nsummary(block3)$r.squared\n\n[1] 0.5967476\n\nsummary(block4)$r.squared\n\n[1] 0.5982496\n\n\nThe first block contains only “previous_scores”. This variable serves as our baseline, capturing the academic ability and preparation that each student brings into the current course before any of the other factors come into play. The \\(R^2\\) for this model is 0.0307, meaning that previous academic performance explains only about 3.07% of the variation in current exam scores. This is a notably small number, and it deserves careful interpretation. One might have expected prior scores to be a powerful predictor of current performance - after all, students who did well in the past tend to do well again. And indeed, if we look back at our earlier multiple regression results, the coefficient for “previous_scores” was statistically significant with a t-value of 22.81. But statistical significance and explanatory power are different things. The coefficient for “previous_scores” was 0.048, meaning that a 10-point advantage in prior scores translates to less than half a point on the current exam. The relationship is real but modest in magnitude, and the variable by itself leaves approximately 97% of the variation in exam scores unexplained. This tells us something substantively important: in this dataset, what a student has done before is a relatively weak predictor of what they will achieve now. The current course’s exam score is primarily determined by factors other than historical performance.\nIn the second block, we add “hours_studied” and “attendance”. The \\(R^2\\) jumps dramatically from 0.0307 to 0.5722. The change in \\(R^2\\), \\(\\Delta R^2\\) = 0.5722 − 0.0307 = 0.5415, is enormous. Adding study hours and attendance to the model increases the explained variance by 54.15 percentage points. This is by far the largest improvement at any step in our hierarchical analysis, and it fundamentally transforms the model from one that explains virtually nothing to one that explains more than half of the total variation. This result carries a profound substantive message. It tells us that the single most important category of factors for predicting exam scores is not what students were capable of before the course, but what they actively do during it - how many hours they invest in studying and how consistently they show up to class. The effort and engagement variables dwarf baseline ability in their explanatory contribution. For educators and policymakers, this is an optimistic finding: it suggests that student performance is more about current behavior than about fixed, pre-existing ability. Students who attend regularly and study diligently tend to perform well, regardless of their prior academic record. It is worth reflecting on why the \\(\\Delta R^2\\) is so much larger than the Block 1 \\(R^2\\). One reason is that “hours_studied” and “attendance” have relatively large coefficients (approximately 0.29 and 0.20 per unit, respectively) and substantial variability across students. Another reason is that these two variables together capture two distinct and complementary dimensions of student effort - in-class engagement and out-of-class preparation - which together account for a broad swath of the academic experience.\nIn the third block, we add “tutoring_sessions”. The \\(R^2\\) increases from 0.5722 to 0.5967, yielding a \\(\\Delta R^2\\) of 0.0246. This means that tutoring sessions explain an additional 2.46 percentage points of variance in exam scores, above and beyond what is already explained by prior scores, study hours, and attendance. While 2.46% may seem small compared to the massive 54.15% gain in Block 2, it is important to interpret this number in context. By Block 3, we have already accounted for the major sources of variation - the “low-hanging fruit” has been picked, so to speak. The remaining unexplained variance at the end of Block 2 was about 42.78% (since 100% − 57.22% = 42.78%). Of this remaining unexplained variance, tutoring sessions account for 2.46 / 42.78 = about 5.75%. So among the factors not yet captured by the model, tutoring makes a meaningful - though not dominant - contribution. This makes theoretical sense: tutoring provides targeted academic support that goes beyond what students gain from attending lectures and studying on their own. It represents an additional layer of help that can address specific gaps in understanding, provide personalized feedback, and reinforce difficult concepts.\nIn the fourth and final block, we add “sleep_hours” and “physical_activity”. The \\(R^2\\) increases from 0.5967 to 0.5982, yielding a \\(\\Delta R^2\\) of 0.0015. This is a very small increment - lifestyle factors explain only about 0.15 additional percentage points of variance in exam scores after all the academic variables have been accounted for. This finding tells us that once we know how a student performed previously, how much they study, how often they attend class, and how many tutoring sessions they receive, knowing about their sleep habits and physical exercise patterns adds almost nothing to our ability to predict their exam score. This does not necessarily mean that sleep and exercise are unimportant for well-being or general cognitive function - there is extensive research suggesting they are. But in terms of their incremental contribution to predicting exam scores specifically, over and above the academic and effort variables, their contribution is negligible. It is also worth recalling that in our earlier multiple regression analysis, “sleep_hours” was not statistically significant (p = 0.384), while “physical_activity” was significant but had a small coefficient (0.144). The hierarchical analysis confirms that these lifestyle variables are the least important block in our model.\nThe ANOVA table provides formal F-tests for the significance of each block’s contribution. Let me explain each row. The first row shows Model 1 (Block 1) with 6,605 residual degrees of freedom and an RSS of 96,921. This serves as the starting point. No comparison is made yet because there is no preceding model. The second row compares Model 2 to Model 1. The difference is 2 degrees of freedom (because we added two predictors: “hours_studied” and “attendance”), and the reduction in RSS is 54,143. This is a massive reduction - the unexplained variance was cut by more than half. The F-statistic is 4,447.92 and the p-value is less than \\(2.2 \\times 10{-16}\\). This F-statistic is extraordinarily large, providing overwhelming evidence that adding study behavior and engagement variables produces a significant improvement in the model. To understand the F-statistic intuitively, the numerator of the F-test takes the reduction in RSS per added predictor (54,143 / 2 = 27,071.5) and divides it by the residual mean square of the fuller model (approximately 6.09). The ratio of 4,448 means that each of the two new predictors reduced the RSS by roughly 4,448 times more than what a useless random predictor would be expected to contribute. This is exceptionally strong evidence. The third row compares Model 3 to Model 2. Adding Tutoring_Sessions (1 degree of freedom) reduced the RSS by 2,459. The F-statistic is 404.02 with a p-value less than \\(2.2 \\times 10^{-16}\\). This is also highly significant, confirming that tutoring sessions provide a meaningful improvement in the model even after study hours and attendance are already included. The F-statistic of 404 is smaller than the 4,448 for Block 2, which makes sense - tutoring contributes less than study behavior, but its contribution is still far beyond what chance alone could produce. The fourth row compares Model 4 to Model 3. Adding “sleep_hours” and “physical_activity” (2 degrees of freedom) reduced the RSS by only 150. The F-statistic is 12.34 with a p-value of \\(4.484 \\times 10^{10-6}\\). Although this p-value is well below 0.05 and therefore statistically significant, the magnitude of the improvement is tiny compared to the earlier blocks. The F-statistic of 12.34, while significant, is orders of magnitude smaller than the F-statistics for Blocks 2 and 3. The RSS decreased from 40,320 to 40,169 - a reduction of less than 0.4%. This confirms that lifestyle factors achieve statistical significance (largely thanks to Physical_Activity, as we know Sleep_Hours alone is not significant), but their practical contribution to explaining exam scores is minimal.",
    "crumbs": [
      "Linear Regression Model",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quntitative Linear Regression</span>"
    ]
  },
  {
    "objectID": "02_02_qualitative_linear_regression.html",
    "href": "02_02_qualitative_linear_regression.html",
    "title": "3  Qualitative Linear Regression",
    "section": "",
    "text": "All the models we have built so far have used only quantitative predictors - variables measured on a numerical scale, such as hours_studied, attendance, and previous_scores. But many of the most important variables in social science research are qualitative, meaning they represent categories rather than numbers. In our student_performance dataset, we have a rich set of qualitative variables: gender (Male or Female), school_type (Public or Private), parental_involvement (Low, Medium, or High), motivation_level (Low, Medium, or High), and several others. These variables clearly have the potential to influence exam scores, but we cannot simply plug them into a regression equation as they are. The equation \\(\\text{exam_score} = \\beta_0 + \\beta_1 \\times gender\\) does not make mathematical sense when gender takes the values “Male” and “Female” - we cannot multiply a number by a word. We need a way to translate qualitative categories into a numerical form that linear regression can work with. This is accomplished through the use of dummy variables.\nA dummy variable (also called an indicator variable) is a numerical variable that takes only the values 0 and 1, where 1 indicates membership in a particular category and 0 indicates non-membership. The key principle is straightforward: for a qualitative predictor with two levels (categories), we create one dummy variable; for a predictor with three levels, we create two dummy variables; and in general, for a predictor with k levels, we create \\(k − 1\\) dummy variables. The category that does not receive its own dummy variable is called the baseline category, and all the other categories are compared against it.\nLet us begin with the simplest case. gender in our dataset has two levels: male and female. To include gender in a regression, we create a single dummy variable. R does this automatically, but conceptually it works as follows: \\(x = 1\\) if the student is male, and \\(x = 0\\) if the student is female.With this coding, our regression model becomes:\n\\(\\text{exam_score} = \\beta_0 + \\beta_1x + \\epsilon\\)\nWhen \\(x = 0\\) (female students), the model reduces to \\(\\text{exam_score} = \\beta_0 + \\epsilon\\), so \\(\\beta_0\\) represents the average exam score for female students. When \\(x = 1\\) (male students), the model becomes \\(\\text{exam_score} = \\beta_0 + \\beta_1 + \\epsilon\\), so \\(\\beta_0 + \\beta_1\\) represents the average exam score for male students. Therefore, \\(\\beta_1\\) represents the difference in average exam scores between male and female students. If \\(\\beta_1\\) is positive, males score higher on average; if negative, females score higher; and if \\(\\beta_1\\) is zero, there is no difference between the groups. The hypothesis test for \\(\\beta_1\\) directly tests whether this difference is statistically significant.\nThe choice of which category serves as the baseline is arbitrary from a mathematical standpoint - it does not affect the model’s predictions or its overall fit. If we had coded female as 1 and male as 0, the intercept would represent the male average and the slope would have the opposite sign, but the predicted values for each group would be exactly the same. R automatically selects the baseline category alphabetically, so female becomes the baseline for gender, and the output will show a coefficient labeled gender male representing the difference relative to females.\n\nmodel_gender &lt;- lm(\n    exam_score ~ gender,\n    data = student_performance\n)\n\nsummary(model_gender)\n\n\nCall:\nlm(formula = exam_score ~ gender, data = student_performance)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-12.229  -2.245  -0.229   1.771  33.755 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 67.24490    0.07362 913.403   &lt;2e-16 ***\ngenderMale  -0.01600    0.09690  -0.165    0.869    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.891 on 6605 degrees of freedom\nMultiple R-squared:  4.13e-06,  Adjusted R-squared:  -0.0001473 \nF-statistic: 0.02728 on 1 and 6605 DF,  p-value: 0.8688\n\n\nNow consider a qualitative predictor with three levels. parental_involvement has the categories Low, Medium, and High. To encode this variable, we need two dummy variables - one fewer than the number of categories. R creates them as follows:\n\n\\(x_1\\) = 1 if parental_involvement is Medium, 0 otherwise and\n\\(x_2\\) = 1 if parental_involvement is High, 0 otherwise.\n\nWhen both \\(x_1\\) = 0 and \\(x_2\\) = 0, the student has Low parental involvement — this is the baseline category. The regression model becomes:\n\\(\\text{exam_score} = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\epsilon\\)\nFor students with Low involvement: \\(\\text{exam_score} = \\beta_0 + \\epsilon\\), so \\(\\beta_0\\) is the average score for the Low group. For students with Medium involvement: \\(\\text{exam_score} = \\beta_0 + \\beta_1 + \\epsilon\\), so \\(\\beta_1\\) is the difference in average scores between the Medium and Low groups. For students with High involvement: \\(\\text{exam_score} = \\beta_0 + \\beta_2 + \\epsilon\\), so \\(\\beta_2\\) is the difference in average scores between the High and Low groups. Each coefficient represents a comparison against the baseline category - this is a crucial point for interpretation. We are not estimating absolute average scores for each group; we are estimating differences relative to the reference group.\nOne important consequence of this coding scheme is that the p-value for each dummy variable tests whether that specific category differs significantly from the baseline category, but it does not directly test whether the qualitative variable as a whole is significant. For example, it is possible that neither Medium nor High individually differs significantly from Low, yet the variable as a whole still has a significant overall effect.\nTo test the overall significance of a qualitative predictor, we need to use the F-test that compares a model with the variable included to a model without it. We will see this in practice with our results.\n\nmodel_parental &lt;- lm(\n    exam_score ~ parental_involvement,\n    data = student_performance\n)\n\nsummary(model_parental)\n\n\nCall:\nlm(formula = exam_score ~ parental_involvement, data = student_performance)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-12.098  -2.098  -0.098   1.907  34.642 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                68.09277    0.08796 774.123   &lt;2e-16 ***\nparental_involvementLow    -1.73450    0.13704 -12.657   &lt;2e-16 ***\nparental_involvementMedium -0.99461    0.11013  -9.031   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.842 on 6604 degrees of freedom\nMultiple R-squared:  0.02495,   Adjusted R-squared:  0.02465 \nF-statistic: 84.49 on 2 and 6604 DF,  p-value: &lt; 2.2e-16\n\nmodel_motivation &lt;- lm(\n    exam_score ~ motivation_level,\n    data = student_performance\n)\n\nsummary(model_motivation)\n\n\nCall:\nlm(formula = exam_score ~ motivation_level, data = student_performance)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-12.331  -2.331  -0.331   2.248  34.248 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             67.7043     0.1067 634.391  &lt; 2e-16 ***\nmotivation_levelLow     -0.9521     0.1384  -6.881 6.49e-12 ***\nmotivation_levelMedium  -0.3737     0.1260  -2.966  0.00303 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.876 on 6604 degrees of freedom\nMultiple R-squared:  0.007728,  Adjusted R-squared:  0.007428 \nF-statistic: 25.72 on 2 and 6604 DF,  p-value: 7.492e-12\n\nmodel_combined &lt;- lm(\n    exam_score ~ hours_studied + attendance + previous_scores + tutoring_sessions + gender + parental_involvement + motivation_level + school_type,\n    data = student_performance\n)\n\nsummary(model_combined)\n\n\nCall:\nlm(formula = exam_score ~ hours_studied + attendance + previous_scores + \n    tutoring_sessions + gender + parental_involvement + motivation_level + \n    school_type, data = student_performance)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-4.458 -0.967 -0.138  0.678 31.787 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                42.594542   0.288356 147.715  &lt; 2e-16 ***\nhours_studied               0.294074   0.004825  60.945  &lt; 2e-16 ***\nattendance                  0.198441   0.002503  79.282  &lt; 2e-16 ***\nprevious_scores             0.048870   0.002008  24.338  &lt; 2e-16 ***\ntutoring_sessions           0.496782   0.023481  21.157  &lt; 2e-16 ***\ngenderMale                 -0.018845   0.058490  -0.322    0.747    \nparental_involvementLow    -1.962245   0.083800 -23.416  &lt; 2e-16 ***\nparental_involvementMedium -1.027930   0.067318 -15.270  &lt; 2e-16 ***\nmotivation_levelLow        -1.083139   0.083848 -12.918  &lt; 2e-16 ***\nmotivation_levelMedium     -0.522208   0.076362  -6.839 8.71e-12 ***\nschool_typePublic          -0.001743   0.062818  -0.028    0.978    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.347 on 6596 degrees of freedom\nMultiple R-squared:  0.6365,    Adjusted R-squared:  0.6359 \nF-statistic:  1155 on 10 and 6596 DF,  p-value: &lt; 2.2e-16\n\n# ============================================================\n# F-tests for overall significance of each qualitative predictor\n# Model without qualitative predictors (quantitative only)\n# ============================================================\nmodel_quant_only &lt;- lm(exam_score ~ hours_studied + attendance + previous_scores + tutoring_sessions, data = student_performance)\nmodel_plus_gender &lt;- lm(exam_score ~ hours_studied + attendance + previous_scores + tutoring_sessions + gender, data = student_performance)\nmodel_plus_parental &lt;- lm(exam_score ~ hours_studied + attendance + previous_scores + tutoring_sessions + parental_involvement, data = student_performance)\nmodel_plus_motivation &lt;- lm(exam_score ~ hours_studied + attendance + previous_scores + tutoring_sessions + motivation_level, data = student_performance)\nmodel_plus_school &lt;- lm(exam_score ~ hours_studied + attendance + previous_scores + tutoring_sessions + school_type, data = student_performance)\nanova(model_quant_only, model_plus_school)\n\nAnalysis of Variance Table\n\nModel 1: exam_score ~ hours_studied + attendance + previous_scores + tutoring_sessions\nModel 2: exam_score ~ hours_studied + attendance + previous_scores + tutoring_sessions + \n    school_type\n  Res.Df   RSS Df Sum of Sq      F Pr(&gt;F)\n1   6602 40320                           \n2   6601 40319  1   0.29711 0.0486 0.8255",
    "crumbs": [
      "Linear Regression Model",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Qualitative Linear Regression</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Field, Andy. 2024. Discovering Statistics Using IBM SPSS Statistics. 6th ed. London, England: SAGE Publications Ltd.\n\n\nGareth James, Trevor Hastie, Daniela Witten. 2021. An Introduction to Statistical Learning: With Applications in r. 2nd ed. Springer Texts in Statistics. New York, NY, USA: Springer New York, NY. https://doi.org/10.1007/978-1-0716-1418-1.",
    "crumbs": [
      "References"
    ]
  }
]