[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Learning in R for Social Sciences",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "introduction_to_statistical_learning.html",
    "href": "introduction_to_statistical_learning.html",
    "title": "1  Introduction to statistical learning",
    "section": "",
    "text": "Statistical learning refers to a broad set of approaches and techniques for estimating the function that connects input variables to an output variable. At its core, statistical learning is concerned with understanding the relationship between variables and using that understanding either to make predictions about future observations or to gain insight into how different factors influence an outcome of interest.\nThe fundamental idea of statistical learning can be expressed through a simple formula:\n\\[\nY = f(X) + ϵ\n\\]\nThis formula tells us that any outcome we wish to study or predict can be understood as the result of some systematic relationship between inputs and outputs, plus some random variation that we cannot fully explain or control. The goal of statistical learning is to estimate the function \\(f\\) based on observed data, so that we can either predict Y for new observations or understand how changes in X are associated with changes in Y.\nLet me now explain each component of this formula in detail.\n\nThe dependent variable, denoted by Y, represents the response that we are trying to understand, explain, or predict. This is the variable whose variation we want to account for using other available information, so we refer to Y as the dependent variable. It is called “dependent” precisely because its values depend on, or are influenced by, other variables in the system we are studying.\nThe independent variable or predictor, denoted by X, represents the input information that we use to explain or predict the outcome Y. In most realistic situations, we have multiple predictors rather than just one, so X typically represents a collection of variables written as \\(X = (X_1, X_2, ..., X_p)\\), where \\(_p\\) indicates the total number of predictors available. The key characteristic of these variables is that they provide information that helps us understand or anticipate the values of the dependent variable.\nThe function \\(f\\) represents the systematic relationship between the dependent variable and the indipendent variable. This function captures all the information that the input variables collectively provide about the output variable. In other words, \\(f\\) describes the pattern or rule that connects inputs to outputs in a consistent, reproducible way. The crucial point is that in real-world applications, the true form of \\(f\\) is almost always unknown to us. We never directly observe this function; instead, we must estimate it based on the data we have collected. The entire enterprise of statistical learning revolves around developing methods to estimate \\(f\\) as accurately as possible, given the constraints of our data and our analytical goals.\nThe error term, denoted by \\(ϵ\\), represents the random component of the relationship between dependent and independent. This term captures all the variation in Y that cannot be explained by the predictors X. The error term is assumed to be independent of X and to have a mean of zero, which means that on average, the errors cancel out and do not systematically bias our predictions in one direction or another. The error term exists for several important reasons.\n\nFirst, there may be variables that influence Y but that we have not measured or included in our analysis.\nSecond, even if we could measure every relevant variable, there might be inherent randomness or unpredictability in the phenomenon we are studying.\nThird, our measurements themselves may contain some degree of imprecision or noise.\n\n\nTo make these concepts concrete, let me illustrate them with the example drawn from sociological research.\nConsider a sociologist studying income inequality and social mobility. The researcher might want to understand what determines a person’s income in adulthood. The dependent variable Y would be adult income, measured in monetary units. The predictors X might encompass the person’s own educational credentials, their occupation, the region where they live, their parents’ socioeconomic status, their race and gender, and the number of years of work experience they have accumulated. The function \\(f\\) would capture the systematic relationships between these characteristics and income, revealing how the labor market rewards different attributes and how social background continues to influence economic outcomes across generations. The error term \\(ϵ\\) would account for all the variation in income that these measured factors cannot explain. This residual variation might stem from unmeasured differences in job performance, luck in finding particularly good or bad employment matches, health shocks that affect earning capacity, or discrimination that varies in ways not captured by the measured variables.\nWe can write this relationship as:\n\\[\nY\\ =\\ f(X_1,\\ X_2,\\ X_3,\\ X_4,\\ X_5,\\ X_6,\\ X_7)\\ +\\ ϵ\n\\]\n\\[\nadult\\ income = f(educational\\ credentials,\\ occupation,\\ geographic\\ region,\\ parents'\\ socioeconomic\\ status,\\ race,\\ gender,\\ gender,\\ years\\ of\\ work\\ experience) + ϵ\n\\]\nIn this formula, Y represents adult income measured in monetary units such as annual earnings in dollars or euros. This is the outcome we are trying to understand and potentially predict. The predictors are defined as follows. \\(X_1\\) represents the person’s educational credentials, which might be measured as years of schooling completed or as the highest degree obtained. \\(X_2\\) represents occupation, which could be coded as occupational prestige scores or as categorical indicators for different types of jobs. \\(X_3\\) represents the geographic region where the person lives and works, capturing spatial variation in labor markets and cost of living. \\(X_4\\) represents parents’ socioeconomic status, which might be measured through parental income, parental education, or a composite index combining multiple indicators of family background. \\(X_5\\) represents race, coded as categorical indicators for different racial or ethnic groups. \\(X_6\\) represents gender, typically coded as a binary or categorical variable. \\(X_7\\) represents years of work experience, measuring how long the person has been participating in the labor force. The function \\(f\\) captures the systematic relationship between all these predictors and adult income. This function describes how the labor market values different combinations of education, occupation, location, background, and demographic characteristics. The precise form of \\(f\\) is unknown to us and must be estimated from data. It might be relatively simple, such as a linear combination of the predictors, or it might be quite complex, involving interactions between variables and nonlinear relationships. The error term \\(ϵ\\) represents all the variation in adult income that cannot be explained by the seven predictors we have included. This encompasses unmeasured factors such as individual differences in productivity, motivation, and interpersonal skills, as well as random events like fortunate or unfortunate timing in job searches, health events that affect earning capacity, and idiosyncratic experiences of discrimination or favoritism in the workplace.\nThe function \\(f\\) is the central object of interest in statistical learning. It represents the systematic relationship between the independent variable X and the dependent variable Y, capturing all the information that the independent variables provide about the dependent variable. When we say that \\(Y = f(X) + ϵ\\), we are asserting that the outcome Y can be decomposed into two parts: a predictable component \\(f(X)\\) that depends on the values of the predictors, and an unpredictable component \\(ϵ\\) that represents random variation. The function \\(f\\) is what connects the world of inputs to the world of predictors in a consistent, reproducible manner.\nUnderstanding the nature of \\(f\\) is crucial because it embodies the underlying pattern that governs how changes in the independent variable translate into changes in the dependent variable. If we knew \\(f\\) perfectly, we would understand exactly how each predictor influences the response, how predictors interact with one another, and what outcome to expect for any given combination of input values. However, in virtually all real-world applications, \\(f\\) is unknown. We never observe \\(f\\) directly; we only observe data points consisting of predictor values and corresponding outcomes. The entire purpose of statistical learning is to use these observed data points to construct an estimate of \\(f\\), which we denote as \\(f̂\\) (read as “f-hat”). This estimate allows us to either make predictions about future outcomes or draw inferences about the relationships between variables.\nThe reasons we might want to estimate \\(f\\) fall into two broad categories: prediction and inference. These two goals are conceptually distinct, and they often lead us to prefer different types of statistical learning methods.\nPrediction is concerned with accurately anticipating the value of Y for new observations where we know the predictors X but do not yet know the outcome. In prediction tasks, we treat \\(f̂\\) as a kind of black box. We do not necessarily care about the internal workings of our estimated function or about which specific predictors matter most. What we care about is whether our estimate \\(f̂\\) produces accurate predictions when applied to new data. The quality of our predictions depends on two sources of error. The first is reducible error, which arises because our estimate \\(f̂\\) is imperfect and does not exactly match the true f. We can potentially reduce this error by using better statistical learning methods or by collecting more data. The second is irreducible error, which corresponds to the variance of \\(ϵ\\). Even if we had a perfect estimate of \\(f\\), our predictions would still contain some error because Y is inherently influenced by random factors that cannot be predicted from X alone.\nTo illustrate prediction using our income example, imagine that a government agency wants to identify individuals who are at risk of falling into poverty so that it can target social assistance programs more effectively. The agency has access to administrative data containing information about people’s education, occupation, geographic location, family background, race, gender, and work experience. The goal is to predict each person’s income based on these characteristics. In this context, the agency does not need to understand precisely why certain combinations of predictors lead to low income. What matters is that the predictive model accurately identifies individuals whose incomes are likely to fall below some threshold. The function \\(f̂\\) serves as a tool for sorting people into risk categories, and its value is judged entirely by how well it predicts actual incomes.\nInference, by contrast, is concerned with understanding the relationship between the predictors and the outcome. When our goal is inference, we cannot treat \\(f̂\\) as a black box because we need to know its exact form. We want to answer questions such as which predictors are associated with the response, what is the direction and magnitude of each predictor’s effect, and whether the relationships are linear or more complex. Inference requires that our estimate \\(f̂\\) be interpretable, meaning that we can examine it and draw substantive conclusions about how the world works.\nReturning to the income example, suppose a sociologist wants to understand the mechanisms of income inequality. The researcher might ask questions such as: How much does an additional year of education increase expected income? Does the effect of education differ by race or gender? How large is the income penalty associated with being female, after controlling for education, occupation, and experience? These are inferential questions because they seek to illuminate the structure of \\(f\\) itself, not merely to use \\(f\\) for prediction. Answering these questions requires a model that allows the researcher to isolate the contribution of each predictor and to interpret coefficients or other parameters in substantively meaningful ways. A model that predicts income very accurately but does not reveal anything about how individual predictors matter would be useless for this purpose.\nIn practice, many research projects involve elements of both prediction and inference. A sociologist studying income might want to understand the determinants of earnings while also developing a model that can predict incomes for new individuals. However, there is often tension between these goals because the methods that produce the most accurate predictions are not always the most interpretable, and the most interpretable methods do not always produce the best predictions.\nHaving established why we want to estimate \\(f\\), let me now turn to the question of how we estimate \\(f\\). Statistical learning methods for estimating \\(f\\) can be broadly divided into two categories: parametric methods and non-parametric methods. These two approaches differ fundamentally in the assumptions they make about the form of \\(f\\) and in the way they use data to construct an estimate.\nParametric methods proceed in two steps. In the first step, we make an assumption about the functional form of \\(f\\). That is, we specify in advance what kind of mathematical relationship we believe connects the predictors to the outcome. The most common assumption is that \\(f\\) is linear, meaning that we assume the relationship can be written as \\(f(X) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p\\). This linear model asserts that the outcome is a weighted sum of the predictors, where the weights \\(\\beta_1, \\beta_2, ..., \\beta_p\\) are unknown coefficients that quantify the contribution of each predictor, and \\(\\beta_0\\) is an intercept term representing the expected outcome when all predictors equal zero. By assuming a linear form, we have dramatically simplified the problem. Instead of having to estimate an arbitrary, potentially very complex function \\(f\\), we only need to estimate the intercept and the p coefficients. In the second step of the parametric approach, we use the observed data to fit or train the model. This means finding values of the parameters that make the model match the data as closely as possible. For the linear model, the most common fitting procedure is ordinary least squares, which chooses the parameter values that minimize the sum of squared differences between the observed outcomes and the outcomes predicted by the model. Once we have estimated the parameters, our estimate \\(f̂\\) is fully specified, and we can use it for prediction or inference.\nThe parametric approach has several important advantages. Because we have reduced the problem to estimating a fixed number of parameters, parametric methods are computationally efficient and can work well even with relatively small samples. Furthermore, the resulting models are typically easy to interpret. In a linear model, each coefficient tells us how much the expected outcome changes when the corresponding predictor increases by one unit, holding all other predictors constant. This interpretability makes parametric models particularly valuable for inference. However, parametric methods also have a significant limitation. The assumption we make about the form of \\(f\\) may be wrong. If the true relationship between the predictors and the outcome is nonlinear or involves complex interactions, a linear model will fail to capture these features and will provide a poor approximation to \\(f\\). We can try to address this problem by using more flexible parametric models that include polynomial terms, interaction effects, or other elaborations of the basic linear form. But as we make our parametric model more flexible, we must estimate more parameters, which requires more data and increases the risk of a phenomenon called overfitting. Overfitting occurs when a model fits the training data very well but performs poorly on new data because it has captured random noise rather than genuine patterns. The model essentially memorizes the idiosyncrasies of the particular sample rather than learning the underlying relationship.\nNon-parametric methods take a fundamentally different approach. Instead of assuming a specific functional form for \\(f\\), non-parametric methods seek an estimate that gets close to the data points without imposing strong prior assumptions about the shape of the relationship. The idea is to let the data speak for themselves and to allow \\(f̂\\) to take whatever form best fits the observed patterns. One example of a non-parametric method is the thin-plate spline, which estimates \\(f\\) as a smooth surface that passes near the observed data points. The analyst does not specify in advance that \\(f\\) should be linear or quadratic or any other particular form. Instead, the method finds a smooth function that fits the data well, subject to some constraint on how wiggly or rough the function is allowed to be. Another example is the k-nearest neighbors method, which predicts the outcome for a new observation by averaging the outcomes of the k training observations that are most similar to it in terms of the predictor values.\nThe main advantage of non-parametric methods is their flexibility. Because they do not assume a particular form for \\(f\\), they can potentially capture a much wider range of relationships, including highly nonlinear patterns and complex interactions that would be missed by a simple parametric model. If the true \\(f\\) has an unusual or complicated shape, a non-parametric method has a better chance of approximating it accurately. However, non-parametric methods also have important disadvantages. Because they do not reduce the problem to estimating a small number of parameters, they typically require much larger samples to produce accurate estimates. The flexibility that allows non-parametric methods to fit complex patterns also makes them prone to overfitting, especially when sample sizes are limited. Furthermore, the estimates produced by non-parametric methods are often difficult to interpret. A thin-plate spline or a k-nearest neighbors prediction does not come with coefficients that tell us how each predictor contributes to the outcome. This lack of interpretability makes non-parametric methods less useful for inference, even when they excel at prediction.\nThe choice between parametric and non-parametric methods involves a fundamental trade-off. Parametric methods impose structure on the problem, which makes estimation easier and results more interpretable, but at the cost of potentially misspecifying the true form of \\(f\\). Non-parametric methods avoid this misspecification risk by staying flexible, but they require more data and produce less interpretable results. In practice, the best choice depends on the goals of the analysis, the amount of data available, and how much prior knowledge we have about the likely form of the relationship.\nThis brings us to a closely related issue: the trade-off between prediction accuracy and model interpretability. In statistical learning, there is often an inverse relationship between how flexible a method is and how interpretable its results are. Methods that impose strong restrictions on the form of \\(f\\) tend to be highly interpretable but may not fit complex patterns very well. Methods that are highly flexible can capture intricate relationships but produce results that are difficult for humans to understand.\nAt one end of the spectrum, we have highly restrictive methods like linear regression and its close relatives. Linear regression assumes that \\(f\\) is a linear combination of the predictors, which is a very strong restriction. This restriction means that linear regression can only produce straight lines in one dimension, flat planes in two dimensions, and hyperplanes in higher dimensions. The advantage is that the results are extremely interpretable. Each coefficient has a clear meaning: it tells us the expected change in Y associated with a one-unit increase in the corresponding predictor, holding other predictors constant. We can examine the coefficients and immediately understand which predictors matter, how large their effects are, and in which direction they operate. For inference purposes, this interpretability is invaluable.\nMoving along the spectrum toward greater flexibility, we encounter methods like generalized additive models, which relax the linearity assumption by allowing each predictor to have a potentially nonlinear effect on the outcome, while still maintaining an additive structure. These models are more flexible than linear regression and can capture curved relationships, but they remain reasonably interpretable because we can plot and examine the estimated effect of each predictor separately. Further along the spectrum, we find decision trees, which partition the predictor space into regions and assign a predicted outcome to each region. Trees are moderately flexible and can capture interactions and nonlinearities, but they remain somewhat interpretable because we can visualize the tree structure and see which predictors are used to make splits and at what values. At the far end of the spectrum, we have highly flexible methods such as bagging, boosting, support vector machines with nonlinear kernels, and deep neural networks. These methods can approximate extremely complex functions and often achieve superior predictive accuracy on difficult problems. However, their results are very hard to interpret. A neural network with thousands of parameters does not yield simple statements about how each predictor influences the outcome. The model operates as a black box: we can feed in predictors and obtain predictions, but we cannot easily understand why the model makes the predictions it does.\nOne might think that we should always prefer the most flexible method available, reasoning that greater flexibility means better ability to capture the true \\(f\\). Surprisingly, this is not the case. More flexible methods are not always better, even when our sole goal is prediction. The reason is overfitting. A highly flexible method can fit the training data very closely, including the random noise in that particular sample. When we apply the model to new data, the noise patterns will be different, and the overfitted model will perform poorly. In many situations, a simpler, more restrictive model that does not fit the training data as closely will actually generalize better to new observations.\nThis phenomenon is especially important when sample sizes are limited. With a small sample, there is not enough information to reliably estimate a complex, flexible model, and the risk of overfitting is high. In such cases, imposing structure through a parametric model can actually improve predictive performance by preventing the model from chasing noise. As sample sizes grow larger, we can afford to use more flexible methods because there is enough information to distinguish genuine patterns from random variation.\nThe choice of method therefore depends on our goals and our circumstances. If our primary goal is inference, we typically prefer more restrictive, interpretable methods like linear regression, even if they sacrifice some predictive accuracy. The interpretability allows us to draw substantive conclusions about how the world works. If our primary goal is prediction and we have ample data, we might prefer more flexible methods that can capture complex patterns, accepting that we will not be able to easily interpret the results. If we want prediction but have limited data, we might still prefer restrictive methods to avoid overfitting.\nLet me now bring together these various dimensions by summarizing how different statistical learning methods can be characterized according to their suitability for prediction versus inference, their parametric versus non-parametric nature, and their position on the interpretability-flexibility spectrum.\nLinear regression and its extensions such as ridge regression and the lasso are parametric methods that assume a linear relationship between predictors and outcome. They occupy the high-interpretability, low-flexibility end of the spectrum. Each predictor has a coefficient that directly quantifies its contribution to the outcome, making these methods ideal for inference. Their predictive accuracy is good when the true relationship is approximately linear, but they will miss nonlinear patterns. Because they estimate relatively few parameters, they work well with moderate sample sizes and are resistant to overfitting.\nLogistic regression is the classification analog of linear regression. It is a parametric method that models the probability of belonging to a particular class as a function of the predictors. Like linear regression, it is highly interpretable because the coefficients can be transformed into odds ratios that describe how each predictor affects the odds of the outcome. It is well-suited for inference in classification problems but assumes a particular functional form that may not hold in all situations.\nGeneralized additive models are parametric in the sense that they assume an additive structure, but they allow nonlinear effects for individual predictors. They occupy a middle position on the flexibility-interpretability spectrum. The additive structure maintains considerable interpretability because we can examine the effect of each predictor separately, but the nonlinear components provide more flexibility than standard linear regression. These models are useful when we suspect that relationships are nonlinear but still want to understand how each predictor contributes to the outcome.\nDecision trees are non-parametric methods that recursively partition the predictor space. They are moderately flexible and can capture interactions and nonlinearities. Their interpretability is moderate: small trees are easy to understand and visualize, but large, complex trees become difficult to interpret. Trees are prone to overfitting, especially when grown deep, but they provide a foundation for more sophisticated ensemble methods.\nBagging and random forests are ensemble methods that combine many decision trees to improve predictive accuracy and reduce overfitting. They are non-parametric and highly flexible, capable of capturing complex patterns in the data. However, by aggregating many trees, they sacrifice the interpretability of individual trees. These methods are primarily useful for prediction rather than inference, particularly when sample sizes are large enough to support their flexibility.\nBoosting methods build ensembles of weak learners, typically small decision trees, in a sequential manner that focuses on observations that previous models predicted poorly. Like bagging and random forests, boosting methods are highly flexible and achieve excellent predictive performance on many problems. They are non-parametric and operate toward the low-interpretability end of the spectrum, making them suitable for prediction but less useful for inference.\nSupport vector machines are methods that find optimal boundaries between classes in classification problems or fit regression functions in a way that ignores errors smaller than some threshold. With linear kernels, they are similar to linear methods and retain interpretability. With nonlinear kernels, they become highly flexible and can capture complex decision boundaries, but they lose interpretability. The flexibility of support vector machines can be tuned by choosing different kernels and adjusting regularization parameters.\nNeural networks and deep learning represent the extreme of flexibility. These methods can approximate virtually any function given enough data and computational resources. They have achieved remarkable success in applications such as image recognition, natural language processing, and game playing. However, their complexity makes them essentially uninterpretable. A deep neural network with millions of parameters cannot be summarized in a way that humans can understand. These methods are purely predictive tools, unsuitable for inference, and they require very large datasets to train effectively without overfitting.\nK-nearest neighbors is a simple non-parametric method that makes predictions by averaging the outcomes of similar observations in the training data. It is highly flexible and makes no assumptions about the form of f. However, it is not very interpretable because it does not produce coefficients or other summaries of how predictors relate to outcomes. Its predictive accuracy depends strongly on the choice of k and on having a sufficiently large and representative training sample.\nIn summary, the landscape of statistical learning methods can be understood through several interconnected dimensions. The parametric versus non-parametric distinction concerns whether a method assumes a specific functional form for \\(f\\) or lets the data determine the shape of the relationship. The interpretability versus flexibility trade-off reflects the tension between methods that produce easily understood results and methods that can capture complex patterns. The prediction versus inference distinction concerns whether our goal is to forecast future outcomes accurately or to understand the mechanisms connecting predictors to responses. These dimensions are interrelated: parametric methods tend to be more interpretable and better suited for inference, while non-parametric methods tend to be more flexible and better suited for prediction when interpretability is not required. The choice among methods depends on the specific goals of an analysis, the nature of the data, the sample size available, and the analyst’s prior beliefs about the likely form of the relationship being studied.\nTo complete our overview of the foundational concepts in statistical learning, we need to understand additional distinction between supervised and unsupervised learning that help us categorize different types of learning problems.\nSupervised learning refers to situations where for each observation in our dataset, we have both predictor measurements and a corresponding outcome measurement. The term “supervised” reflects the idea that the learning process is guided or supervised by the known outcomes. We observe what actually happened for each case in our training data, and we use this information to learn the relationship between predictors and outcomes. All the methods we have discussed so far, including linear regression, logistic regression, decision trees, random forests, and neural networks, fall into the supervised learning category when applied to problems where outcomes are observed. The fundamental goal of supervised learning is to build a model that can predict the outcome for new observations based on their predictor values, or to understand how the predictors relate to the outcome. In sociological research, most studies involve supervised learning because we typically have data on both the explanatory variables and the outcome of interest. For example, when we study the relationship between education and income, we observe both variables for the individuals in our sample, which allows us to estimate how education influences earnings.\nUnsupervised learning describes a fundamentally different situation where we observe predictor measurements for each observation but have no corresponding outcome variable. Without an outcome to predict or explain, we cannot fit a regression model or train a classifier. Instead, unsupervised learning seeks to discover patterns, structures, or groupings within the data itself. The most common unsupervised learning task is cluster analysis, which attempts to identify subgroups of observations that are similar to one another. For instance, a sociologist might have survey data containing many variables about people’s attitudes, behaviors, and demographic characteristics, but no predefined categorization of people into types. Cluster analysis could reveal that the respondents naturally fall into distinct groups based on their patterns of responses, perhaps identifying clusters that correspond to different lifestyles, political orientations, or consumption patterns. The key feature of unsupervised learning is that there is no “correct answer” to supervise the learning process. We are not trying to predict a known outcome but rather to uncover hidden structure in the data. This makes unsupervised learning more exploratory and somewhat more subjective than supervised learning, since there is no objective criterion like prediction accuracy to evaluate whether we have found the right structure.\n\nTable 1.1 Summary table of statistical learning methods\n\n\n\n\n\n\n\n\n\n\n\nMethod\nUnsupervised / Supervised\nParametric / Non-parametric\nFlexibility\nInterpretability\nBest Suited For\nRegression / Classification\n\n\n\n\nLinear Regression\nSupervised\nParametric\nLow\nHigh\nInference\nRegression\n\n\nRidge Regression\nSupervised\nParametric\nLow\nHigh\nInference & Prediction\nRegression\n\n\nLasso\nSupervised\nParametric\nLow\nHigh\nInference & Prediction\nRegression\n\n\nLogistic Regression\nSupervised\nParametric\nLow\nHigh\nInference\nClassification\n\n\nGeneralized Additive Models (GAMs)\nSupervised\nParametric (additive structure)\nMedium\nMedium-High\nInference & Prediction\nBoth\n\n\nDecision Trees\nSupervised\nNon-parametric\nMedium\nMedium\nInference & Prediction\nBoth\n\n\nBagging\nSupervised\nNon-parametric\nHigh\nLow\nPrediction\nBoth\n\n\nRandom Forests\nSupervised\nNon-parametric\nHigh\nLow\nPrediction\nBoth\n\n\nBoosting\nSupervised\nNon-parametric\nHigh\nLow\nPrediction\nBoth\n\n\nSupport Vector Machines (Linear Kernel)\nSupervised\nParametric\nLow-Medium\nMedium\nPrediction & Inference\nBoth\n\n\nSupport Vector Machines (Nonlinear Kernel)\nSupervised\nNon-parametric\nHigh\nLow\nPrediction\nBoth\n\n\nK-Nearest Neighbors\nSupervised\nNon-parametric\nHigh\nLow\nPrediction\nBoth\n\n\nNeural Networks / Deep Learning\nSupervised\nNon-parametric\nVery High\nVery Low\nPrediction\nBoth\n\n\nK-Means Clustering\nUnsupervised\nNon-parametric\nMedium\nMedium\nDiscovering groups in data\nModerate to Large\n\n\nHierarchical Clustering\nUnsupervised\nNon-parametric\nMedium\nMedium-High\nDiscovering nested group structures\nModerate\n\n\nPrincipal Component Analysis (PCA)\nUnsupervised\nParametric\nLow\nMedium-High\nDimensionality reduction\nModerate\n\n\nFactor Analysis\nUnsupervised\nParametric\nLow\nHigh\nIdentifying latent constructs\nModerate",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to statistical learning</span>"
    ]
  }
]