<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>2&nbsp; Quntitative Linear Regression Model – Statistical Learning in R for Social Sciences</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./02_00_linear_regression_model.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-ed96de9b727972fe78a7b5d16c58bf87.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-86b712c1a9842e5c5be4cb0afbdd663e.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>

      .quarto-title-block .quarto-title-banner h1,
      .quarto-title-block .quarto-title-banner h2,
      .quarto-title-block .quarto-title-banner h3,
      .quarto-title-block .quarto-title-banner h4,
      .quarto-title-block .quarto-title-banner h5,
      .quarto-title-block .quarto-title-banner h6
      {
        color: white;
      }

      .quarto-title-block .quarto-title-banner {
        color: white;
background: #00868B;
      }
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./02_00_linear_regression_model.html">Linear Regression Model</a></li><li class="breadcrumb-item"><a href="./02_01_quntitative_linear_regression_model.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Quntitative Linear Regression Model</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./02_00_linear_regression_model.html">Linear Regression Model</a></li><li class="breadcrumb-item"><a href="./02_01_quntitative_linear_regression_model.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Quntitative Linear Regression Model</span></a></li></ol></nav>
      <div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Quntitative Linear Regression Model</span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Statistical Learning in R for Social Sciences</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./introduction_to_statistical_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction to Statistical Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./02_00_linear_regression_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Linear Regression Model</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02_01_quntitative_linear_regression_model.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Quntitative Linear Regression Model</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#estimating-the-coefficients-of-parameters" id="toc-estimating-the-coefficients-of-parameters" class="nav-link active" data-scroll-target="#estimating-the-coefficients-of-parameters"><span class="header-section-number">1</span> Estimating the Coefficients of Parameters</a></li>
  <li><a href="#accuracy-of-the-coefficient-estimates" id="toc-accuracy-of-the-coefficient-estimates" class="nav-link" data-scroll-target="#accuracy-of-the-coefficient-estimates"><span class="header-section-number">2</span> Accuracy of the Coefficient Estimates</a>
  <ul class="collapse">
  <li><a href="#standard-errors" id="toc-standard-errors" class="nav-link" data-scroll-target="#standard-errors"><span class="header-section-number">2.1</span> Standard Errors</a></li>
  <li><a href="#hypothesis-testing-and-the-t-statistic" id="toc-hypothesis-testing-and-the-t-statistic" class="nav-link" data-scroll-target="#hypothesis-testing-and-the-t-statistic"><span class="header-section-number">2.2</span> Hypothesis Testing and the t-Statistic</a></li>
  <li><a href="#the-p-value" id="toc-the-p-value" class="nav-link" data-scroll-target="#the-p-value"><span class="header-section-number">2.3</span> The p-Value</a></li>
  </ul></li>
  <li><a href="#model-fit" id="toc-model-fit" class="nav-link" data-scroll-target="#model-fit"><span class="header-section-number">3</span> Model Fit</a>
  <ul class="collapse">
  <li><a href="#residual-standard-error" id="toc-residual-standard-error" class="nav-link" data-scroll-target="#residual-standard-error"><span class="header-section-number">3.1</span> Residual Standard Error</a></li>
  <li><a href="#r2" id="toc-r2" class="nav-link" data-scroll-target="#r2"><span class="header-section-number">3.2</span> <span class="math inline">\(R^2\)</span></a></li>
  <li><a href="#f-statistic" id="toc-f-statistic" class="nav-link" data-scroll-target="#f-statistic"><span class="header-section-number">3.3</span> F-statistic</a></li>
  </ul></li>
  <li><a href="#interaction-terms" id="toc-interaction-terms" class="nav-link" data-scroll-target="#interaction-terms"><span class="header-section-number">4</span> Interaction Terms</a></li>
  <li><a href="#hierarchical-linear-regression" id="toc-hierarchical-linear-regression" class="nav-link" data-scroll-target="#hierarchical-linear-regression"><span class="header-section-number">5</span> Hierarchical Linear Regression</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>The simplest form of linear regression involves just one independent variable. This is called <strong>simple linear regression</strong>. Mathematically, it takes the form:</p>
<p><span class="math inline">\(Y \approx \beta_0 + \beta_1X\)</span></p>
<p>In this equation, Y is the dependent variable - the outcome we want to predict. In our example, Y is, for example, the variable “exam_score”. X is the independent variable - the factor we believe is related to the outcome. For instance, X could be “hours_studied”. The symbol <span class="math inline">\(\beta_0\)</span> is called the <strong>intercept</strong>. It represents the expected value of Y when X equals zero. In our context, it would represent the expected exam score for a hypothetical student who studies zero hours. The symbol <span class="math inline">\(\beta_1\)</span> is called the <strong>slope</strong>. It represents the average change in Y that is associated with a one-unit increase in X. In our example, it tells us how many additional points on the exam a student can expect to gain for each additional hour of studying. Together, <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are called the <strong>model coefficients or parameters</strong>.</p>
<p>Of course, in real life we do not know the true values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. We must estimate them from the data. Once we have estimated these coefficients - and we denote the estimates with a hat symbol as <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> - we can write our prediction equation as: <span class="math inline">\(y = \hat{\beta_0} + \hat{\beta_1}x\)</span>. Here, <span class="math inline">\(\hat{y}\)</span> is the predicted value of the response for a given value <span class="math inline">\(x\)</span> of the predictor. The hat symbol always indicates that we are dealing with an estimate rather than a true and known quantity.</p>
<p>In practice, a single predictor is rarely sufficient to explain all the variation in the response. A student’s exam score is not determined by study hours alone - attendance, prior academic performance, tutoring, and many other factors play a role. <strong>Multiple linear regression</strong> extends the simple model to accommodate several predictors simultaneously. The general formula is:</p>
<p><span class="math inline">\(Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p + \epsilon\)</span></p>
<p>In this equation, <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, <span class="math inline">\(X_3\)</span>, …, <span class="math inline">\(X_p\)</span> represent <span class="math inline">\(p\)</span> different independent variables, and <span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\beta_2\)</span>, …, <span class="math inline">\(\beta_p\)</span> are their corresponding slope coefficients. Each coefficient <span class="math inline">\(\beta_j\)</span> represents the average change in Y associated with a one-unit increase in the predictor <span class="math inline">\(\beta_j\)</span>, while <strong>holding all other predictors constant</strong> which distinguishes multiple regression from simply running many separate simple regressions. The term <span class="math inline">\(\epsilon\)</span> represents the <strong>error term</strong> - it captures everything that our model does not explain, including the influence of unmeasured variables, measurement error, and the inherent randomness in human behavior.</p>
<p>In our student performance example, a multiple linear regression model might look like this:</p>
<p>exam_score = <span class="math inline">\(\beta_0\)</span> + <span class="math inline">\(\beta_1\)</span> × hours_studied + <span class="math inline">\(\beta_2\)</span> × attendance + <span class="math inline">\(\beta_3\)</span> × previous_scores + <span class="math inline">\(\beta_4\)</span> × sleep_hours + <span class="math inline">\(\beta_5\)</span> × tutoring_sessions + <span class="math inline">\(\beta_6\)</span> × physical_activity + <span class="math inline">\(\epsilon\)</span></p>
<p>This model allows us to estimate the unique contribution of each predictor to the exam score. For instance, <span class="math inline">\(\beta_1\)</span> tells us the expected change in exam score for each additional hour of study, after accounting for the effects of attendance, previous scores, sleep, tutoring, and physical activity. This is fundamentally different from simple linear regression, where <span class="math inline">\(\beta_1\)</span> would capture the total association between study hours and exam scores without adjusting for any other factor.</p>
<section id="estimating-the-coefficients-of-parameters" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="estimating-the-coefficients-of-parameters"><span class="header-section-number">1</span> Estimating the Coefficients of Parameters</h2>
<p>The key question is how do we actually find the best values for our coefficient estimates <span class="math inline">\(\hat{\beta}_0\)</span>, <span class="math inline">\(\hat{\beta}_1\)</span>, …, <span class="math inline">\(\hat{\beta}_p\)</span>. The answer lies in the <strong>least squares method</strong>, which is the most common approach for fitting a linear regression model. The basic idea is intuitive: we want our predicted values <span class="math inline">\(\hat{y}_i\)</span> to be as close as possible to the actual observed values <span class="math inline">\(y_0\)</span> for every observation in our dataset.</p>
<p>For each observation <span class="math inline">\(i\)</span>, the difference between the observed value and the predicted value is called the <strong>residual</strong>, denoted <span class="math inline">\(e_i = y_i - \hat{y}_i\)</span>. The residual tells us how much our model’s prediction misses the actual outcome for that particular student. Some residuals will be positive (when the model underpredicts) and some will be negative (when the model overpredicts).</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="02_01_quntitative_linear_regression_model_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>: Simple Linear Regression: Exam Score ~ Hours Studied</p>
<p>To get an overall measure of how well the model fits all the data, we cannot simply add up the residuals, because the positive and negative ones would cancel each other out. Instead, we square each residual and then sum them all up. This quantity is called the <strong>residual sum of squares</strong> (RSS):</p>
<p><span class="math inline">\(RSS = e_1^2 + e_2^2 + ... + e_n^2 = \sum(y_i - \hat{y}_i)^2\)</span></p>
<p>The least squares method chooses the coefficient estimates <span class="math inline">\(\hat{\beta}_0\)</span>, <span class="math inline">\(\hat{\beta}_1\)</span>, …, <span class="math inline">\(\hat{\beta}_p\)</span> that minimize this RSS. In other words, the least squares approach finds the line (in simple regression) or the hyperplane (in multiple regression) that makes the total squared prediction error as small as possible. This is a well-defined mathematical optimization problem, and the solution can be computed using calculus. For simple linear regression, the formulas for the minimizers have a closed-form expression:</p>
<p><span class="math inline">\(\hat{\beta}_0 = \bar{y} - \hat{\beta}_1\hat{x}\)</span></p>
<p><span class="math inline">\(\hat{\beta}_1 = \frac{\sum_{i = 1}^n(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i = 1}^n(x_i - \bar{x})^2}\)</span></p>
<p>Here, <span class="math inline">\(\bar{x}\)</span> and <span class="math inline">\(\bar{y}\)</span> are the sample means of the predictor and the response, respectively. The formula for <span class="math inline">\(\hat{\beta}_1\)</span> has an intuitive interpretation: it measures the degree to which X and Y vary together (the numerator captures their joint variation) relative to the total variation in X (the denominator). For multiple linear regression, the coefficient estimates are computed using matrix algebra.</p>
<p>The beauty of the least squares method is that it provides a principled, objective way to estimate the model parameters. It does not require any subjective judgment about what the “best” line should look like - the method simply finds the line that minimizes the total squared distance between the observed data points and the fitted line.</p>
<p>To fit a simple linear regression model in R, we use the <code>lm()</code> function, which stands for linear model. The syntax follows the pattern <code>lm(response ~ predictor, data = dataset)</code>. The tilde symbol (<code>~</code>) can be read as “<em>is modeled as a function of</em>”. For our simple linear regression of “exam_score” onto “hours_studied”, we write:</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>simple_model <span class="ot">&lt;-</span> <span class="fu">lm</span>(</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    exam_score <span class="sc">~</span> hours_studied,</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> student_performance</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>The lm() function fits the model by computing the least squares coefficient estimates. The <code>summary()</code> function then provides a detailed output that includes the estimated coefficients, their standard errors, t-statistics, p-values, the residual standard error, and the <span class="math inline">\(R^2\)</span> statistic. The <code>confint()</code> function computes the 95% confidence intervals for each coefficient estimate, which tell us the range of plausible values for the true population parameters.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(simple_model)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = exam_score ~ hours_studied, data = student_performance)

Residuals:
   Min     1Q Median     3Q    Max 
-8.532 -2.243 -0.111  2.046 33.493 

Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   61.456984   0.149196  411.92   &lt;2e-16 ***
hours_studied  0.289291   0.007154   40.44   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 3.483 on 6605 degrees of freedom
Multiple R-squared:  0.1984,    Adjusted R-squared:  0.1983 
F-statistic:  1635 on 1 and 6605 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>The <code>lm()</code> fits the model by computing the least squares coefficient estimates, and the output shows the estimated parameters of a simple linear regression model predicting exam score from hours studied.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(simple_model)<span class="sc">$</span>coefficients[, <span class="st">"Estimate"</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>  (Intercept) hours_studied 
   61.4569836     0.2892906 </code></pre>
</div>
</div>
<p>The intercept (<span class="math inline">\(\hat{\beta}_0\)</span>) is estimated at 61.4570. This means that when “hours_studied” equals zero, the model predicts an exam score of approximately 61.46 points. In substantive terms, a hypothetical student who does not study at all would be expected to score about 61.5 on the exam, according to this model. This makes intuitive sense - students would still have some baseline level of knowledge from attending classes, even without additional study outside the classroom. The slope for “hours_studied” (<span class="math inline">\(\hat{\beta}_1\)</span>) is estimated at 0.289. This is the key coefficient for our research question. It tells us that for each additional hour of study per week, a student’s exam score is expected to increase by approximately 0.29 points, on average. So a student who studies 10 hours more per week than another student would be expected to score about 2.89 points higher on the exam.</p>
<p>For the multiple linear regression model, we simply add more predictors to the right side of the formula, separated by the <code>+</code> sign:</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>multiple_model <span class="ot">&lt;-</span> <span class="fu">lm</span>(</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    exam_score <span class="sc">~</span> hours_studied <span class="sc">+</span> attendance <span class="sc">+</span> previous_scores <span class="sc">+</span> sleep_hours <span class="sc">+</span> tutoring_sessions <span class="sc">+</span> physical_activity,</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> student_performance</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(multiple_model)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = exam_score ~ hours_studied + attendance + previous_scores + 
    sleep_hours + tutoring_sessions + physical_activity, data = student_performance)

Residuals:
    Min      1Q  Median      3Q     Max 
-5.4391 -1.1316 -0.1619  0.8435 30.9951 

Coefficients:
                   Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)       40.927132   0.338284 120.985  &lt; 2e-16 ***
hours_studied      0.291579   0.005069  57.517  &lt; 2e-16 ***
attendance         0.197978   0.002631  75.262  &lt; 2e-16 ***
previous_scores    0.048123   0.002110  22.809  &lt; 2e-16 ***
sleep_hours       -0.018022   0.020686  -0.871    0.384    
tutoring_sessions  0.493505   0.024679  19.997  &lt; 2e-16 ***
physical_activity  0.143997   0.029449   4.890 1.03e-06 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 2.467 on 6600 degrees of freedom
Multiple R-squared:  0.5982,    Adjusted R-squared:  0.5979 
F-statistic:  1638 on 6 and 6600 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>This tells R to fit a model that predicts “exam_score” using all six quantitative predictors simultaneously. The least squares method will estimate a separate slope coefficient for each predictor, along with a single intercept, by minimizing the total RSS across all 6,607 observations. Our multiple linear regression model includes six quantitative predictors simultaneously.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(multiple_model)<span class="sc">$</span>coefficients[, <span class="st">"Estimate"</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>      (Intercept)     hours_studied        attendance   previous_scores 
      40.92713226        0.29157938        0.19797753        0.04812256 
      sleep_hours tutoring_sessions physical_activity 
      -0.01802166        0.49350511        0.14399695 </code></pre>
</div>
</div>
<p>The estimated model can be written as:</p>
<p>Exam_Score ≈ 40.93 + 0.292 × Hours_Studied + 0.198 × Attendance + 0.048 × Previous_Scores − 0.018 × Sleep_Hours + 0.494 × Tutoring_Sessions + 0.144 × Physical_Activity</p>
<p>The intercept (<span class="math inline">\(\hat{\beta}_0\)</span>) is now estimated at 40.927. This is substantially lower than in the simple model (61.46), which makes sense. In the simple model, the intercept represented the predicted score when only “hours_studied” was zero. In the multiple model, the intercept represents the predicted score when all six predictors are on their average value. Such a student is of course entirely hypothetical and unrealistic, which is why we should not over-interpret the intercept in multiple regression. Its main role is mathematical - it anchors the regression plane in the right position.</p>
<p>The coefficient for “hours_studied” is 0.292, which is remarkably similar to its value in the simple regression (0.289). This tells us something important: the relationship between study hours and exam scores is robust - it persists even after we account for the effects of attendance, prior scores, sleep, tutoring, and physical activity. In substantive terms, holding all other factors constant, each additional hour of study per week is associated with an increase of about 0.29 points on the exam.</p>
<p>The coefficient for “attendance” is 0.198, meaning that each additional percentage point of class attendance is associated with about 0.20 additional points on the exam, after controlling for the other variables. To put this in perspective, a student who attends 90% of classes versus one who attends 70% of classes (a 20 percentage-point difference) would be expected to score about 3.96 points higher, all else being equal. The coefficient for “previous_scores” is 0.048. This means that for each additional point a student earned on their previous assessments, their exam score is expected to increase by about 0.048 points, holding other factors constant. A student whose prior scores are 20 points higher than another student’s would be expected to score only about 0.96 points higher on this exam. This suggests that while past performance does predict future performance, its incremental contribution is small once study habits and attendance are already accounted for. The coefficient for “sleep_hours” is -0.018. The negative sign suggests that more sleep is associated with slightly lower exam scores, meaning that each additional hour of sleep per night is associated with a decrease of about 0.02 points on the exam, after controlling for the other variables. The coefficient for “tutoring_sessions” is 0.494, the largest individual slope coefficient in the model. Each additional tutoring session is associated with about half a point increase on the exam. The coefficient for physical_activity is 0.144, meaning that each additional hour per week of physical activity is associated with about 0.14 additional exam points, after controlling for the other variables.</p>
</section>
<section id="accuracy-of-the-coefficient-estimates" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="accuracy-of-the-coefficient-estimates"><span class="header-section-number">2</span> Accuracy of the Coefficient Estimates</h2>
<p>In the previous section, we estimated the coefficients of our linear regression models using the least squares method. We found, for instance, that the estimated slope for “hours_studied” was 0.289 in the simple model and 0.292 in the multiple model. But a natural and critically important question follows: <em>How accurate are these estimates? If we collected a different sample of 6,607 students, would we get the same coefficient estimates, or would they change? And how confident can we be that the true relationship between study hours and exam scores is actually positive, rather than our estimate simply being a product of random chance in this particular sample?</em></p>
<p>These questions lie at the heart of statistical inference, and the tools we use to answer them - standard errors, confidence intervals, t-statistics, and p-values - are among the most important concepts in all of applied statistics. To understand these tools, we must first understand the concept of the error term and the distinction between the population regression line and our estimated regression line.</p>
<p>When we write the linear regression model as <span class="math inline">\(Y = \beta_0 + \beta_1X + \epsilon\)</span>, we are making a statement about the true, underlying relationship between X and Y in the entire population - not just in our particular sample. The coefficients <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are the true population parameters, which we will never know exactly. The term <span class="math inline">\(\epsilon\)</span> is the error term, and it represents everything that our model fails to capture. There are several reasons why the error term exists.</p>
<ol type="1">
<li><p>First, the true relationship between the predictor and the response may not be perfectly linear - there may be curvature or other patterns that a straight line cannot capture.</p></li>
<li><p>Second, there may be other variables that influence the response but are not included in our model.</p></li>
<li><p>Third, there is always some inherent randomness and measurement error in any data we collect.</p></li>
</ol>
<p>The error term absorbs all of these sources of discrepancy between what our model predicts and what actually happens.</p>
<p>In our <code>student_performance</code> example, even if we knew the exact true values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> for the relationship between “hours_studied” and “exam_score” in the entire population of all students, we still could not predict any individual student’s exam score perfectly. Some students who study 20 hours per week will score higher than the regression line predicts, and others will score lower. These deviations are captured by <span class="math inline">\(\epsilon\)</span>. We typically assume that the error term has a mean of zero (meaning the model does not systematically overpredict or underpredict), that the errors for different observations are independent of each other, and that the errors have a constant variance <span class="math inline">\(\sigma^2\)</span> across all values of X.</p>
<p>The true population regression line, <span class="math inline">\(Y = \beta_0 + \beta_1X\)</span>, represents the best linear summary of the relationship between X and Y in the entire population. Our estimated regression line, <span class="math inline">\(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1x\)</span>, is our best approximation of this population line based on the data we have. The key insight is that <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> are estimates of the true parameters, computed from one particular sample. If we were to draw a different random sample of 6,607 students, we would get slightly different estimates. This variability across samples is what motivates the need for standard errors, confidence intervals, and hypothesis tests - they allow us to quantify how much uncertainty surrounds our estimates.</p>
<section id="standard-errors" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="standard-errors"><span class="header-section-number">2.1</span> Standard Errors</h3>
<p>The <strong>standard error</strong> of a coefficient estimate measures how much that estimate would vary if we repeatedly drew new samples from the same population and re-estimated the model each time. In other words, it quantifies the precision of our estimate. A small standard error means that our estimate is very precise - different samples would give us very similar coefficient values. A large standard error means that our estimate is imprecise - it might change substantially from sample to sample.</p>
<p>The standard error of the coefficients or parameters in linear regression is given by the formulas:</p>
<p><span class="math inline">\(SE(\hat{\beta}_0) = \sigma^2 [\frac{1}{n} + \frac{\bar{x}^2}{\sum_{i =1}^n(x_i - \bar{x})^2}]\)</span></p>
<p><span class="math inline">\(SE(\hat{\beta}_1) = \frac{\sigma}{\sum_{i=1}^n(x_i - \bar{x})^2}\)</span></p>
<p>This formulas reveals two important things. First, the standard error depends on <span class="math inline">\(\sigma\)</span>, the standard deviation of the error term. If there is a lot of noise in the data (large <span class="math inline">\(\sigma\)</span>), then the estimated slope will be less precise, because the true signal is harder to detect amid the noise. Second, the standard error depends on the spread of the predictor values. When the <span class="math inline">\(x_i\)</span> values are more spread out (that is, when <span class="math inline">\(\sum_{i=1}^n(x_i - \bar{x})^2\)</span> is large), the standard error is smaller. Intuitively, this makes sense: if we observe students who study anywhere from 1 to 44 hours per week, we have a much better basis for estimating the slope than if all students studied between 19 and 21 hours. A wider range of predictor values gives us more “leverage” to pin down the relationship. In practice, the true value of <span class="math inline">\(\sigma\)</span> is unknown and must be estimated from the data. This estimate is the Residual Standard Error.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(simple_model)<span class="sc">$</span>coefficients[, <span class="st">"Std. Error"</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>  (Intercept) hours_studied 
  0.149196057   0.007154262 </code></pre>
</div>
</div>
<p>Looking at our simple regression output, the standard error for the “hours_studied” coefficient is 0.00715. This is very small relative to the coefficient estimate of 0.289, which tells us that our estimate is highly precise. The reason for this high precision is our large sample size combined with a good spread in study hours.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(multiple_model)<span class="sc">$</span>coefficients[, <span class="st">"Std. Error"</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>      (Intercept)     hours_studied        attendance   previous_scores 
      0.338284044       0.005069489       0.002630507       0.002109816 
      sleep_hours tutoring_sessions physical_activity 
      0.020685503       0.024678804       0.029448624 </code></pre>
</div>
</div>
<p>In the multiple regression model, the standard error for “hours_studied” is even smaller at 0.00507. This decrease occurs because the multiple model has a lower RSE (2.467 compared to 3.483), which means there is less unexplained noise once we account for the additional predictors - and less noise translates directly into more precise coefficient estimates. The standard errors for the other coefficients in the multiple model tell a similar story. “attendance” has a standard error of 0.00263, “previous_scores” has 0.00211, “sleep_hours” has 0.0207, “tutoring_sessions” has 0.0247, and “physical_activity” has 0.0294. Notice that “sleep_hours” has a relatively large standard error compared to its coefficient estimate (-0.018), which foreshadows the fact that this coefficient will not be statistically significant - the estimate is so imprecise relative to its magnitude that we cannot confidently distinguish it from zero.</p>
<p>A <strong>confidence interval</strong> provides a range of plausible values for the true population parameter. The 95% confidence interval for a regression coefficient is constructed using the formula:</p>
<p><span class="math inline">\(\hat{\beta}_j \pm 2 \times SE(\hat{\beta}_j)\)</span></p>
<p>More precisely, the multiplier is not exactly 2 but rather the 97.5th percentile of the t-distribution with <span class="math inline">\(n - p - 1\)</span> degrees of freedom, where n is the sample size and p is the number of predictors. For large samples like ours (n = 6,607), this value is very close to 1.96, which is approximately 2. The interpretation of a 95% confidence interval is as follows: if we were to repeat the study many times, drawing a new random sample each time and computing a 95% confidence interval from each sample, then 95% of those intervals would contain the true population parameter. It is important to note that this does not mean there is a 95% probability that the true parameter lies within our specific interval - the true parameter is a fixed value, not a random quantity. Rather, the 95% refers to the long-run reliability of the procedure.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(simple_model)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>                  2.5 %     97.5 %
(Intercept)   61.164511 61.7494561
hours_studied  0.275266  0.3033153</code></pre>
</div>
</div>
<p>Let us examine the confidence intervals from our outputs. In the simple regression, the 95% confidence interval for the “hours_studied” coefficient is [0.275, 0.303]. This interval is narrow, reflecting the high precision of our estimate, and it lies entirely above zero. We can therefore state with 95% confidence that the true effect of one additional hour of study falls somewhere between 0.275 and 0.303 points on the exam.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(multiple_model)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>                        2.5 %      97.5 %
(Intercept)       40.26398610 41.59027841
hours_studied      0.28164155  0.30151722
attendance         0.19282088  0.20313417
previous_scores    0.04398664  0.05225848
sleep_hours       -0.05857194  0.02252862
tutoring_sessions  0.44512667  0.54188355
physical_activity  0.08626812  0.20172578</code></pre>
</div>
</div>
<p>In the multiple regression, the confidence intervals are similarly informative. For “attendance”, the interval is [0.193, 0.203], meaning we are 95% confident that each additional percentage point of attendance is associated with between 0.19 and 0.20 additional exam points, after controlling for the other predictors. For “tutoring_sessions”, the interval is [0.445, 0.542], and for “physical_activity” it is [0.086, 0.202] - all comfortably above zero. The critical case is “sleep_hours”, whose confidence interval is [-0.059, 0.023]. Because this interval spans from a negative value to a positive value, crossing zero in the middle, we cannot determine whether the true effect of sleep hours on exam scores is positive, negative, or simply zero.</p>
</section>
<section id="hypothesis-testing-and-the-t-statistic" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="hypothesis-testing-and-the-t-statistic"><span class="header-section-number">2.2</span> Hypothesis Testing and the t-Statistic</h3>
<p><strong>Hypothesis testing</strong> provides a formal framework for determining whether the relationship we observe in our sample is likely to reflect a real relationship in the population, or whether it could plausibly be due to random chance. In linear regression, the most common hypothesis test for each coefficient is:</p>
<p><strong>The null hypothesis (<span class="math inline">\(H_0\)</span>):</strong> <span class="math inline">\(\beta_j = 0\)</span>, meaning there is no relationship between <span class="math inline">\(X_j\)</span> and Y.</p>
<p><strong>The alternative hypothesis (<span class="math inline">\(H_a\)</span>):</strong> <span class="math inline">\(\beta_j \neq 0\)</span>, meaning there is some relationship between <span class="math inline">\(X_j\)</span> and Y.</p>
<p>If the null hypothesis is true and the predictor truly has no effect on the response, then the true slope is zero, and any non-zero slope we estimate from our sample is simply due to random noise. The t-statistic allows us to assess how likely this scenario is.</p>
<p>The <strong>t-statistic</strong> is computed as:</p>
<p><span class="math inline">\(t = \frac{\hat{\beta}_j}{SE(\hat{\beta}_j)}\)</span></p>
<p>This is simply the coefficient estimate divided by its standard error. It measures how many standard errors the coefficient estimate is away from zero. A t-statistic close to zero means that the coefficient estimate is small relative to its uncertainty, which is consistent with the null hypothesis. A t-statistic far from zero (either very positive or very negative) means that the coefficient estimate is large relative to its uncertainty, which provides evidence against the null hypothesis.</p>
<p>Under the null hypothesis, the t-statistic follows a t-distribution with <span class="math inline">\(n - p - 1\)</span> degrees of freedom, where n is the number of observations and p is the number of predictors. For large samples, the t-distribution is virtually identical to the standard normal distribution, so a t-statistic beyond roughly <span class="math inline">\(\pm2\)</span> is generally considered statistically significant at the 5% level, and beyond roughly <span class="math inline">\(\pm2,75\)</span> is significant at the 1% level.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(simple_model)<span class="sc">$</span>coefficients[, <span class="st">"t value"</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>  (Intercept) hours_studied 
    411.92096      40.43612 </code></pre>
</div>
</div>
<p>In the simple regression model, the t-statistic for “hours_studied” is 40.44. This means the coefficient estimate is more than 40 standard errors away from zero. To put this in perspective, if study hours truly had no effect on exam scores, observing a t-statistic this large would be essentially impossible - it would be like flipping a fair coin and getting heads 40 times in a row, except far less likely even than that. This gives us overwhelming evidence that the relationship between study hours and exam scores is real.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(multiple_model)<span class="sc">$</span>coefficients[, <span class="st">"t value"</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>      (Intercept)     hours_studied        attendance   previous_scores 
      120.9845189        57.5165220        75.2621118        22.8088935 
      sleep_hours tutoring_sessions physical_activity 
       -0.8712218        19.9971244         4.8897684 </code></pre>
</div>
</div>
<p>In the multiple regression model, the t-statistics reveal a clear hierarchy of evidence. “attendance” has the largest t-statistic at 75.26, making it the most precisely estimated and most strongly significant predictor. “hours_studied” follows with t = 57.52, then “previous_scores” at 22.81, “tutoring_sessions” at 20.00, and “physical_activity” at 4.89. All of these are far beyond any conventional significance threshold. The exception, as we have seen, is “sleep_hours”, with a t-statistic of only -0.871. This value is well within the range we would expect to see even if the true coefficient were zero - it is less than one standard error away from zero, which is entirely unremarkable.</p>
</section>
<section id="the-p-value" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="the-p-value"><span class="header-section-number">2.3</span> The p-Value</h3>
<p>The <strong>p-value</strong> is the probability of observing a t-statistic as extreme as the one we actually computed, assuming that the null hypothesis is true. In other words, it answers the question: <em>if there were truly no relationship between this predictor and the response, how surprising would our observed result be?</em></p>
<p>A small p-value (typically below 0.05) indicates that the observed result would be very surprising under the null hypothesis, and we therefore reject the null hypothesis in favor of the alternative. A large p-value indicates that the observed result is not particularly surprising under the null hypothesis, and we therefore fail to reject it. It is important to emphasize that “failing to reject” is not the same as “accepting” the null hypothesis - it simply means we do not have enough evidence to conclude that a relationship exists.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(simple_model)<span class="sc">$</span>coefficients[, <span class="st">"Pr(&gt;|t|)"</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>  (Intercept) hours_studied 
 0.000000e+00 1.286349e-319 </code></pre>
</div>
</div>
<p>In our simple regression output, the p-value for “hours_studied” is less than <span class="math inline">\(2 \times 10^{-16}\)</span>, which R displays as “&lt;2e-16”. This is the smallest p-value that R can represent numerically, and it is so close to zero that for all practical purposes it means the probability of observing our results by chance alone is essentially zero.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(multiple_model)<span class="sc">$</span>coefficients[, <span class="st">"Pr(&gt;|t|)"</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>      (Intercept)     hours_studied        attendance   previous_scores 
     0.000000e+00      0.000000e+00      0.000000e+00     6.622656e-111 
      sleep_hours tutoring_sessions physical_activity 
     3.836647e-01      2.030741e-86      1.033431e-06 </code></pre>
</div>
</div>
<p>In the multiple regression, the p-values for “hours_studied”, “attendance”, “previous_scores”, and “tutoring_sessions” are all less than <span class="math inline">\(2 \times 10^{-16}\)</span>, and the p-value for “physical_activity” is approximately <span class="math inline">\(1.03 \times 10^{-6}\)</span> (or about one in a million). All of these are far below any conventional significance threshold, providing overwhelming evidence that these predictors are genuinely related to exam scores.</p>
<p>The p-value for “sleep_hours”, however, is 0.384. This means that if sleep hours truly had no effect on exam scores (after controlling for the other predictors), there would be about a 38.4% probability of observing a coefficient estimate as far from zero as the one we found. In other words, our observed result is entirely unsurprising under the null hypothesis - it is the kind of result we would expect to see by random chance alone roughly 38 times out of 100. We therefore have no grounds to reject the null hypothesis for “sleep_hours”, and we conclude that this variable does not have a statistically significant linear relationship with exam scores in this model.</p>
</section>
</section>
<section id="model-fit" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="model-fit"><span class="header-section-number">3</span> Model Fit</h2>
<p>In the previous section, we focused on assessing the accuracy of individual coefficient estimates - asking whether each specific predictor is significantly related to the response. Now we shift our perspective and ask a broader question: <em>how well does the model as a whole fit the data? In other words, once we have estimated our regression equation, how good is it at capturing the actual patterns in student exam scores? Is the model useful, or does it leave too much unexplained?</em></p>
<p>To answer these questions, we rely on three complementary statistics that appear at the bottom of every regression summary in R: the Residual Standard Error (RSE), the <span class="math inline">\(R^2\)</span> statistic, and the F-statistic. Each of these measures provides a different lens through which to evaluate the overall quality of the model, and together they give us a well-rounded picture of model performance.</p>
<section id="residual-standard-error" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="residual-standard-error"><span class="header-section-number">3.1</span> Residual Standard Error</h3>
<p>The <strong>Residual Standard Error</strong> (RSE) is perhaps the most intuitive measure of model accuracy, because it is expressed in the same units as the dependent variable. It estimates the standard deviation of the error term <span class="math inline">\(\epsilon\)</span> - that is, it tells us the typical size of the prediction errors our model makes. The RSE is computed using the formula:</p>
<p><span class="math inline">\(RSE = \sqrt{\frac{RSS}{n - p - 1}}\)</span></p>
<p>In this formula, RSS is the residual sum of squares (the sum of all squared residuals), n is the number of observations, and p is the number of predictors. The denominator uses <span class="math inline">\(n - p - 1\)</span> rather than simply n because we have used up <span class="math inline">\(p + 1\)</span> degrees of freedom in estimating the intercept and the p slope coefficients. This correction ensures that the RSE is an unbiased estimate of the true error standard deviation <span class="math inline">\(\sigma\)</span>. The concept of degrees of freedom can be understood intuitively: each parameter we estimate “uses up” one piece of information from the data, leaving fewer independent pieces of information available to estimate the error variance.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(simple_model)<span class="sc">$</span>sigma</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 3.483406</code></pre>
</div>
</div>
<p>In our simple regression model, the RSE is 3.483. This means that, on average, the actual exam scores deviate from the scores predicted by the model by approximately 3.48 points. Given that the mean exam score in our dataset is 67.24, we can express this as a percentage error of about 5.18% (3.483 / 67.24 × 100). Whether this level of error is acceptable depends entirely on the context of the research. In educational research, where human behavior is inherently variable and influenced by countless unmeasured factors like test-day anxiety, mood, or the specific questions that happened to appear on the exam, a prediction error of about 3.5 points may be considered quite reasonable. However, from a pure prediction standpoint, it also tells us that our simple model leaves a lot of room for improvement - knowing only how many hours a student studies does not allow us to predict their exam score with great precision.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(multiple_model)<span class="sc">$</span>sigma</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 2.467039</code></pre>
</div>
</div>
<p>In our multiple regression model, the RSE drops to 2.467. This represents a substantial improvement over the simple model - the average prediction error has decreased by about 29%, from 3.48 to 2.47 points. The percentage error relative to the mean is now approximately 3.67% (2.467 / 67.24 × 100). This decrease makes intuitive sense: by adding attendance, previous scores, tutoring sessions, and physical activity to the model, we have incorporated additional information that helps explain why some students score higher than others. The model now captures more of the systematic patterns in the data, leaving less to the error term. It is worth noting, however, that the RSE can never reach zero unless our model perfectly predicts every single observation, which is essentially impossible with real-world data involving human behavior. There will always be some irreducible error that no model can eliminate.</p>
<p>The RSE also plays a foundational role in the other inference tools we discussed earlier. Recall that the standard errors of the coefficient estimates depend on the RSE - a smaller RSE leads to smaller standard errors, which in turn leads to larger t-statistics and smaller p-values. This is exactly what we observed when comparing our two models: the multiple model had a smaller RSE, which produced more precise coefficient estimates and stronger evidence of statistical significance for the predictors that truly matter.</p>
<p>One important limitation of the RSE is that it is measured in the units of the response variable, which makes it difficult to compare across different studies or datasets with different response scales. If another researcher studied a test scored on a scale of 0 to 500, their RSE would naturally be much larger in absolute terms, even if their model were proportionally just as accurate as ours. This limitation motivates the need for a scale-independent measure of model fit, which is exactly what the <span class="math inline">\(R^2\)</span> statistic provides.</p>
</section>
<section id="r2" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="r2"><span class="header-section-number">3.2</span> <span class="math inline">\(R^2\)</span></h3>
<p>The <strong><span class="math inline">\(R^2\)</span></strong> statistic, also known as the coefficient of determination, is one of the most commonly reported measures of model fit in applied research. Unlike the RSE, <span class="math inline">\(R^2\)</span> is a proportion that always takes a value between 0 and 1, making it easy to interpret and compare across studies regardless of the scale of the response variable. <span class="math inline">\(R^2\)</span> answers a very specific question: <em>what fraction of the total variation in the response variable is explained by the model?</em> To understand <span class="math inline">\(R^2\)</span>, we need to consider two quantities.</p>
<p>The first is the <strong>Total Sum of Squares</strong> (TSS), defined as:</p>
<p><span class="math inline">\(TSS = \sum(y_i - \hat{y})^2\)</span></p>
<p>This measures the total variability in the response variable before any regression is performed. It is simply the sum of the squared deviations of each observed exam score from the overall mean exam score. In our dataset, this captures the full extent to which students’ exam scores differ from one another. Some of this variation is systematic and some of it is random noise.</p>
<p>The second quantity is the <strong>Residual Sum of Squares</strong> (RSS), which we have already encountered:</p>
<p><span class="math inline">\(RSS = \sum(y_i - \hat{y}_i)^2\)</span></p>
<p>This measures the variability that remains unexplained after fitting the regression model. It is the sum of the squared residuals - the squared differences between the actual exam scores and the scores predicted by the model.</p>
<p>The <span class="math inline">\(R^2\)</span> statistic is then defined as:</p>
<p><span class="math inline">\(R^2 = \frac{(TSS - RSS)}{TSS} = 1 - \frac{RSS}{TSS}\)</span></p>
<p>The numerator, TSS - RSS, represents the amount of variability in the response that is explained by the regression - it is the reduction in prediction error achieved by using the model instead of simply predicting the mean for every student. Dividing by TSS converts this into a proportion.</p>
<p>When <span class="math inline">\(R^2\)</span> is close to 1, it means that the model explains nearly all of the variation in the response, and the RSS is very small compared to the TSS. In such a case, the predicted values <span class="math inline">\(\hat{y}_i\)</span> are very close to the actual values <span class="math inline">\(y_i\)</span>, and the model provides an excellent fit. When <span class="math inline">\(R^2\)</span> is close to 0, the model explains very little of the variation, and using the model is hardly better than simply predicting the mean exam score for every student.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(simple_model)<span class="sc">$</span>r.squared</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.1984301</code></pre>
</div>
</div>
<p>In our simple regression model, <span class="math inline">\(R^2\)</span> is 0.1984. This tells us that “hours_studied” alone explains approximately 19.84% of the total variation in exam scores. In other words, about one-fifth of the differences in exam scores among students can be attributed to differences in how many hours they study. This is a meaningful finding - it confirms that study time matters - but it also reveals that roughly 80% of the variation is driven by other factors not captured in this simple model.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(multiple_model)<span class="sc">$</span>r.squared</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5982496</code></pre>
</div>
</div>
<p>In our multiple regression model, <span class="math inline">\(R^2\)</span> jumps to 0.5982. Now the model explains approximately 59.82% of the variation in exam scores. This is a dramatic improvement - by adding attendance, previous scores, tutoring sessions, and physical activity as predictors alongside study hours, we have nearly tripled the proportion of explained variance. The remaining approximately 40% of the variation is still unexplained, presumably due to factors that are not included as quantitative predictors in this model - things like motivation level, family income, teacher quality, peer influence, and other qualitative variables in our dataset that we have not yet incorporated, as well as entirely unmeasured factors like test anxiety, the specific content of the exam, or simple luck.</p>
<p>What constitutes a “good” <span class="math inline">\(R^2\)</span> depends heavily on the field of study. In the physical sciences, where experiments can be tightly controlled and measurement is very precise, <span class="math inline">\(R^2\)</span> values above 0.95 are common and expected. In the social sciences and education research, where human behavior is inherently noisy and influenced by a vast number of interacting factors, <span class="math inline">\(R^2\)</span> values between 0.30 and 0.60 are often considered quite good for observational studies. Our multiple model’s <span class="math inline">\(R^2\)</span> of 0.598 is therefore quite respectable for educational data - it suggests that we have identified a set of predictors that genuinely capture a large portion of what drives student performance.</p>
<p>It is critical to understand one important caveat about <span class="math inline">\(R^2\)</span>: it will always increase (or at least never decrease) when more predictors are added to the model, even if those predictors are completely unrelated to the response. This happens because adding any variable, even a random one, gives the model more flexibility to fit the training data, and the RSS can only go down or stay the same - it can never go up. This means that a high <span class="math inline">\(R^2\)</span> does not necessarily indicate a good model; it could simply reflect the fact that many predictors have been thrown in. To guard against this problem, the <strong>Adjusted <span class="math inline">\(R^2\)</span></strong> was developed. The Adjusted <span class="math inline">\(R^2\)</span> modifies the standard <span class="math inline">\(R^2\)</span> by imposing a penalty for each additional predictor. If adding a new predictor does not reduce the RSS enough to offset the penalty for the lost degree of freedom, the Adjusted <span class="math inline">\(R^2\)</span> will actually decrease, signaling that the added predictor is not contributing meaningfully.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(simple_model)<span class="sc">$</span>adj.r.squared</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.1983088</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(multiple_model)<span class="sc">$</span>adj.r.squared</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5978844</code></pre>
</div>
</div>
<p>In our simple model, the Adjusted <span class="math inline">\(R^2\)</span> is 0.1983, virtually identical to <span class="math inline">\(R^2\)</span> because there is only one predictor and the penalty is negligible. In our multiple model, the Adjusted <span class="math inline">\(R^2\)</span> is 0.5979, also nearly identical to the regular <span class="math inline">\(R^2\)</span> of 0.5982. The fact that the Adjusted <span class="math inline">\(R^2\)</span> barely differs from <span class="math inline">\(R^2\)</span> in the multiple model tells us that all six predictors (or at least most of them) are genuinely contributing to the model’s explanatory power - the improvement in fit is not merely an artifact of adding more variables.</p>
<p>It is also worth noting the connection between <span class="math inline">\(R^2\)</span> and correlation. In simple linear regression, <span class="math inline">\(R^2\)</span> is exactly equal to the square of the Pearson correlation coefficient r between X and Y. In our case, <span class="math inline">\(R^2\)</span> = 0.1984 for the simple model, so the correlation between “hours_studied” and “exam_score” is <span class="math inline">\(r = \sqrt{0.1984} \approx 0.445\)</span>. In multiple regression, this simple relationship no longer holds (because there are multiple predictors), but <span class="math inline">\(R^2\)</span> can be shown to equal the squared correlation between the observed values <span class="math inline">\(y_i\)</span> and the fitted values <span class="math inline">\(\hat{y}_i\)</span>. This provides a nice intuitive interpretation: <span class="math inline">\(R^2\)</span> tells us how closely the model’s predictions track the actual outcomes.</p>
</section>
<section id="f-statistic" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="f-statistic"><span class="header-section-number">3.3</span> F-statistic</h3>
<p>While the t-statistic and its associated p-value allow us to test whether each individual predictor is significantly related to the response, the <strong>F-statistic</strong> addresses a different and more fundamental question: <em>is the model as a whole useful?</em> Specifically, the F-statistic tests the null hypothesis that all slope coefficients in the model are simultaneously equal to zero <span class="math inline">\(H_0: \beta_1 = \beta_2 = ... = \beta_p = 0\)</span> against the alternative hypothesis:</p>
<p><span class="math inline">\(H_a\)</span>: at least one <span class="math inline">\(\beta_j\)</span> is non-zero.</p>
<p>If the null hypothesis is true, then none of the predictors have any relationship with the response, and the model is no better than simply predicting the mean for every observation. The F-statistic is computed as:</p>
<p><span class="math inline">\(F = \frac{\frac{TSS - RSS}{p}}{\frac{RSS}{n - p -1}}\)</span></p>
<p>The numerator measures how much of the total variance the model explains, divided by the number of predictors p.&nbsp;The denominator measures how much variance remains unexplained, divided by the residual degrees of freedom. If the model is no better than chance, the numerator and denominator should be roughly equal, producing an F-statistic close to 1. If the model captures real patterns in the data, the numerator will be much larger than the denominator, producing a large F-statistic.</p>
<p>One might reasonably ask: why do we need the F-statistic at all when we already have individual t-tests for each coefficient? The answer lies in the multiple testing problem. When we have many predictors, each with its own t-test, the probability of finding at least one “significant” result by pure chance increases dramatically. For example, if we tested 100 completely useless predictors at the 5% significance level, we would expect about 5 of them to appear significant purely by chance. The F-statistic avoids this problem because it is a single, omnibus test that accounts for the total number of predictors. It maintains the correct overall error rate regardless of how many predictors are in the model. So the proper approach is to first check the F-statistic to determine whether the model as a whole is significant, and only then examine the individual t-statistics to identify which specific predictors are contributing.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb38"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(simple_model)<span class="sc">$</span>fstatistic</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>  value   numdf   dendf 
1635.08    1.00 6605.00 </code></pre>
</div>
</div>
<p>In our simple regression model, the F-statistic is 1,635 with 1 and 6,605 degrees of freedom, and the associated p-value is less than <span class="math inline">\(2.2 \times 10^{16}\)</span>. Since we have only one predictor in the simple model, the F-test is equivalent to the t-test for that predictor. In fact, the F-statistic in simple regression is exactly the square of the t-statistic: <span class="math inline">\(40.44^2 \approx 1,635\)</span>. The overwhelming magnitude of this F-statistic and its essentially zero p-value tell us that the model is highly significant - “hours_studied” is unquestionably related to “exam_score”.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb40"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(multiple_model)<span class="sc">$</span>fstatistic</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>   value    numdf    dendf 
1638.019    6.000 6600.000 </code></pre>
</div>
</div>
<p>In our multiple regression model, the F-statistic is 1,638 with 6 and 6,600 degrees of freedom, and the p-value is again less than <span class="math inline">\(2.2 \times 10^{-16}\)</span>. This tests whether at least one of the six predictors is related to exam scores. The result decisively rejects the null hypothesis - the model as a whole is highly significant, and at least one (and as we saw from the individual t-tests, five out of six) predictors are genuinely related to student performance. The fact that the F-statistic is 1,638 rather than close to 1 tells us that the model explains vastly more variance than we would expect by chance alone.</p>
<p>It is worth pausing to note an interesting detail. Even though “sleep_hours” was not individually significant (p = 0.384), the overall F-test is still overwhelmingly significant. This is entirely consistent: the F-test only requires that at least one predictor be related to the response, and the other five predictors more than satisfy this requirement. The F-test does not tell us which predictors are significant - that is the job of the individual t-tests. But it does tell us that the model as a whole is capturing real patterns.</p>
</section>
</section>
<section id="interaction-terms" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="interaction-terms"><span class="header-section-number">4</span> Interaction Terms</h2>
<p>Up to this point, all the linear regression models we have discussed have assumed something that may seem natural but is in fact a very strong assumption: that the effect of each predictor on the response is independent of the values of the other predictors. This is called the <strong>additivity assumption</strong>, and it is built into the standard multiple linear regression model. When we write our model as <span class="math inline">\(exam_score = \beta_0 + \beta_1 \times hours_studied + \beta_2 \times attendance + \epsilon\)</span>, we are implicitly saying that the effect of studying one additional hour is always the same - roughly 0.29 additional points - regardless of whether a student has 60% attendance or 100% attendance. Similarly, we are saying that the effect of one additional percentage point of attendance is always the same, regardless of how many hours the student studies. The model treats each predictor’s contribution as completely separate and simply adds them together, which is where the term “additive” comes from.</p>
<p>But is this assumption realistic? Consider the following scenario. A student who attends nearly all classes has been exposed to lectures, discussions, and in-class explanations throughout the course. When this student sits down to study at home, each hour of studying is highly productive, because the student is reinforcing and deepening material they have already encountered in the classroom. Now consider a student who has very low attendance and has missed most of the lectures. When this student tries to study, each hour of studying may be less productive, because the student must learn the material from scratch rather than building on what was covered in class. In this scenario, the effect of study hours on exam scores depends on attendance - studying is more effective for students who also attend class regularly. This is exactly the kind of phenomenon that the additive model cannot capture, and it is what we call an <strong>interaction effect</strong>. In statistics, an interaction effect occurs when the relationship between one predictor and the response changes depending on the value of another predictor.</p>
<p>To incorporate an interaction effect into a linear regression model, we create a new predictor variable that is the product of two existing predictors. Standard additive model with two predictors assumes that the effect of <span class="math inline">\(X_1\)</span> on Y is <span class="math inline">\(\beta_1\)</span>, regardless of the value of <span class="math inline">\(X_2\)</span>. To relax this assumption, we add a third term - the interaction term - which is simply the product <span class="math inline">\(X_1 \times X_2\)</span>:</p>
<p><span class="math inline">\(Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3(X_1 \times X_2) + \epsilon\)</span></p>
<p>The key to understanding how this works is to rearrange the equation algebraically. We can rewrite it as:</p>
<p><span class="math inline">\(Y = \beta_0 + (\beta_1 + \beta_3X_2)X_1 + \beta_2X_2 + \epsilon\)</span></p>
<p>Written in this form, it becomes clear that the effective slope of <span class="math inline">\(X_1\)</span> is no longer a fixed constant <span class="math inline">\(\beta_1\)</span> - it is now <span class="math inline">\((\beta_1 + \beta_3X_2)\)</span>, which depends on the value of <span class="math inline">\(X_2\)</span>. In other words, the effect of <span class="math inline">\(X_1\)</span> on Y changes as <span class="math inline">\(X_2\)</span> changes. If <span class="math inline">\(\beta_3\)</span> is positive, then higher values of <span class="math inline">\(X_2\)</span> amplify the effect of <span class="math inline">\(X_1\)</span>. If <span class="math inline">\(\beta_3\)</span> is negative, then higher values of <span class="math inline">\(X_2\)</span> diminish the effect of <span class="math inline">\(X_1\)</span>. And if <span class="math inline">\(\beta_3\)</span> is zero, then the interaction is absent and we are back to the simple additive model.</p>
<p>The same rearrangement works in the other direction. We can also write the model as:</p>
<p><span class="math inline">\(Y = \beta_0 + \beta_1X_1 + (\beta_2 + \beta_3X_1)X_2 + \epsilon\)</span></p>
<p>This shows that the effect of <span class="math inline">\(X_2\)</span> on Y is <span class="math inline">\((\beta_2 + \beta_3X_1)\)</span>, which depends on <span class="math inline">\(X_1\)</span>. The interaction is symmetric: if the effect of study hours depends on attendance, then equally the effect of attendance depends on study hours. The interaction coefficient <span class="math inline">\(\beta_3\)</span> captures this mutual dependence.</p>
<p>In our Student Performance example, an interaction between Hours_Studied and Attendance would be expressed as:</p>
<p>exam_score = <span class="math inline">\(\beta_0\)</span> + <span class="math inline">\(\beta_1\)</span> <span class="math inline">\(\times\)</span> hours_studied + <span class="math inline">\(\beta_2\)</span> <span class="math inline">\(\times\)</span> attendance + <span class="math inline">\(\beta_3\)</span> <span class="math inline">\(\times\)</span> (hours_studied <span class="math inline">\(\times\)</span> attendance) + <span class="math inline">\(\epsilon\)</span></p>
<p>The coefficient <span class="math inline">\(\beta_3\)</span> would tell us how the effectiveness of studying changes as attendance increases (or equivalently, how the effectiveness of attendance changes as study hours increase). If <span class="math inline">\(\beta_3\)</span> is positive and significant, it would confirm our intuition that studying and attending class work together synergistically - each one makes the other more effective.</p>
<p>In R, there are two convenient ways to specify interaction terms. The syntax <code>hours_studied:attendance</code> includes only the interaction term itself, while the syntax <code>hours_studied * attendance</code> is a shorthand that automatically includes both the individual predictors (called main effects) and their interaction. In other words, <code>hours_studied * attendance</code> is equivalent to writing <code>hours_studied + attendance + hours_studied:attendance</code>.</p>
<p>We have now fitted three models that allow us to progressively examine whether an interaction exists between “hours_studied” and “attendance” in predicting “exam_score”. The results tell a clear and instructive story - one that is just as valuable for what it does not find as for what it does find.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb42"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>additive_model <span class="ot">&lt;-</span> <span class="fu">lm</span>(</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>    exam_score <span class="sc">~</span> hours_studied <span class="sc">+</span> attendance,</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> student_performance</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(additive_model)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = exam_score ~ hours_studied + attendance, data = student_performance)

Residuals:
    Min      1Q  Median      3Q     Max 
-6.0523 -1.3295 -0.1674  1.0310 31.5633 

Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   45.604170   0.252314  180.74   &lt;2e-16 ***
hours_studied  0.293058   0.005413   54.14   &lt;2e-16 ***
attendance     0.197275   0.002808   70.25   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 2.635 on 6604 degrees of freedom
Multiple R-squared:  0.5413,    Adjusted R-squared:  0.5411 
F-statistic:  3896 on 2 and 6604 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>Our additive model serves as the baseline. It estimates the following equation:</p>
<p>exam_score ≈ 45.60 + 0.293 <span class="math inline">\(\times\)</span> hours_studied + 0.197 <span class="math inline">\(\times\)</span> attendance</p>
<p>In this model, the effect of each additional hour of study is always 0.293 points, regardless of how often the student attends class. And the effect of each additional percentage point of attendance is always 0.197 points, regardless of how many hours the student studies. The two predictors operate independently - their contributions are simply added together. The model explains 54.13% of the variance in exam scores (<span class="math inline">\(R^2\)</span> = 0.5413), with a residual standard error of 2.635. This additive interpretation may or may not reflect reality. It is possible that studying and attending class reinforce each other - that the benefit of studying is amplified when a student has also been attending lectures regularly. The interaction model tests this possibility directly.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb44"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>interaction_model <span class="ot">&lt;-</span> <span class="fu">lm</span>(</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>    exam_score <span class="sc">~</span> hours_studied <span class="sc">*</span> attendance,</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> student_performance</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(interaction_model)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = exam_score ~ hours_studied * attendance, data = student_performance)

Residuals:
    Min      1Q  Median      3Q     Max 
-6.0118 -1.3290 -0.1687  1.0450 31.5866 

Coefficients:
                          Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)              4.693e+01  7.788e-01  60.253  &lt; 2e-16 ***
hours_studied            2.266e-01  3.742e-02   6.055 1.49e-09 ***
attendance               1.808e-01  9.624e-03  18.781  &lt; 2e-16 ***
hours_studied:attendance 8.308e-04  4.628e-04   1.795   0.0727 .  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 2.635 on 6603 degrees of freedom
Multiple R-squared:  0.5415,    Adjusted R-squared:  0.5413 
F-statistic:  2599 on 3 and 6603 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb46"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(interaction_model)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>                                 2.5 %       97.5 %
(Intercept)               4.539998e+01 48.453506206
hours_studied             1.532255e-01  0.299954598
attendance                1.618848e-01  0.199618365
hours_studied:attendance -7.655732e-05  0.001738105</code></pre>
</div>
</div>
<p>The interaction model adds the product of Hours_Studied and Attendance as a new predictor. The estimated equation is:</p>
<p>exam_score <span class="math inline">\(\approx\)</span> 46.93 + 0.227 <span class="math inline">\(\times\)</span> hours_studied + 0.181 <span class="math inline">\(\times\)</span> attendance + 0.000831 <span class="math inline">\(\times\)</span> (hours_studied <span class="math inline">\(\times\)</span> attendance)</p>
<p>Let us carefully examine each coefficient and what it means in the context of this model, because the interpretation of coefficients changes fundamentally when an interaction term is present. The intercept of 46.93 represents the predicted exam score when both “hours_studied” and “attendance” are zero. As always, this is a mathematical anchor point for the model rather than a substantively meaningful quantity, since no real student has zero hours of study and zero percent attendance. The coefficient for “hours_studied” is now 0.227, which is noticeably different from its value in the additive model (0.293). However, the interpretation of this coefficient has changed. In the additive model, 0.293 represented the effect of study hours at any level of attendance.</p>
<p>In the interaction model, 0.227 represents the effect of study hours specifically when “attendance” equals zero. This is because when we rearrange the interaction model as exam_score = 46.93 + (0.227 + 0.000831 <span class="math inline">\(\times\)</span> attendance) <span class="math inline">\(\times\)</span> hours_studied + 0.181 <span class="math inline">\(\times\)</span> attendance we can see that the effective slope for “hours_studied” is (0.227 + 0.000831 <span class="math inline">\(\times\)</span> attendance). When “attendance” is zero, this reduces to 0.227. When “attendance” is at its mean of about 80, the effective slope becomes 0.227 + 0.000831 × 80 = 0.227 + 0.066 = 0.293 - which is almost exactly the slope we found in the additive model. This is reassuring and makes intuitive sense: the additive model’s coefficient represents a kind of average effect across all attendance levels, and that average coincides with the effective slope at the mean level of attendance.</p>
<p>Similarly, the coefficient for “attendance” is 0.181, which represents the effect of attendance when “hours_studied” equals zero. The effective slope for “attendance” at the mean study hours of about 20 is 0.181 + 0.000831 <span class="math inline">\(\times\)</span> 20 = 0.181 + 0.017 = 0.198 - again, essentially the same as in the additive model.</p>
<p>The interaction coefficient itself is 0.000831. This is the critical number. It tells us how the effect of one predictor changes for each one-unit increase in the other. Specifically, for each additional percentage point of attendance, the effect of one hour of study increases by 0.000831 points. Conversely, for each additional hour of study, the effect of one percentage point of attendance increases by 0.000831 points. In concrete terms, this means that a student with 90% attendance gains 0.227 + 0.000831 <span class="math inline">\(\times\)</span> 90 = 0.302 points per hour of study, while a student with 70% attendance gains 0.227 + 0.000831 <span class="math inline">\(\times\)</span> 70 = 0.285 points per hour of study. The difference is 0.017 points per hour - a very small amount.</p>
<p>Now, the crucial question is whether this interaction effect is statistically significant. The t-statistic for the interaction term is 1.795, and the p-value is 0.0727. This p-value is above the conventional 0.05 significance threshold, although it is below 0.10. The 95% confidence interval for the interaction coefficient is [-0.0000766, 0.001738], which includes zero. This means that at the 5% significance level, we cannot reject the null hypothesis that the interaction coefficient is zero. The evidence for an interaction is suggestive but not strong enough to meet the conventional standard of statistical significance.</p>
<p>Looking at the model-level statistics reinforces this conclusion. The R² of the interaction model is 0.5415, compared to 0.5413 for the additive model. The increase is only 0.0002 - an almost imperceptible improvement. The residual standard error remains at 2.635, completely unchanged. Adding the interaction term has contributed virtually nothing to the model’s explanatory power.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb48"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>full_interaction_model <span class="ot">&lt;-</span> <span class="fu">lm</span>(</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>    exam_score <span class="sc">~</span> hours_studied <span class="sc">*</span> attendance <span class="sc">+</span> previous_scores <span class="sc">+</span> tutoring_sessions <span class="sc">+</span> physical_activity,</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> student_performance</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(full_interaction_model)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = exam_score ~ hours_studied * attendance + previous_scores + 
    tutoring_sessions + physical_activity, data = student_performance)

Residuals:
    Min      1Q  Median      3Q     Max 
-5.4580 -1.1344 -0.1632  0.8570 31.0587 

Coefficients:
                          Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)              4.181e+01  7.570e-01  55.238  &lt; 2e-16 ***
hours_studied            2.406e-01  3.504e-02   6.867 7.16e-12 ***
attendance               1.854e-01  9.014e-03  20.563  &lt; 2e-16 ***
previous_scores          4.810e-02  2.109e-03  22.804  &lt; 2e-16 ***
tutoring_sessions        4.938e-01  2.467e-02  20.014  &lt; 2e-16 ***
physical_activity        1.436e-01  2.945e-02   4.875 1.11e-06 ***
hours_studied:attendance 6.363e-04  4.334e-04   1.468    0.142    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 2.467 on 6600 degrees of freedom
Multiple R-squared:  0.5983,    Adjusted R-squared:  0.598 
F-statistic:  1639 on 6 and 6600 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb50"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(full_interaction_model)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>                                 2.5 %       97.5 %
(Intercept)              40.3298287765 43.297645958
hours_studied             0.1719352716  0.309320955
attendance                0.1676866849  0.203027537
previous_scores           0.0439690360  0.052239444
tutoring_sessions         0.4454663363  0.542206078
physical_activity         0.0858242758  0.201276450
hours_studied:attendance -0.0002133552  0.001485881</code></pre>
</div>
</div>
<p>Our third model includes the interaction between Hours_Studied and Attendance alongside three additional predictors: “previous_scores”, “tutoring_sessions”, and “physical_activity”. The estimated equation is:</p>
<p>exam_score <span class="math inline">\(\approx\)</span> 41.81 + 0.241 <span class="math inline">\(\times\)</span> hours_studied + 0.185 <span class="math inline">\(\times\)</span> attendance + 0.048 <span class="math inline">\(\times\)</span> previous_scores + 0.494 <span class="math inline">\(\times\)</span> tutoring_sessions + 0.144 <span class="math inline">\(\times\)</span> physical_activity + 0.000636 <span class="math inline">\(\times\)</span> (hours_studied <span class="math inline">\(\times\)</span> attendance)</p>
<p>In this richer model, the interaction coefficient has shrunk further to 0.000636, the t-statistic has decreased to 1.468, and the p-value has increased to 0.142. The 95% confidence interval is [-0.000213, 0.001486], which now spans zero even more broadly than before. The interaction effect is clearly not statistically significant in this model, and its magnitude is even smaller than in Model 2.</p>
<p>The <span class="math inline">\(R^2\)</span> of this full interaction model is 0.5983, compared to 0.5982 for the multiple regression model without the interaction term that we fitted earlier. The difference is 0.0001 — the interaction term adds essentially no explanatory power beyond what is already captured by the main effects and the other predictors. The residual standard error is 2.467, identical to the model without the interaction.</p>
<p>The <code>anova()</code> function at the end performs a formal comparison between the additive model and the interaction model. It tests whether the addition of the interaction term leads to a statistically significant improvement in model fit, using an F-test that compares the RSS of the two models. If the interaction term significantly reduces the RSS, the F-statistic will be large and the p-value will be small, indicating that the interaction is a meaningful addition to the model.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb52"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(additive_model, interaction_model)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Analysis of Variance Table

Model 1: exam_score ~ hours_studied + attendance
Model 2: exam_score ~ hours_studied * attendance
  Res.Df   RSS Df Sum of Sq      F  Pr(&gt;F)  
1   6604 45868                              
2   6603 45846  1    22.369 3.2217 0.07271 .
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<p>The ANOVA output provides a formal statistical test that directly compares our two nested models: Model 1, the additive model with only “hours_studied” and “attendance” as separate predictors, and Model 2, the interaction model that adds the hours_studied <span class="math inline">\(\times\)</span> “attendance” interaction term. The word “nested” is important here - it means that Model 1 is a special case of Model 2, obtained by setting the interaction coefficient to zero. The ANOVA test asks whether allowing that coefficient to be non-zero produces a meaningfully better fit to the data.</p>
<p>The output shows us the residual degrees of freedom and the residual sum of squares (RSS) for each model. Model 1 has 6,604 residual degrees of freedom and an RSS of 45,868. Model 2 has 6,603 residual degrees of freedom and an RSS of 45,846. The difference in RSS between the two models is 22.369, which represents the amount of additional variance in exam scores that is explained by including the interaction term. The column labeled “Df” shows that the difference is 1 degree of freedom, confirming that exactly one additional parameter was added.</p>
<p>The F-statistic for this comparison is 3.2217. Recall from our earlier discussion that the F-statistic is constructed by comparing the improvement in fit (the reduction in RSS) to the amount of variance that remains unexplained. Specifically, it takes the reduction in RSS per additional parameter (22.369 / 1 = 22.369) and divides it by the residual mean square of the fuller model (45,846 / 6,603 <span class="math inline">\(\approx\)</span> 6.943). The resulting F-value of 3.22 tells us that the interaction term reduced the RSS by about 3.22 times more than what a single random, useless predictor would be expected to reduce it. This is a modest improvement - certainly not trivial, but not overwhelming either.</p>
<p>The p-value is 0.07271, which is shown with a single dot (.) next to it in R’s significance coding system, indicating that it falls between 0.05 and 0.10. This p-value means that if the true interaction coefficient were zero (that is, if there were truly no interaction between studying and attendance in the population), there would be approximately a 7.3% probability of observing an improvement in fit as large as or larger than the one we found. At the conventional 5% significance level, we do not reject the null hypothesis - the interaction term does not provide a statistically significant improvement in model fit. The evidence is suggestive, sitting in that ambiguous zone between 0.05 and 0.10, but it does not meet the standard threshold for statistical significance.</p>
<p>It is worth noting how perfectly consistent this ANOVA result is with the individual coefficient test for the interaction term that we saw in Model 2’s summary output. The t-statistic for the interaction coefficient was 1.795, and its p-value was 0.0727 - virtually identical to the ANOVA p-value. This is not a coincidence. When we compare two models that differ by exactly one predictor, the ANOVA F-statistic is exactly the square of the t-statistic for that predictor (<span class="math inline">\(1.795^2\)</span> <span class="math inline">\(\approx\)</span> 3.222), and the p-values are identical. The two tests are mathematically equivalent in this case. The ANOVA approach becomes particularly valuable when we want to compare models that differ by more than one predictor - for instance, if we wanted to test whether a whole set of interaction terms simultaneously improves the model, we could not do this with individual t-tests, but we could do it with a single ANOVA F-test.</p>
<p>To put the practical magnitude of this result in perspective, the interaction term reduced the RSS from 45,868 to 45,846 - a reduction of just 22.369 out of a total RSS of 45,868, which amounts to a 0.049% reduction in unexplained variance. The <span class="math inline">\(R^2\)</span> increased from 0.5413 to 0.5415, a gain of 0.0002. These numbers confirm what the p-value already suggested: even if the interaction were real, its practical importance would be negligible. The additive model, which is simpler and easier to interpret, captures the relationships between study hours, attendance, and exam scores just as effectively as the interaction model does. For this reason, both on statistical grounds and on grounds of parsimony, we would choose to proceed with the additive model and conclude that “hours_studied” and “attendance” contribute independently to student performance in this dataset.</p>
<p>The decision to include interaction terms should be guided by both theory and evidence. From a theoretical standpoint, we should include interactions when we have a substantive reason to believe that the effect of one predictor depends on the value of another. In educational research, for example, we might hypothesize that the effect of tutoring depends on motivation level (highly motivated students may benefit more from tutoring), or that the effect of parental involvement depends on family income (parental involvement may matter more in lower-income families where other resources are scarce). These are theoretically grounded hypotheses that deserve to be tested.</p>
<p>From an empirical standpoint, we should look for evidence in the data - specifically, a significant interaction coefficient with a meaningful magnitude. In our case, the evidence does not support the interaction between “hours_studied” and “attendance”: the coefficient is tiny, the p-value exceeds 0.05, the confidence interval includes zero, and the <span class="math inline">\(R^2\)</span> improvement is negligible. The additive model is therefore preferred on grounds of parsimony - it is simpler, easier to interpret, and fits the data just as well. Including a non-significant interaction term would add unnecessary complexity to the model without any compensating benefit in explanatory power or predictive accuracy.</p>
</section>
<section id="hierarchical-linear-regression" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="hierarchical-linear-regression"><span class="header-section-number">5</span> Hierarchical Linear Regression</h2>
<p>In all the models we have fitted so far, we have made decisions about which predictors to include based on theoretical reasoning and then examined the results as a single, complete model. But in social science research, we are often interested in something more nuanced than simply knowing which predictors are significant. We want to understand how different groups of factors contribute to the outcome, and specifically, whether adding a new group of factors improves our ability to explain the response above and beyond what was already explained by previously entered factors. This is the core logic behind hierarchical regression, also known as sequential regression or blockwise regression.</p>
<p><strong>Hierarchical regression</strong> or sequential regression is not a fundamentally different statistical technique from the multiple linear regression we have already discussed. It uses exactly the same least squares estimation, the same coefficient estimates, the same standard errors, and the same t-statistics. What makes it distinctive is the strategy for entering predictors into the model. Rather than entering all predictors at once, the researcher builds the model in a series of deliberate steps - called blocks or stages - adding one group of theoretically related predictors at each step. After each block is added, the researcher examines how the model’s explanatory power changes, paying particular attention to the change in <span class="math inline">\(R^2\)</span> (denoted <span class="math inline">\(\Delta R^2\)</span>). This approach allows us to ask questions like: how much additional variance in exam scores do study habits explain, after we have already accounted for students’ baseline academic ability? Or: does the school environment contribute anything meaningful once we already know about students’ personal characteristics and study behavior?</p>
<p>The order in which blocks are entered is not arbitrary - it should be guided by theory, prior research, or the specific research questions being investigated. Typically, researchers enter more fundamental, stable, or demographic variables first, and then add variables that are more proximal, malleable, or of primary theoretical interest in subsequent blocks. The rationale is that by entering background variables first, we establish a baseline, and then we can see whether the variables we are most interested in contribute explanatory power beyond what those background factors already provide.</p>
<p>For our <code>student_performance</code> dataset, a theoretically motivated hierarchical regression might proceed as follows. In the first block, we would enter a variable that captures students’ baseline academic ability - “previous_scores”. This is the most fundamental predictor, as it reflects everything a student brings to the table before the current course even begins: their prior knowledge, their learning capacity, and their historical academic trajectory. By entering this first, we establish how much of the variation in exam scores is explained simply by pre-existing differences in ability.</p>
<p>In the second block, we would add variables related to study behavior and engagement - “hours_studied” and “attendance”. These represent the deliberate efforts a student makes during the course. The key question at this stage is: do study habits and class attendance explain additional variance in exam scores above and beyond what is already explained by baseline ability? If <span class="math inline">\(\Delta R^2\)</span> is large and significant at this step, it tells us that what students do during the course matters over and above what they could do coming in.</p>
<p>In the third block, we would add variables related to academic support - “tutoring_sessions”. This captures the additional help a student receives outside of regular classes and personal study. The question here is whether external academic support adds anything once we already know about baseline ability and personal effort.</p>
<p>In the fourth and final block, we would add variables related to lifestyle and well-being - “sleep_hours” and “physical_activity”. These are factors that are somewhat more distal from academic performance, and we are interested in whether they contribute any additional explanatory power after the more directly academic factors have already been accounted for.</p>
<p>In R, hierarchical regression is implemented by fitting a series of separate <code>lm()</code> models, each adding a new block of predictors to the previous model. We then compare the models using the <code>anova()</code> function, which performs an F-test for the significance of the improvement at each step, and we manually compute the change in <span class="math inline">\(R^2\)</span> between steps.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb54"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>block1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>    exam_score <span class="sc">~</span> previous_scores,</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> student_performance</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>block2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(</span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>    exam_score <span class="sc">~</span> previous_scores <span class="sc">+</span> hours_studied <span class="sc">+</span> attendance,</span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> student_performance</span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a>block3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(</span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a>    exam_score <span class="sc">~</span> previous_scores <span class="sc">+</span> hours_studied <span class="sc">+</span> attendance <span class="sc">+</span> tutoring_sessions,</span>
<span id="cb54-13"><a href="#cb54-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> student_performance</span>
<span id="cb54-14"><a href="#cb54-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb54-15"><a href="#cb54-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-16"><a href="#cb54-16" aria-hidden="true" tabindex="-1"></a>block4 <span class="ot">&lt;-</span> <span class="fu">lm</span>(</span>
<span id="cb54-17"><a href="#cb54-17" aria-hidden="true" tabindex="-1"></a>    exam_score <span class="sc">~</span> previous_scores <span class="sc">+</span> hours_studied <span class="sc">+</span> attendance <span class="sc">+</span> tutoring_sessions <span class="sc">+</span> sleep_hours <span class="sc">+</span> physical_activity,</span>
<span id="cb54-18"><a href="#cb54-18" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> student_performance</span>
<span id="cb54-19"><a href="#cb54-19" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb54-20"><a href="#cb54-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-21"><a href="#cb54-21" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(block1, block2, block3, block4)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Analysis of Variance Table

Model 1: exam_score ~ previous_scores
Model 2: exam_score ~ previous_scores + hours_studied + attendance
Model 3: exam_score ~ previous_scores + hours_studied + attendance + tutoring_sessions
Model 4: exam_score ~ previous_scores + hours_studied + attendance + tutoring_sessions + 
    sleep_hours + physical_activity
  Res.Df   RSS Df Sum of Sq        F    Pr(&gt;F)    
1   6605 96921                                    
2   6603 42779  2     54143 4447.924 &lt; 2.2e-16 ***
3   6602 40320  1      2459  404.023 &lt; 2.2e-16 ***
4   6600 40169  2       150   12.338 4.484e-06 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb56"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(block1)<span class="sc">$</span>r.squared</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.03065269</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb58"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(block2)<span class="sc">$</span>r.squared</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5721542</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb60"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(block3)<span class="sc">$</span>r.squared</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5967476</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb62"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(block4)<span class="sc">$</span>r.squared</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5982496</code></pre>
</div>
</div>
<p>The first block contains only “previous_scores”. This variable serves as our baseline, capturing the academic ability and preparation that each student brings into the current course before any of the other factors come into play. The <span class="math inline">\(R^2\)</span> for this model is 0.0307, meaning that previous academic performance explains only about 3.07% of the variation in current exam scores. This is a notably small number, and it deserves careful interpretation. One might have expected prior scores to be a powerful predictor of current performance - after all, students who did well in the past tend to do well again. And indeed, if we look back at our earlier multiple regression results, the coefficient for “previous_scores” was statistically significant with a t-value of 22.81. But statistical significance and explanatory power are different things. The coefficient for “previous_scores” was 0.048, meaning that a 10-point advantage in prior scores translates to less than half a point on the current exam. The relationship is real but modest in magnitude, and the variable by itself leaves approximately 97% of the variation in exam scores unexplained. This tells us something substantively important: in this dataset, what a student has done before is a relatively weak predictor of what they will achieve now. The current course’s exam score is primarily determined by factors other than historical performance.</p>
<p>In the second block, we add “hours_studied” and “attendance”. The <span class="math inline">\(R^2\)</span> jumps dramatically from 0.0307 to 0.5722. The change in <span class="math inline">\(R^2\)</span>, <span class="math inline">\(\Delta R^2\)</span> = 0.5722 − 0.0307 = 0.5415, is enormous. Adding study hours and attendance to the model increases the explained variance by 54.15 percentage points. This is by far the largest improvement at any step in our hierarchical analysis, and it fundamentally transforms the model from one that explains virtually nothing to one that explains more than half of the total variation. This result carries a profound substantive message. It tells us that the single most important category of factors for predicting exam scores is not what students were capable of before the course, but what they actively do during it - how many hours they invest in studying and how consistently they show up to class. The effort and engagement variables dwarf baseline ability in their explanatory contribution. For educators and policymakers, this is an optimistic finding: it suggests that student performance is more about current behavior than about fixed, pre-existing ability. Students who attend regularly and study diligently tend to perform well, regardless of their prior academic record. It is worth reflecting on why the <span class="math inline">\(\Delta R^2\)</span> is so much larger than the Block 1 <span class="math inline">\(R^2\)</span>. One reason is that “hours_studied” and “attendance” have relatively large coefficients (approximately 0.29 and 0.20 per unit, respectively) and substantial variability across students. Another reason is that these two variables together capture two distinct and complementary dimensions of student effort - in-class engagement and out-of-class preparation - which together account for a broad swath of the academic experience.</p>
<p>In the third block, we add “tutoring_sessions”. The <span class="math inline">\(R^2\)</span> increases from 0.5722 to 0.5967, yielding a <span class="math inline">\(\Delta R^2\)</span> of 0.0246. This means that tutoring sessions explain an additional 2.46 percentage points of variance in exam scores, above and beyond what is already explained by prior scores, study hours, and attendance. While 2.46% may seem small compared to the massive 54.15% gain in Block 2, it is important to interpret this number in context. By Block 3, we have already accounted for the major sources of variation - the “low-hanging fruit” has been picked, so to speak. The remaining unexplained variance at the end of Block 2 was about 42.78% (since 100% − 57.22% = 42.78%). Of this remaining unexplained variance, tutoring sessions account for 2.46 / 42.78 = about 5.75%. So among the factors not yet captured by the model, tutoring makes a meaningful - though not dominant - contribution. This makes theoretical sense: tutoring provides targeted academic support that goes beyond what students gain from attending lectures and studying on their own. It represents an additional layer of help that can address specific gaps in understanding, provide personalized feedback, and reinforce difficult concepts.</p>
<p>In the fourth and final block, we add “sleep_hours” and “physical_activity”. The <span class="math inline">\(R^2\)</span> increases from 0.5967 to 0.5982, yielding a <span class="math inline">\(\Delta R^2\)</span> of 0.0015. This is a very small increment - lifestyle factors explain only about 0.15 additional percentage points of variance in exam scores after all the academic variables have been accounted for. This finding tells us that once we know how a student performed previously, how much they study, how often they attend class, and how many tutoring sessions they receive, knowing about their sleep habits and physical exercise patterns adds almost nothing to our ability to predict their exam score. This does not necessarily mean that sleep and exercise are unimportant for well-being or general cognitive function - there is extensive research suggesting they are. But in terms of their incremental contribution to predicting exam scores specifically, over and above the academic and effort variables, their contribution is negligible. It is also worth recalling that in our earlier multiple regression analysis, “sleep_hours” was not statistically significant (p = 0.384), while “physical_activity” was significant but had a small coefficient (0.144). The hierarchical analysis confirms that these lifestyle variables are the least important block in our model.</p>
<p>The ANOVA table provides formal F-tests for the significance of each block’s contribution. Let me explain each row. The first row shows Model 1 (Block 1) with 6,605 residual degrees of freedom and an RSS of 96,921. This serves as the starting point. No comparison is made yet because there is no preceding model. The second row compares Model 2 to Model 1. The difference is 2 degrees of freedom (because we added two predictors: “hours_studied” and “attendance”), and the reduction in RSS is 54,143. This is a massive reduction - the unexplained variance was cut by more than half. The F-statistic is 4,447.92 and the p-value is less than <span class="math inline">\(2.2 \times 10{-16}\)</span>. This F-statistic is extraordinarily large, providing overwhelming evidence that adding study behavior and engagement variables produces a significant improvement in the model. To understand the F-statistic intuitively, the numerator of the F-test takes the reduction in RSS per added predictor (54,143 / 2 = 27,071.5) and divides it by the residual mean square of the fuller model (approximately 6.09). The ratio of 4,448 means that each of the two new predictors reduced the RSS by roughly 4,448 times more than what a useless random predictor would be expected to contribute. This is exceptionally strong evidence. The third row compares Model 3 to Model 2. Adding Tutoring_Sessions (1 degree of freedom) reduced the RSS by 2,459. The F-statistic is 404.02 with a p-value less than <span class="math inline">\(2.2 \times 10^{-16}\)</span>. This is also highly significant, confirming that tutoring sessions provide a meaningful improvement in the model even after study hours and attendance are already included. The F-statistic of 404 is smaller than the 4,448 for Block 2, which makes sense - tutoring contributes less than study behavior, but its contribution is still far beyond what chance alone could produce. The fourth row compares Model 4 to Model 3. Adding “sleep_hours” and “physical_activity” (2 degrees of freedom) reduced the RSS by only 150. The F-statistic is 12.34 with a p-value of <span class="math inline">\(4.484 \times 10^{10-6}\)</span>. Although this p-value is well below 0.05 and therefore statistically significant, the magnitude of the improvement is tiny compared to the earlier blocks. The F-statistic of 12.34, while significant, is orders of magnitude smaller than the F-statistics for Blocks 2 and 3. The RSS decreased from 40,320 to 40,169 - a reduction of less than 0.4%. This confirms that lifestyle factors achieve statistical significance (largely thanks to Physical_Activity, as we know Sleep_Hours alone is not significant), but their practical contribution to explaining exam scores is minimal.</p>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./02_00_linear_regression_model.html" class="pagination-link" aria-label="Linear Regression Model">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Linear Regression Model</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb64" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Quntitative Linear Regression Model"</span></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> ""</span></span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a><span class="an">number-sections:</span><span class="co"> true</span></span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a><span class="an">title-block-banner:</span><span class="co"> "#00868B"</span></span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a><span class="an">title-block-banner-color:</span><span class="co"> "white"</span></span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a><span class="an">crossref:</span></span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a><span class="co">  fig-title: "Figure"</span></span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a><span class="co">  tbl-title: "Table"</span></span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-14"><a href="#cb64-14" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb64-15"><a href="#cb64-15" aria-hidden="true" tabindex="-1"></a><span class="co">#| include: false</span></span>
<span id="cb64-16"><a href="#cb64-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-17"><a href="#cb64-17" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb64-18"><a href="#cb64-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-19"><a href="#cb64-19" aria-hidden="true" tabindex="-1"></a>student_performance <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">"datasets/StudentPerformanceFactors.csv"</span>)</span>
<span id="cb64-20"><a href="#cb64-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-21"><a href="#cb64-21" aria-hidden="true" tabindex="-1"></a>student_performance <span class="ot">&lt;-</span> student_performance <span class="sc">|&gt;</span></span>
<span id="cb64-22"><a href="#cb64-22" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(</span>
<span id="cb64-23"><a href="#cb64-23" aria-hidden="true" tabindex="-1"></a>        <span class="at">Extracurricular_Activities =</span> <span class="fu">case_when</span>(</span>
<span id="cb64-24"><a href="#cb64-24" aria-hidden="true" tabindex="-1"></a>            Extracurricular_Activities <span class="sc">==</span> <span class="st">"Yes"</span> <span class="sc">~</span> <span class="cn">TRUE</span>,</span>
<span id="cb64-25"><a href="#cb64-25" aria-hidden="true" tabindex="-1"></a>            Extracurricular_Activities <span class="sc">==</span> <span class="st">"No"</span> <span class="sc">~</span> <span class="cn">FALSE</span>,</span>
<span id="cb64-26"><a href="#cb64-26" aria-hidden="true" tabindex="-1"></a>            <span class="cn">TRUE</span> <span class="sc">~</span> <span class="cn">NA</span></span>
<span id="cb64-27"><a href="#cb64-27" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb64-28"><a href="#cb64-28" aria-hidden="true" tabindex="-1"></a>        <span class="at">Internet_Access =</span> <span class="fu">case_when</span>(</span>
<span id="cb64-29"><a href="#cb64-29" aria-hidden="true" tabindex="-1"></a>            Internet_Access <span class="sc">==</span> <span class="st">"Yes"</span> <span class="sc">~</span> <span class="cn">TRUE</span>,</span>
<span id="cb64-30"><a href="#cb64-30" aria-hidden="true" tabindex="-1"></a>            Internet_Access <span class="sc">==</span> <span class="st">"No"</span> <span class="sc">~</span> <span class="cn">FALSE</span>,</span>
<span id="cb64-31"><a href="#cb64-31" aria-hidden="true" tabindex="-1"></a>            <span class="cn">TRUE</span> <span class="sc">~</span> <span class="cn">NA</span></span>
<span id="cb64-32"><a href="#cb64-32" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb64-33"><a href="#cb64-33" aria-hidden="true" tabindex="-1"></a>        <span class="at">Learning_Disabilities =</span> <span class="fu">case_when</span>(</span>
<span id="cb64-34"><a href="#cb64-34" aria-hidden="true" tabindex="-1"></a>            Learning_Disabilities <span class="sc">==</span> <span class="st">"Yes"</span> <span class="sc">~</span> <span class="cn">TRUE</span>,</span>
<span id="cb64-35"><a href="#cb64-35" aria-hidden="true" tabindex="-1"></a>            Learning_Disabilities <span class="sc">==</span> <span class="st">"No"</span> <span class="sc">~</span> <span class="cn">FALSE</span>,</span>
<span id="cb64-36"><a href="#cb64-36" aria-hidden="true" tabindex="-1"></a>            <span class="cn">TRUE</span> <span class="sc">~</span> <span class="cn">NA</span></span>
<span id="cb64-37"><a href="#cb64-37" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb64-38"><a href="#cb64-38" aria-hidden="true" tabindex="-1"></a>    ) <span class="sc">|&gt;</span></span>
<span id="cb64-39"><a href="#cb64-39" aria-hidden="true" tabindex="-1"></a>    <span class="fu">rename</span>(</span>
<span id="cb64-40"><a href="#cb64-40" aria-hidden="true" tabindex="-1"></a>        <span class="at">hours_studied =</span> Hours_Studied,</span>
<span id="cb64-41"><a href="#cb64-41" aria-hidden="true" tabindex="-1"></a>        <span class="at">attendance =</span> Attendance,</span>
<span id="cb64-42"><a href="#cb64-42" aria-hidden="true" tabindex="-1"></a>        <span class="at">parental_involvement =</span> Parental_Involvement,</span>
<span id="cb64-43"><a href="#cb64-43" aria-hidden="true" tabindex="-1"></a>        <span class="at">access_to_resources =</span> Access_to_Resources,</span>
<span id="cb64-44"><a href="#cb64-44" aria-hidden="true" tabindex="-1"></a>        <span class="at">extracurricular_activities =</span> Extracurricular_Activities,</span>
<span id="cb64-45"><a href="#cb64-45" aria-hidden="true" tabindex="-1"></a>        <span class="at">sleep_hours =</span> Sleep_Hours,</span>
<span id="cb64-46"><a href="#cb64-46" aria-hidden="true" tabindex="-1"></a>        <span class="at">previous_scores =</span> Previous_Scores,</span>
<span id="cb64-47"><a href="#cb64-47" aria-hidden="true" tabindex="-1"></a>        <span class="at">motivation_level =</span> Motivation_Level,</span>
<span id="cb64-48"><a href="#cb64-48" aria-hidden="true" tabindex="-1"></a>        <span class="at">internet_access =</span> Internet_Access,</span>
<span id="cb64-49"><a href="#cb64-49" aria-hidden="true" tabindex="-1"></a>        <span class="at">tutoring_sessions =</span> Tutoring_Sessions,</span>
<span id="cb64-50"><a href="#cb64-50" aria-hidden="true" tabindex="-1"></a>        <span class="at">family_income =</span> Family_Income,</span>
<span id="cb64-51"><a href="#cb64-51" aria-hidden="true" tabindex="-1"></a>        <span class="at">teacher_quality =</span> Teacher_Quality,</span>
<span id="cb64-52"><a href="#cb64-52" aria-hidden="true" tabindex="-1"></a>        <span class="at">school_type =</span> School_Type,</span>
<span id="cb64-53"><a href="#cb64-53" aria-hidden="true" tabindex="-1"></a>        <span class="at">peer_influence =</span> Peer_Influence,</span>
<span id="cb64-54"><a href="#cb64-54" aria-hidden="true" tabindex="-1"></a>        <span class="at">physical_activity =</span> Physical_Activity,</span>
<span id="cb64-55"><a href="#cb64-55" aria-hidden="true" tabindex="-1"></a>        <span class="at">learning_disabilities =</span> Learning_Disabilities,</span>
<span id="cb64-56"><a href="#cb64-56" aria-hidden="true" tabindex="-1"></a>        <span class="at">parental_education_level =</span> Parental_Education_Level,</span>
<span id="cb64-57"><a href="#cb64-57" aria-hidden="true" tabindex="-1"></a>        <span class="at">distance_from_home =</span> Distance_from_Home,</span>
<span id="cb64-58"><a href="#cb64-58" aria-hidden="true" tabindex="-1"></a>        <span class="at">gender =</span> Gender,</span>
<span id="cb64-59"><a href="#cb64-59" aria-hidden="true" tabindex="-1"></a>        <span class="at">exam_score =</span> Exam_Score,</span>
<span id="cb64-60"><a href="#cb64-60" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb64-61"><a href="#cb64-61" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb64-62"><a href="#cb64-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-63"><a href="#cb64-63" aria-hidden="true" tabindex="-1"></a>The simplest form of linear regression involves just one independent variable. This is called **simple linear regression**. Mathematically, it takes the form:</span>
<span id="cb64-64"><a href="#cb64-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-65"><a href="#cb64-65" aria-hidden="true" tabindex="-1"></a>$Y \approx \beta_0 + \beta_1X$</span>
<span id="cb64-66"><a href="#cb64-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-67"><a href="#cb64-67" aria-hidden="true" tabindex="-1"></a>In this equation, Y is the dependent variable - the outcome we want to predict. In our example, Y is, for example, the variable "exam_score". X is the independent variable - the factor we believe is related to the outcome. For instance, X could be "hours_studied". The symbol $\beta_0$ is called the **intercept**. It represents the expected value of Y when X equals zero. In our context, it would represent the expected exam score for a hypothetical student who studies zero hours. The symbol $\beta_1$ is called the **slope**. It represents the average change in Y that is associated with a one-unit increase in X. In our example, it tells us how many additional points on the exam a student can expect to gain for each additional hour of studying. Together, $\beta_0$ and $\beta_1$ are called the **model coefficients or parameters**.</span>
<span id="cb64-68"><a href="#cb64-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-69"><a href="#cb64-69" aria-hidden="true" tabindex="-1"></a>Of course, in real life we do not know the true values of $\beta_0$ and $\beta_1$. We must estimate them from the data. Once we have estimated these coefficients - and we denote the estimates with a hat symbol as $\hat{\beta}_0$ and $\hat{\beta}_1$ - we can write our prediction equation as: $y = \hat{\beta_0} + \hat{\beta_1}x$. Here, $\hat{y}$ is the predicted value of the response for a given value $x$ of the predictor. The hat symbol always indicates that we are dealing with an estimate rather than a true and known quantity.</span>
<span id="cb64-70"><a href="#cb64-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-71"><a href="#cb64-71" aria-hidden="true" tabindex="-1"></a>In practice, a single predictor is rarely sufficient to explain all the variation in the response. A student's exam score is not determined by study hours alone - attendance, prior academic performance, tutoring, and many other factors play a role. **Multiple linear regression** extends the simple model to accommodate several predictors simultaneously. The general formula is:</span>
<span id="cb64-72"><a href="#cb64-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-73"><a href="#cb64-73" aria-hidden="true" tabindex="-1"></a>$Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p + \epsilon$</span>
<span id="cb64-74"><a href="#cb64-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-75"><a href="#cb64-75" aria-hidden="true" tabindex="-1"></a>In this equation, $X_1$, $X_2$, $X_3$, ..., $X_p$ represent $p$ different independent variables, and $\beta_1$, $\beta_2$, ..., $\beta_p$ are their corresponding slope coefficients. Each coefficient $\beta_j$ represents the average change in Y associated with a one-unit increase in the predictor $\beta_j$, while **holding all other predictors constant** which distinguishes multiple regression from simply running many separate simple regressions. The term $\epsilon$ represents the **error term** - it captures everything that our model does not explain, including the influence of unmeasured variables, measurement error, and the inherent randomness in human behavior.</span>
<span id="cb64-76"><a href="#cb64-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-77"><a href="#cb64-77" aria-hidden="true" tabindex="-1"></a>In our student performance example, a multiple linear regression model might look like this:</span>
<span id="cb64-78"><a href="#cb64-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-79"><a href="#cb64-79" aria-hidden="true" tabindex="-1"></a>exam_score = $\beta_0$ + $\beta_1$ × hours_studied + $\beta_2$ × attendance + $\beta_3$ × previous_scores + $\beta_4$ × sleep_hours + $\beta_5$ × tutoring_sessions + $\beta_6$ × physical_activity + $\epsilon$</span>
<span id="cb64-80"><a href="#cb64-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-81"><a href="#cb64-81" aria-hidden="true" tabindex="-1"></a>This model allows us to estimate the unique contribution of each predictor to the exam score. For instance, $\beta_1$ tells us the expected change in exam score for each additional hour of study, after accounting for the effects of attendance, previous scores, sleep, tutoring, and physical activity. This is fundamentally different from simple linear regression, where $\beta_1$ would capture the total association between study hours and exam scores without adjusting for any other factor.</span>
<span id="cb64-82"><a href="#cb64-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-83"><a href="#cb64-83" aria-hidden="true" tabindex="-1"></a><span class="fu">## Estimating the Coefficients of Parameters</span></span>
<span id="cb64-84"><a href="#cb64-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-85"><a href="#cb64-85" aria-hidden="true" tabindex="-1"></a>The key question is how do we actually find the best values for our coefficient estimates $\hat{\beta}_0$, $\hat{\beta}_1$, ..., $\hat{\beta}_p$. The answer lies in the **least squares method**, which is the most common approach for fitting a linear regression model. The basic idea is intuitive: we want our predicted values $\hat{y}_i$ to be as close as possible to the actual observed values $y_0$ for every observation in our dataset.</span>
<span id="cb64-86"><a href="#cb64-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-87"><a href="#cb64-87" aria-hidden="true" tabindex="-1"></a>For each observation $i$, the difference between the observed value and the predicted value is called the **residual**, denoted $e_i = y_i - \hat{y}_i$. The residual tells us how much our model's prediction misses the actual outcome for that particular student. Some residuals will be positive (when the model underpredicts) and some will be negative (when the model overpredicts).</span>
<span id="cb64-88"><a href="#cb64-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-91"><a href="#cb64-91" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb64-92"><a href="#cb64-92" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb64-93"><a href="#cb64-93" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb64-94"><a href="#cb64-94" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb64-95"><a href="#cb64-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-96"><a href="#cb64-96" aria-hidden="true" tabindex="-1"></a>simple_model <span class="ot">&lt;-</span> <span class="fu">lm</span>(exam_score <span class="sc">~</span> hours_studied, <span class="at">data =</span> student_performance)</span>
<span id="cb64-97"><a href="#cb64-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-98"><a href="#cb64-98" aria-hidden="true" tabindex="-1"></a>student_performance<span class="sc">$</span>predicted <span class="ot">&lt;-</span> <span class="fu">predict</span>(simple_model)</span>
<span id="cb64-99"><a href="#cb64-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-100"><a href="#cb64-100" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(student_performance, <span class="fu">aes</span>(<span class="at">x =</span> hours_studied, <span class="at">y =</span> exam_score)) <span class="sc">+</span></span>
<span id="cb64-101"><a href="#cb64-101" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="fu">aes</span>(<span class="at">xend =</span> hours_studied, <span class="at">yend =</span> predicted), <span class="at">color =</span> <span class="st">"grey70"</span>, <span class="at">linewidth =</span> <span class="fl">0.4</span>) <span class="sc">+</span></span>
<span id="cb64-102"><a href="#cb64-102" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">color =</span> <span class="st">"#00868B"</span>, <span class="at">size =</span> <span class="fl">1.8</span>) <span class="sc">+</span></span>
<span id="cb64-103"><a href="#cb64-103" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">se =</span> <span class="cn">FALSE</span>, <span class="at">color =</span> <span class="st">"#3333CC"</span>, <span class="at">linewidth =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb64-104"><a href="#cb64-104" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Hours Studied"</span>, <span class="at">y =</span> <span class="st">"Exam Score"</span>) <span class="sc">+</span></span>
<span id="cb64-105"><a href="#cb64-105" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span>
<span id="cb64-106"><a href="#cb64-106" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb64-107"><a href="#cb64-107" aria-hidden="true" tabindex="-1"></a>: Simple Linear Regression: Exam Score ~ Hours Studied</span>
<span id="cb64-108"><a href="#cb64-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-109"><a href="#cb64-109" aria-hidden="true" tabindex="-1"></a>To get an overall measure of how well the model fits all the data, we cannot simply add up the residuals, because the positive and negative ones would cancel each other out. Instead, we square each residual and then sum them all up. This quantity is called the **residual sum of squares** (RSS):</span>
<span id="cb64-110"><a href="#cb64-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-111"><a href="#cb64-111" aria-hidden="true" tabindex="-1"></a>$RSS = e_1^2 + e_2^2 + ... + e_n^2 = \sum(y_i - \hat{y}_i)^2$</span>
<span id="cb64-112"><a href="#cb64-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-113"><a href="#cb64-113" aria-hidden="true" tabindex="-1"></a>The least squares method chooses the coefficient estimates $\hat{\beta}_0$, $\hat{\beta}_1$, ..., $\hat{\beta}_p$ that minimize this RSS. In other words, the least squares approach finds the line (in simple regression) or the hyperplane (in multiple regression) that makes the total squared prediction error as small as possible. This is a well-defined mathematical optimization problem, and the solution can be computed using calculus. For simple linear regression, the formulas for the minimizers have a closed-form expression:</span>
<span id="cb64-114"><a href="#cb64-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-115"><a href="#cb64-115" aria-hidden="true" tabindex="-1"></a>$\hat{\beta}_0 = \bar{y} - \hat{\beta}_1\hat{x}$</span>
<span id="cb64-116"><a href="#cb64-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-117"><a href="#cb64-117" aria-hidden="true" tabindex="-1"></a>$\hat{\beta}_1 = \frac{\sum_{i = 1}^n(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i = 1}^n(x_i - \bar{x})^2}$</span>
<span id="cb64-118"><a href="#cb64-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-119"><a href="#cb64-119" aria-hidden="true" tabindex="-1"></a>Here, $\bar{x}$ and $\bar{y}$ are the sample means of the predictor and the response, respectively. The formula for $\hat{\beta}_1$ has an intuitive interpretation: it measures the degree to which X and Y vary together (the numerator captures their joint variation) relative to the total variation in X (the denominator). For multiple linear regression, the coefficient estimates are computed using matrix algebra.</span>
<span id="cb64-120"><a href="#cb64-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-121"><a href="#cb64-121" aria-hidden="true" tabindex="-1"></a>The beauty of the least squares method is that it provides a principled, objective way to estimate the model parameters. It does not require any subjective judgment about what the "best" line should look like - the method simply finds the line that minimizes the total squared distance between the observed data points and the fitted line.</span>
<span id="cb64-122"><a href="#cb64-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-123"><a href="#cb64-123" aria-hidden="true" tabindex="-1"></a>To fit a simple linear regression model in R, we use the <span class="in">`lm()`</span> function, which stands for linear model. The syntax follows the pattern <span class="in">`lm(response ~ predictor, data = dataset)`</span>. The tilde symbol (<span class="in">`~`</span>) can be read as "*is modeled as a function of*". For our simple linear regression of "exam_score" onto "hours_studied", we write:</span>
<span id="cb64-124"><a href="#cb64-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-127"><a href="#cb64-127" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb64-128"><a href="#cb64-128" aria-hidden="true" tabindex="-1"></a>simple_model <span class="ot">&lt;-</span> <span class="fu">lm</span>(</span>
<span id="cb64-129"><a href="#cb64-129" aria-hidden="true" tabindex="-1"></a>    exam_score <span class="sc">~</span> hours_studied,</span>
<span id="cb64-130"><a href="#cb64-130" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> student_performance</span>
<span id="cb64-131"><a href="#cb64-131" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb64-132"><a href="#cb64-132" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb64-133"><a href="#cb64-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-134"><a href="#cb64-134" aria-hidden="true" tabindex="-1"></a>The lm() function fits the model by computing the least squares coefficient estimates. The <span class="in">`summary()`</span> function then provides a detailed output that includes the estimated coefficients, their standard errors, t-statistics, p-values, the residual standard error, and the $R^2$ statistic. The <span class="in">`confint()`</span> function computes the 95% confidence intervals for each coefficient estimate, which tell us the range of plausible values for the true population parameters.</span>
<span id="cb64-135"><a href="#cb64-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-138"><a href="#cb64-138" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb64-139"><a href="#cb64-139" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(simple_model)</span>
<span id="cb64-140"><a href="#cb64-140" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb64-141"><a href="#cb64-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-142"><a href="#cb64-142" aria-hidden="true" tabindex="-1"></a>The <span class="in">`lm()`</span> fits the model by computing the least squares coefficient estimates, and the output shows the estimated parameters of a simple linear regression model predicting exam score from hours studied.</span>
<span id="cb64-143"><a href="#cb64-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-146"><a href="#cb64-146" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb64-147"><a href="#cb64-147" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(simple_model)<span class="sc">$</span>coefficients[, <span class="st">"Estimate"</span>]</span>
<span id="cb64-148"><a href="#cb64-148" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb64-149"><a href="#cb64-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-150"><a href="#cb64-150" aria-hidden="true" tabindex="-1"></a>The intercept ($\hat{\beta}_0$) is estimated at 61.4570. This means that when "hours_studied" equals zero, the model predicts an exam score of approximately 61.46 points. In substantive terms, a hypothetical student who does not study at all would be expected to score about 61.5 on the exam, according to this model. This makes intuitive sense - students would still have some baseline level of knowledge from attending classes, even without additional study outside the classroom. The slope for "hours_studied" ($\hat{\beta}_1$) is estimated at 0.289. This is the key coefficient for our research question. It tells us that for each additional hour of study per week, a student's exam score is expected to increase by approximately 0.29 points, on average. So a student who studies 10 hours more per week than another student would be expected to score about 2.89 points higher on the exam.</span>
<span id="cb64-151"><a href="#cb64-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-152"><a href="#cb64-152" aria-hidden="true" tabindex="-1"></a>For the multiple linear regression model, we simply add more predictors to the right side of the formula, separated by the <span class="in">`+`</span> sign:</span>
<span id="cb64-153"><a href="#cb64-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-156"><a href="#cb64-156" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb64-157"><a href="#cb64-157" aria-hidden="true" tabindex="-1"></a>multiple_model <span class="ot">&lt;-</span> <span class="fu">lm</span>(</span>
<span id="cb64-158"><a href="#cb64-158" aria-hidden="true" tabindex="-1"></a>    exam_score <span class="sc">~</span> hours_studied <span class="sc">+</span> attendance <span class="sc">+</span> previous_scores <span class="sc">+</span> sleep_hours <span class="sc">+</span> tutoring_sessions <span class="sc">+</span> physical_activity,</span>
<span id="cb64-159"><a href="#cb64-159" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> student_performance</span>
<span id="cb64-160"><a href="#cb64-160" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb64-161"><a href="#cb64-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-162"><a href="#cb64-162" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(multiple_model)</span>
<span id="cb64-163"><a href="#cb64-163" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb64-164"><a href="#cb64-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-165"><a href="#cb64-165" aria-hidden="true" tabindex="-1"></a>This tells R to fit a model that predicts "exam_score" using all six quantitative predictors simultaneously. The least squares method will estimate a separate slope coefficient for each predictor, along with a single intercept, by minimizing the total RSS across all 6,607 observations. Our multiple linear regression model includes six quantitative predictors simultaneously.</span>
<span id="cb64-166"><a href="#cb64-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-169"><a href="#cb64-169" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb64-170"><a href="#cb64-170" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(multiple_model)<span class="sc">$</span>coefficients[, <span class="st">"Estimate"</span>]</span>
<span id="cb64-171"><a href="#cb64-171" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb64-172"><a href="#cb64-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-173"><a href="#cb64-173" aria-hidden="true" tabindex="-1"></a>The estimated model can be written as:</span>
<span id="cb64-174"><a href="#cb64-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-175"><a href="#cb64-175" aria-hidden="true" tabindex="-1"></a>Exam_Score ≈ 40.93 + 0.292 × Hours_Studied + 0.198 × Attendance + 0.048 × Previous_Scores − 0.018 × Sleep_Hours + 0.494 × Tutoring_Sessions + 0.144 × Physical_Activity</span>
<span id="cb64-176"><a href="#cb64-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-177"><a href="#cb64-177" aria-hidden="true" tabindex="-1"></a>The intercept ($\hat{\beta}_0$) is now estimated at 40.927. This is substantially lower than in the simple model (61.46), which makes sense. In the simple model, the intercept represented the predicted score when only "hours_studied" was zero. In the multiple model, the intercept represents the predicted score when all six predictors are on their average value. Such a student is of course entirely hypothetical and unrealistic, which is why we should not over-interpret the intercept in multiple regression. Its main role is mathematical - it anchors the regression plane in the right position.</span>
<span id="cb64-178"><a href="#cb64-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-179"><a href="#cb64-179" aria-hidden="true" tabindex="-1"></a>The coefficient for "hours_studied" is 0.292, which is remarkably similar to its value in the simple regression (0.289). This tells us something important: the relationship between study hours and exam scores is robust - it persists even after we account for the effects of attendance, prior scores, sleep, tutoring, and physical activity. In substantive terms, holding all other factors constant, each additional hour of study per week is associated with an increase of about 0.29 points on the exam.</span>
<span id="cb64-180"><a href="#cb64-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-181"><a href="#cb64-181" aria-hidden="true" tabindex="-1"></a>The coefficient for "attendance" is 0.198, meaning that each additional percentage point of class attendance is associated with about 0.20 additional points on the exam, after controlling for the other variables. To put this in perspective, a student who attends 90% of classes versus one who attends 70% of classes (a 20 percentage-point difference) would be expected to score about 3.96 points higher, all else being equal. The coefficient for "previous_scores" is 0.048. This means that for each additional point a student earned on their previous assessments, their exam score is expected to increase by about 0.048 points, holding other factors constant. A student whose prior scores are 20 points higher than another student's would be expected to score only about 0.96 points higher on this exam. This suggests that while past performance does predict future performance, its incremental contribution is small once study habits and attendance are already accounted for. The coefficient for "sleep_hours" is -0.018. The negative sign suggests that more sleep is associated with slightly lower exam scores, meaning that each additional hour of sleep per night is associated with a decrease of about 0.02 points on the exam, after controlling for the other variables. The coefficient for "tutoring_sessions" is 0.494, the largest individual slope coefficient in the model. Each additional tutoring session is associated with about half a point increase on the exam. The coefficient for physical_activity is 0.144, meaning that each additional hour per week of physical activity is associated with about 0.14 additional exam points, after controlling for the other variables.</span>
<span id="cb64-182"><a href="#cb64-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-183"><a href="#cb64-183" aria-hidden="true" tabindex="-1"></a><span class="fu">## Accuracy of the Coefficient Estimates</span></span>
<span id="cb64-184"><a href="#cb64-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-185"><a href="#cb64-185" aria-hidden="true" tabindex="-1"></a>In the previous section, we estimated the coefficients of our linear regression models using the least squares method. We found, for instance, that the estimated slope for "hours_studied" was 0.289 in the simple model and 0.292 in the multiple model. But a natural and critically important question follows: *How accurate are these estimates? If we collected a different sample of 6,607 students, would we get the same coefficient estimates, or would they change? And how confident can we be that the true relationship between study hours and exam scores is actually positive, rather than our estimate simply being a product of random chance in this particular sample?*</span>
<span id="cb64-186"><a href="#cb64-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-187"><a href="#cb64-187" aria-hidden="true" tabindex="-1"></a>These questions lie at the heart of statistical inference, and the tools we use to answer them - standard errors, confidence intervals, t-statistics, and p-values - are among the most important concepts in all of applied statistics. To understand these tools, we must first understand the concept of the error term and the distinction between the population regression line and our estimated regression line.</span>
<span id="cb64-188"><a href="#cb64-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-189"><a href="#cb64-189" aria-hidden="true" tabindex="-1"></a>When we write the linear regression model as $Y = \beta_0 + \beta_1X + \epsilon$, we are making a statement about the true, underlying relationship between X and Y in the entire population - not just in our particular sample. The coefficients $\beta_0$ and $\beta_1$ are the true population parameters, which we will never know exactly. The term $\epsilon$ is the error term, and it represents everything that our model fails to capture. There are several reasons why the error term exists.</span>
<span id="cb64-190"><a href="#cb64-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-191"><a href="#cb64-191" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>First, the true relationship between the predictor and the response may not be perfectly linear - there may be curvature or other patterns that a straight line cannot capture.</span>
<span id="cb64-192"><a href="#cb64-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-193"><a href="#cb64-193" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Second, there may be other variables that influence the response but are not included in our model.</span>
<span id="cb64-194"><a href="#cb64-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-195"><a href="#cb64-195" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Third, there is always some inherent randomness and measurement error in any data we collect.</span>
<span id="cb64-196"><a href="#cb64-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-197"><a href="#cb64-197" aria-hidden="true" tabindex="-1"></a>The error term absorbs all of these sources of discrepancy between what our model predicts and what actually happens.</span>
<span id="cb64-198"><a href="#cb64-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-199"><a href="#cb64-199" aria-hidden="true" tabindex="-1"></a>In our <span class="in">`student_performance`</span> example, even if we knew the exact true values of $\beta_0$ and $\beta_1$ for the relationship between "hours_studied" and "exam_score" in the entire population of all students, we still could not predict any individual student's exam score perfectly. Some students who study 20 hours per week will score higher than the regression line predicts, and others will score lower. These deviations are captured by $\epsilon$. We typically assume that the error term has a mean of zero (meaning the model does not systematically overpredict or underpredict), that the errors for different observations are independent of each other, and that the errors have a constant variance $\sigma^2$ across all values of X.</span>
<span id="cb64-200"><a href="#cb64-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-201"><a href="#cb64-201" aria-hidden="true" tabindex="-1"></a>The true population regression line, $Y = \beta_0 + \beta_1X$, represents the best linear summary of the relationship between X and Y in the entire population. Our estimated regression line, $\hat{y} = \hat{\beta}_0 + \hat{\beta}_1x$, is our best approximation of this population line based on the data we have. The key insight is that $\hat{\beta}_0$ and $\hat{\beta}_1$ are estimates of the true parameters, computed from one particular sample. If we were to draw a different random sample of 6,607 students, we would get slightly different estimates. This variability across samples is what motivates the need for standard errors, confidence intervals, and hypothesis tests - they allow us to quantify how much uncertainty surrounds our estimates.</span>
<span id="cb64-202"><a href="#cb64-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-203"><a href="#cb64-203" aria-hidden="true" tabindex="-1"></a><span class="fu">### Standard Errors</span></span>
<span id="cb64-204"><a href="#cb64-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-205"><a href="#cb64-205" aria-hidden="true" tabindex="-1"></a>The **standard error** of a coefficient estimate measures how much that estimate would vary if we repeatedly drew new samples from the same population and re-estimated the model each time. In other words, it quantifies the precision of our estimate. A small standard error means that our estimate is very precise - different samples would give us very similar coefficient values. A large standard error means that our estimate is imprecise - it might change substantially from sample to sample.</span>
<span id="cb64-206"><a href="#cb64-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-207"><a href="#cb64-207" aria-hidden="true" tabindex="-1"></a>The standard error of the coefficients or parameters in linear regression is given by the formulas:</span>
<span id="cb64-208"><a href="#cb64-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-209"><a href="#cb64-209" aria-hidden="true" tabindex="-1"></a>$SE(\hat{\beta}_0) = \sigma^2 [\frac{1}{n} + \frac{\bar{x}^2}{\sum_{i =1}^n(x_i - \bar{x})^2}]$</span>
<span id="cb64-210"><a href="#cb64-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-211"><a href="#cb64-211" aria-hidden="true" tabindex="-1"></a>$SE(\hat{\beta}_1) = \frac{\sigma}{\sum_{i=1}^n(x_i - \bar{x})^2}$</span>
<span id="cb64-212"><a href="#cb64-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-213"><a href="#cb64-213" aria-hidden="true" tabindex="-1"></a>This formulas reveals two important things. First, the standard error depends on $\sigma$, the standard deviation of the error term. If there is a lot of noise in the data (large $\sigma$), then the estimated slope will be less precise, because the true signal is harder to detect amid the noise. Second, the standard error depends on the spread of the predictor values. When the $x_i$ values are more spread out (that is, when $\sum_{i=1}^n(x_i - \bar{x})^2$ is large), the standard error is smaller. Intuitively, this makes sense: if we observe students who study anywhere from 1 to 44 hours per week, we have a much better basis for estimating the slope than if all students studied between 19 and 21 hours. A wider range of predictor values gives us more "leverage" to pin down the relationship. In practice, the true value of $\sigma$ is unknown and must be estimated from the data. This estimate is the Residual Standard Error.</span>
<span id="cb64-214"><a href="#cb64-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-217"><a href="#cb64-217" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb64-218"><a href="#cb64-218" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(simple_model)<span class="sc">$</span>coefficients[, <span class="st">"Std. Error"</span>]</span>
<span id="cb64-219"><a href="#cb64-219" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb64-220"><a href="#cb64-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-221"><a href="#cb64-221" aria-hidden="true" tabindex="-1"></a>Looking at our simple regression output, the standard error for the "hours_studied" coefficient is 0.00715. This is very small relative to the coefficient estimate of 0.289, which tells us that our estimate is highly precise. The reason for this high precision is our large sample size combined with a good spread in study hours.</span>
<span id="cb64-222"><a href="#cb64-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-225"><a href="#cb64-225" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb64-226"><a href="#cb64-226" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(multiple_model)<span class="sc">$</span>coefficients[, <span class="st">"Std. Error"</span>]</span>
<span id="cb64-227"><a href="#cb64-227" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb64-228"><a href="#cb64-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-229"><a href="#cb64-229" aria-hidden="true" tabindex="-1"></a>In the multiple regression model, the standard error for "hours_studied" is even smaller at 0.00507. This decrease occurs because the multiple model has a lower RSE (2.467 compared to 3.483), which means there is less unexplained noise once we account for the additional predictors - and less noise translates directly into more precise coefficient estimates. The standard errors for the other coefficients in the multiple model tell a similar story. "attendance" has a standard error of 0.00263, "previous_scores" has 0.00211, "sleep_hours" has 0.0207, "tutoring_sessions" has 0.0247, and "physical_activity" has 0.0294. Notice that "sleep_hours" has a relatively large standard error compared to its coefficient estimate (-0.018), which foreshadows the fact that this coefficient will not be statistically significant - the estimate is so imprecise relative to its magnitude that we cannot confidently distinguish it from zero.</span>
<span id="cb64-230"><a href="#cb64-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-231"><a href="#cb64-231" aria-hidden="true" tabindex="-1"></a>A **confidence interval** provides a range of plausible values for the true population parameter. The 95% confidence interval for a regression coefficient is constructed using the formula:</span>
<span id="cb64-232"><a href="#cb64-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-233"><a href="#cb64-233" aria-hidden="true" tabindex="-1"></a>$\hat{\beta}_j \pm 2 \times SE(\hat{\beta}_j)$</span>
<span id="cb64-234"><a href="#cb64-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-235"><a href="#cb64-235" aria-hidden="true" tabindex="-1"></a>More precisely, the multiplier is not exactly 2 but rather the 97.5th percentile of the t-distribution with $n - p - 1$ degrees of freedom, where n is the sample size and p is the number of predictors. For large samples like ours (n = 6,607), this value is very close to 1.96, which is approximately 2. The interpretation of a 95% confidence interval is as follows: if we were to repeat the study many times, drawing a new random sample each time and computing a 95% confidence interval from each sample, then 95% of those intervals would contain the true population parameter. It is important to note that this does not mean there is a 95% probability that the true parameter lies within our specific interval - the true parameter is a fixed value, not a random quantity. Rather, the 95% refers to the long-run reliability of the procedure.</span>
<span id="cb64-236"><a href="#cb64-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-239"><a href="#cb64-239" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb64-240"><a href="#cb64-240" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(simple_model)</span>
<span id="cb64-241"><a href="#cb64-241" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb64-242"><a href="#cb64-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-243"><a href="#cb64-243" aria-hidden="true" tabindex="-1"></a>Let us examine the confidence intervals from our outputs. In the simple regression, the 95% confidence interval for the "hours_studied" coefficient is <span class="co">[</span><span class="ot">0.275, 0.303</span><span class="co">]</span>. This interval is narrow, reflecting the high precision of our estimate, and it lies entirely above zero. We can therefore state with 95% confidence that the true effect of one additional hour of study falls somewhere between 0.275 and 0.303 points on the exam.</span>
<span id="cb64-244"><a href="#cb64-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-247"><a href="#cb64-247" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb64-248"><a href="#cb64-248" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(multiple_model)</span>
<span id="cb64-249"><a href="#cb64-249" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb64-250"><a href="#cb64-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-251"><a href="#cb64-251" aria-hidden="true" tabindex="-1"></a>In the multiple regression, the confidence intervals are similarly informative. For "attendance", the interval is <span class="co">[</span><span class="ot">0.193, 0.203</span><span class="co">]</span>, meaning we are 95% confident that each additional percentage point of attendance is associated with between 0.19 and 0.20 additional exam points, after controlling for the other predictors. For "tutoring_sessions", the interval is <span class="co">[</span><span class="ot">0.445, 0.542</span><span class="co">]</span>, and for "physical_activity" it is <span class="co">[</span><span class="ot">0.086, 0.202</span><span class="co">]</span> - all comfortably above zero. The critical case is "sleep_hours", whose confidence interval is <span class="co">[</span><span class="ot">-0.059, 0.023</span><span class="co">]</span>. Because this interval spans from a negative value to a positive value, crossing zero in the middle, we cannot determine whether the true effect of sleep hours on exam scores is positive, negative, or simply zero.</span>
<span id="cb64-252"><a href="#cb64-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-253"><a href="#cb64-253" aria-hidden="true" tabindex="-1"></a><span class="fu">### Hypothesis Testing and the t-Statistic</span></span>
<span id="cb64-254"><a href="#cb64-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-255"><a href="#cb64-255" aria-hidden="true" tabindex="-1"></a>**Hypothesis testing** provides a formal framework for determining whether the relationship we observe in our sample is likely to reflect a real relationship in the population, or whether it could plausibly be due to random chance. In linear regression, the most common hypothesis test for each coefficient is:</span>
<span id="cb64-256"><a href="#cb64-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-257"><a href="#cb64-257" aria-hidden="true" tabindex="-1"></a>**The null hypothesis ($H_0$):** $\beta_j = 0$, meaning there is no relationship between $X_j$ and Y.</span>
<span id="cb64-258"><a href="#cb64-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-259"><a href="#cb64-259" aria-hidden="true" tabindex="-1"></a>**The alternative hypothesis ($H_a$):** $\beta_j \neq 0$, meaning there is some relationship between $X_j$ and Y.</span>
<span id="cb64-260"><a href="#cb64-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-261"><a href="#cb64-261" aria-hidden="true" tabindex="-1"></a>If the null hypothesis is true and the predictor truly has no effect on the response, then the true slope is zero, and any non-zero slope we estimate from our sample is simply due to random noise. The t-statistic allows us to assess how likely this scenario is.</span>
<span id="cb64-262"><a href="#cb64-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-263"><a href="#cb64-263" aria-hidden="true" tabindex="-1"></a>The **t-statistic** is computed as:</span>
<span id="cb64-264"><a href="#cb64-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-265"><a href="#cb64-265" aria-hidden="true" tabindex="-1"></a>$t = \frac{\hat{\beta}_j}{SE(\hat{\beta}_j)}$</span>
<span id="cb64-266"><a href="#cb64-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-267"><a href="#cb64-267" aria-hidden="true" tabindex="-1"></a>This is simply the coefficient estimate divided by its standard error. It measures how many standard errors the coefficient estimate is away from zero. A t-statistic close to zero means that the coefficient estimate is small relative to its uncertainty, which is consistent with the null hypothesis. A t-statistic far from zero (either very positive or very negative) means that the coefficient estimate is large relative to its uncertainty, which provides evidence against the null hypothesis.</span>
<span id="cb64-268"><a href="#cb64-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-269"><a href="#cb64-269" aria-hidden="true" tabindex="-1"></a>Under the null hypothesis, the t-statistic follows a t-distribution with $n - p - 1$ degrees of freedom, where n is the number of observations and p is the number of predictors. For large samples, the t-distribution is virtually identical to the standard normal distribution, so a t-statistic beyond roughly $\pm2$ is generally considered statistically significant at the 5% level, and beyond roughly $\pm2,75$ is significant at the 1% level.</span>
<span id="cb64-270"><a href="#cb64-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-273"><a href="#cb64-273" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb64-274"><a href="#cb64-274" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(simple_model)<span class="sc">$</span>coefficients[, <span class="st">"t value"</span>]</span>
<span id="cb64-275"><a href="#cb64-275" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb64-276"><a href="#cb64-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-277"><a href="#cb64-277" aria-hidden="true" tabindex="-1"></a>In the simple regression model, the t-statistic for "hours_studied" is 40.44. This means the coefficient estimate is more than 40 standard errors away from zero. To put this in perspective, if study hours truly had no effect on exam scores, observing a t-statistic this large would be essentially impossible - it would be like flipping a fair coin and getting heads 40 times in a row, except far less likely even than that. This gives us overwhelming evidence that the relationship between study hours and exam scores is real.</span>
<span id="cb64-278"><a href="#cb64-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-281"><a href="#cb64-281" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb64-282"><a href="#cb64-282" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(multiple_model)<span class="sc">$</span>coefficients[, <span class="st">"t value"</span>]</span>
<span id="cb64-283"><a href="#cb64-283" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb64-284"><a href="#cb64-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-285"><a href="#cb64-285" aria-hidden="true" tabindex="-1"></a>In the multiple regression model, the t-statistics reveal a clear hierarchy of evidence. "attendance" has the largest t-statistic at 75.26, making it the most precisely estimated and most strongly significant predictor. "hours_studied" follows with t = 57.52, then "previous_scores" at 22.81, "tutoring_sessions" at 20.00, and "physical_activity" at 4.89. All of these are far beyond any conventional significance threshold. The exception, as we have seen, is "sleep_hours", with a t-statistic of only -0.871. This value is well within the range we would expect to see even if the true coefficient were zero - it is less than one standard error away from zero, which is entirely unremarkable.</span>
<span id="cb64-286"><a href="#cb64-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-287"><a href="#cb64-287" aria-hidden="true" tabindex="-1"></a><span class="fu">### The p-Value</span></span>
<span id="cb64-288"><a href="#cb64-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-289"><a href="#cb64-289" aria-hidden="true" tabindex="-1"></a>The **p-value** is the probability of observing a t-statistic as extreme as the one we actually computed, assuming that the null hypothesis is true. In other words, it answers the question: *if there were truly no relationship between this predictor and the response, how surprising would our observed result be?*</span>
<span id="cb64-290"><a href="#cb64-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-291"><a href="#cb64-291" aria-hidden="true" tabindex="-1"></a>A small p-value (typically below 0.05) indicates that the observed result would be very surprising under the null hypothesis, and we therefore reject the null hypothesis in favor of the alternative. A large p-value indicates that the observed result is not particularly surprising under the null hypothesis, and we therefore fail to reject it. It is important to emphasize that "failing to reject" is not the same as "accepting" the null hypothesis - it simply means we do not have enough evidence to conclude that a relationship exists.</span>
<span id="cb64-292"><a href="#cb64-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-295"><a href="#cb64-295" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb64-296"><a href="#cb64-296" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(simple_model)<span class="sc">$</span>coefficients[, <span class="st">"Pr(&gt;|t|)"</span>]</span>
<span id="cb64-297"><a href="#cb64-297" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb64-298"><a href="#cb64-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-299"><a href="#cb64-299" aria-hidden="true" tabindex="-1"></a>In our simple regression output, the p-value for "hours_studied" is less than $2 \times 10^{-16}$, which R displays as "&lt;2e-16". This is the smallest p-value that R can represent numerically, and it is so close to zero that for all practical purposes it means the probability of observing our results by chance alone is essentially zero.</span>
<span id="cb64-300"><a href="#cb64-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-303"><a href="#cb64-303" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb64-304"><a href="#cb64-304" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(multiple_model)<span class="sc">$</span>coefficients[, <span class="st">"Pr(&gt;|t|)"</span>]</span>
<span id="cb64-305"><a href="#cb64-305" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb64-306"><a href="#cb64-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-307"><a href="#cb64-307" aria-hidden="true" tabindex="-1"></a>In the multiple regression, the p-values for "hours_studied", "attendance", "previous_scores", and "tutoring_sessions" are all less than $2 \times 10^{-16}$, and the p-value for "physical_activity" is approximately $1.03 \times 10^{-6}$ (or about one in a million). All of these are far below any conventional significance threshold, providing overwhelming evidence that these predictors are genuinely related to exam scores.</span>
<span id="cb64-308"><a href="#cb64-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-309"><a href="#cb64-309" aria-hidden="true" tabindex="-1"></a>The p-value for "sleep_hours", however, is 0.384. This means that if sleep hours truly had no effect on exam scores (after controlling for the other predictors), there would be about a 38.4% probability of observing a coefficient estimate as far from zero as the one we found. In other words, our observed result is entirely unsurprising under the null hypothesis - it is the kind of result we would expect to see by random chance alone roughly 38 times out of 100. We therefore have no grounds to reject the null hypothesis for "sleep_hours", and we conclude that this variable does not have a statistically significant linear relationship with exam scores in this model.</span>
<span id="cb64-310"><a href="#cb64-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-311"><a href="#cb64-311" aria-hidden="true" tabindex="-1"></a><span class="fu">## Model Fit</span></span>
<span id="cb64-312"><a href="#cb64-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-313"><a href="#cb64-313" aria-hidden="true" tabindex="-1"></a>In the previous section, we focused on assessing the accuracy of individual coefficient estimates - asking whether each specific predictor is significantly related to the response. Now we shift our perspective and ask a broader question: *how well does the model as a whole fit the data? In other words, once we have estimated our regression equation, how good is it at capturing the actual patterns in student exam scores? Is the model useful, or does it leave too much unexplained?*</span>
<span id="cb64-314"><a href="#cb64-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-315"><a href="#cb64-315" aria-hidden="true" tabindex="-1"></a>To answer these questions, we rely on three complementary statistics that appear at the bottom of every regression summary in R: the Residual Standard Error (RSE), the $R^2$ statistic, and the F-statistic. Each of these measures provides a different lens through which to evaluate the overall quality of the model, and together they give us a well-rounded picture of model performance.</span>
<span id="cb64-316"><a href="#cb64-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-317"><a href="#cb64-317" aria-hidden="true" tabindex="-1"></a><span class="fu">### Residual Standard Error</span></span>
<span id="cb64-318"><a href="#cb64-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-319"><a href="#cb64-319" aria-hidden="true" tabindex="-1"></a>The **Residual Standard Error** (RSE) is perhaps the most intuitive measure of model accuracy, because it is expressed in the same units as the dependent variable. It estimates the standard deviation of the error term $\epsilon$ - that is, it tells us the typical size of the prediction errors our model makes. The RSE is computed using the formula:</span>
<span id="cb64-320"><a href="#cb64-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-321"><a href="#cb64-321" aria-hidden="true" tabindex="-1"></a>$RSE = \sqrt{\frac{RSS}{n - p - 1}}$</span>
<span id="cb64-322"><a href="#cb64-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-323"><a href="#cb64-323" aria-hidden="true" tabindex="-1"></a>In this formula, RSS is the residual sum of squares (the sum of all squared residuals), n is the number of observations, and p is the number of predictors. The denominator uses $n - p - 1$ rather than simply n because we have used up $p + 1$ degrees of freedom in estimating the intercept and the p slope coefficients. This correction ensures that the RSE is an unbiased estimate of the true error standard deviation $\sigma$. The concept of degrees of freedom can be understood intuitively: each parameter we estimate "uses up" one piece of information from the data, leaving fewer independent pieces of information available to estimate the error variance.</span>
<span id="cb64-324"><a href="#cb64-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-327"><a href="#cb64-327" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb64-328"><a href="#cb64-328" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(simple_model)<span class="sc">$</span>sigma</span>
<span id="cb64-329"><a href="#cb64-329" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb64-330"><a href="#cb64-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-331"><a href="#cb64-331" aria-hidden="true" tabindex="-1"></a>In our simple regression model, the RSE is 3.483. This means that, on average, the actual exam scores deviate from the scores predicted by the model by approximately 3.48 points. Given that the mean exam score in our dataset is 67.24, we can express this as a percentage error of about 5.18% (3.483 / 67.24 × 100). Whether this level of error is acceptable depends entirely on the context of the research. In educational research, where human behavior is inherently variable and influenced by countless unmeasured factors like test-day anxiety, mood, or the specific questions that happened to appear on the exam, a prediction error of about 3.5 points may be considered quite reasonable. However, from a pure prediction standpoint, it also tells us that our simple model leaves a lot of room for improvement - knowing only how many hours a student studies does not allow us to predict their exam score with great precision.</span>
<span id="cb64-332"><a href="#cb64-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-335"><a href="#cb64-335" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb64-336"><a href="#cb64-336" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(multiple_model)<span class="sc">$</span>sigma</span>
<span id="cb64-337"><a href="#cb64-337" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb64-338"><a href="#cb64-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-339"><a href="#cb64-339" aria-hidden="true" tabindex="-1"></a>In our multiple regression model, the RSE drops to 2.467. This represents a substantial improvement over the simple model - the average prediction error has decreased by about 29%, from 3.48 to 2.47 points. The percentage error relative to the mean is now approximately 3.67% (2.467 / 67.24 × 100). This decrease makes intuitive sense: by adding attendance, previous scores, tutoring sessions, and physical activity to the model, we have incorporated additional information that helps explain why some students score higher than others. The model now captures more of the systematic patterns in the data, leaving less to the error term. It is worth noting, however, that the RSE can never reach zero unless our model perfectly predicts every single observation, which is essentially impossible with real-world data involving human behavior. There will always be some irreducible error that no model can eliminate.</span>
<span id="cb64-340"><a href="#cb64-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-341"><a href="#cb64-341" aria-hidden="true" tabindex="-1"></a>The RSE also plays a foundational role in the other inference tools we discussed earlier. Recall that the standard errors of the coefficient estimates depend on the RSE - a smaller RSE leads to smaller standard errors, which in turn leads to larger t-statistics and smaller p-values. This is exactly what we observed when comparing our two models: the multiple model had a smaller RSE, which produced more precise coefficient estimates and stronger evidence of statistical significance for the predictors that truly matter.</span>
<span id="cb64-342"><a href="#cb64-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-343"><a href="#cb64-343" aria-hidden="true" tabindex="-1"></a>One important limitation of the RSE is that it is measured in the units of the response variable, which makes it difficult to compare across different studies or datasets with different response scales. If another researcher studied a test scored on a scale of 0 to 500, their RSE would naturally be much larger in absolute terms, even if their model were proportionally just as accurate as ours. This limitation motivates the need for a scale-independent measure of model fit, which is exactly what the $R^2$ statistic provides.</span>
<span id="cb64-344"><a href="#cb64-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-345"><a href="#cb64-345" aria-hidden="true" tabindex="-1"></a><span class="fu">### $R^2$</span></span>
<span id="cb64-346"><a href="#cb64-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-347"><a href="#cb64-347" aria-hidden="true" tabindex="-1"></a>The **$R^2$** statistic, also known as the coefficient of determination, is one of the most commonly reported measures of model fit in applied research. Unlike the RSE, $R^2$ is a proportion that always takes a value between 0 and 1, making it easy to interpret and compare across studies regardless of the scale of the response variable. $R^2$ answers a very specific question: *what fraction of the total variation in the response variable is explained by the model?* To understand $R^2$, we need to consider two quantities.</span>
<span id="cb64-348"><a href="#cb64-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-349"><a href="#cb64-349" aria-hidden="true" tabindex="-1"></a>The first is the **Total Sum of Squares** (TSS), defined as:</span>
<span id="cb64-350"><a href="#cb64-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-351"><a href="#cb64-351" aria-hidden="true" tabindex="-1"></a>$TSS = \sum(y_i - \hat{y})^2$</span>
<span id="cb64-352"><a href="#cb64-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-353"><a href="#cb64-353" aria-hidden="true" tabindex="-1"></a>This measures the total variability in the response variable before any regression is performed. It is simply the sum of the squared deviations of each observed exam score from the overall mean exam score. In our dataset, this captures the full extent to which students' exam scores differ from one another. Some of this variation is systematic and some of it is random noise.</span>
<span id="cb64-354"><a href="#cb64-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-355"><a href="#cb64-355" aria-hidden="true" tabindex="-1"></a>The second quantity is the **Residual Sum of Squares** (RSS), which we have already encountered:</span>
<span id="cb64-356"><a href="#cb64-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-357"><a href="#cb64-357" aria-hidden="true" tabindex="-1"></a>$RSS = \sum(y_i - \hat{y}_i)^2$</span>
<span id="cb64-358"><a href="#cb64-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-359"><a href="#cb64-359" aria-hidden="true" tabindex="-1"></a>This measures the variability that remains unexplained after fitting the regression model. It is the sum of the squared residuals - the squared differences between the actual exam scores and the scores predicted by the model.</span>
<span id="cb64-360"><a href="#cb64-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-361"><a href="#cb64-361" aria-hidden="true" tabindex="-1"></a>The $R^2$ statistic is then defined as:</span>
<span id="cb64-362"><a href="#cb64-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-363"><a href="#cb64-363" aria-hidden="true" tabindex="-1"></a>$R^2 = \frac{(TSS - RSS)}{TSS} = 1 - \frac{RSS}{TSS}$</span>
<span id="cb64-364"><a href="#cb64-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-365"><a href="#cb64-365" aria-hidden="true" tabindex="-1"></a>The numerator, TSS - RSS, represents the amount of variability in the response that is explained by the regression - it is the reduction in prediction error achieved by using the model instead of simply predicting the mean for every student. Dividing by TSS converts this into a proportion.</span>
<span id="cb64-366"><a href="#cb64-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-367"><a href="#cb64-367" aria-hidden="true" tabindex="-1"></a>When $R^2$ is close to 1, it means that the model explains nearly all of the variation in the response, and the RSS is very small compared to the TSS. In such a case, the predicted values $\hat{y}_i$ are very close to the actual values $y_i$, and the model provides an excellent fit. When $R^2$ is close to 0, the model explains very little of the variation, and using the model is hardly better than simply predicting the mean exam score for every student.</span>
<span id="cb64-368"><a href="#cb64-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-371"><a href="#cb64-371" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb64-372"><a href="#cb64-372" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(simple_model)<span class="sc">$</span>r.squared</span>
<span id="cb64-373"><a href="#cb64-373" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb64-374"><a href="#cb64-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-375"><a href="#cb64-375" aria-hidden="true" tabindex="-1"></a>In our simple regression model, $R^2$ is 0.1984. This tells us that "hours_studied" alone explains approximately 19.84% of the total variation in exam scores. In other words, about one-fifth of the differences in exam scores among students can be attributed to differences in how many hours they study. This is a meaningful finding - it confirms that study time matters - but it also reveals that roughly 80% of the variation is driven by other factors not captured in this simple model.</span>
<span id="cb64-376"><a href="#cb64-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-379"><a href="#cb64-379" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb64-380"><a href="#cb64-380" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(multiple_model)<span class="sc">$</span>r.squared</span>
<span id="cb64-381"><a href="#cb64-381" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb64-382"><a href="#cb64-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-383"><a href="#cb64-383" aria-hidden="true" tabindex="-1"></a>In our multiple regression model, $R^2$ jumps to 0.5982. Now the model explains approximately 59.82% of the variation in exam scores. This is a dramatic improvement - by adding attendance, previous scores, tutoring sessions, and physical activity as predictors alongside study hours, we have nearly tripled the proportion of explained variance. The remaining approximately 40% of the variation is still unexplained, presumably due to factors that are not included as quantitative predictors in this model - things like motivation level, family income, teacher quality, peer influence, and other qualitative variables in our dataset that we have not yet incorporated, as well as entirely unmeasured factors like test anxiety, the specific content of the exam, or simple luck.</span>
<span id="cb64-384"><a href="#cb64-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-385"><a href="#cb64-385" aria-hidden="true" tabindex="-1"></a>What constitutes a "good" $R^2$ depends heavily on the field of study. In the physical sciences, where experiments can be tightly controlled and measurement is very precise, $R^2$ values above 0.95 are common and expected. In the social sciences and education research, where human behavior is inherently noisy and influenced by a vast number of interacting factors, $R^2$ values between 0.30 and 0.60 are often considered quite good for observational studies. Our multiple model's $R^2$ of 0.598 is therefore quite respectable for educational data - it suggests that we have identified a set of predictors that genuinely capture a large portion of what drives student performance.</span>
<span id="cb64-386"><a href="#cb64-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-387"><a href="#cb64-387" aria-hidden="true" tabindex="-1"></a>It is critical to understand one important caveat about $R^2$: it will always increase (or at least never decrease) when more predictors are added to the model, even if those predictors are completely unrelated to the response. This happens because adding any variable, even a random one, gives the model more flexibility to fit the training data, and the RSS can only go down or stay the same - it can never go up. This means that a high $R^2$ does not necessarily indicate a good model; it could simply reflect the fact that many predictors have been thrown in. To guard against this problem, the **Adjusted $R^2$** was developed. The Adjusted $R^2$ modifies the standard $R^2$ by imposing a penalty for each additional predictor. If adding a new predictor does not reduce the RSS enough to offset the penalty for the lost degree of freedom, the Adjusted $R^2$ will actually decrease, signaling that the added predictor is not contributing meaningfully.</span>
<span id="cb64-388"><a href="#cb64-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-391"><a href="#cb64-391" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb64-392"><a href="#cb64-392" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(simple_model)<span class="sc">$</span>adj.r.squared</span>
<span id="cb64-393"><a href="#cb64-393" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(multiple_model)<span class="sc">$</span>adj.r.squared</span>
<span id="cb64-394"><a href="#cb64-394" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb64-395"><a href="#cb64-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-396"><a href="#cb64-396" aria-hidden="true" tabindex="-1"></a>In our simple model, the Adjusted $R^2$ is 0.1983, virtually identical to $R^2$ because there is only one predictor and the penalty is negligible. In our multiple model, the Adjusted $R^2$ is 0.5979, also nearly identical to the regular $R^2$ of 0.5982. The fact that the Adjusted $R^2$ barely differs from $R^2$ in the multiple model tells us that all six predictors (or at least most of them) are genuinely contributing to the model's explanatory power - the improvement in fit is not merely an artifact of adding more variables.</span>
<span id="cb64-397"><a href="#cb64-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-398"><a href="#cb64-398" aria-hidden="true" tabindex="-1"></a>It is also worth noting the connection between $R^2$ and correlation. In simple linear regression, $R^2$ is exactly equal to the square of the Pearson correlation coefficient r between X and Y. In our case, $R^2$ = 0.1984 for the simple model, so the correlation between "hours_studied" and "exam_score" is $r = \sqrt{0.1984} \approx 0.445$. In multiple regression, this simple relationship no longer holds (because there are multiple predictors), but $R^2$ can be shown to equal the squared correlation between the observed values $y_i$ and the fitted values $\hat{y}_i$. This provides a nice intuitive interpretation: $R^2$ tells us how closely the model's predictions track the actual outcomes.</span>
<span id="cb64-399"><a href="#cb64-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-400"><a href="#cb64-400" aria-hidden="true" tabindex="-1"></a><span class="fu">### F-statistic</span></span>
<span id="cb64-401"><a href="#cb64-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-402"><a href="#cb64-402" aria-hidden="true" tabindex="-1"></a>While the t-statistic and its associated p-value allow us to test whether each individual predictor is significantly related to the response, the **F-statistic** addresses a different and more fundamental question: *is the model as a whole useful?* Specifically, the F-statistic tests the null hypothesis that all slope coefficients in the model are simultaneously equal to zero $H_0: \beta_1 = \beta_2 = ... = \beta_p = 0$ against the alternative hypothesis:</span>
<span id="cb64-403"><a href="#cb64-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-404"><a href="#cb64-404" aria-hidden="true" tabindex="-1"></a>$H_a$: at least one $\beta_j$ is non-zero.</span>
<span id="cb64-405"><a href="#cb64-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-406"><a href="#cb64-406" aria-hidden="true" tabindex="-1"></a>If the null hypothesis is true, then none of the predictors have any relationship with the response, and the model is no better than simply predicting the mean for every observation. The F-statistic is computed as:</span>
<span id="cb64-407"><a href="#cb64-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-408"><a href="#cb64-408" aria-hidden="true" tabindex="-1"></a>$F = \frac{\frac{TSS - RSS}{p}}{\frac{RSS}{n - p -1}}$</span>
<span id="cb64-409"><a href="#cb64-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-410"><a href="#cb64-410" aria-hidden="true" tabindex="-1"></a>The numerator measures how much of the total variance the model explains, divided by the number of predictors p. The denominator measures how much variance remains unexplained, divided by the residual degrees of freedom. If the model is no better than chance, the numerator and denominator should be roughly equal, producing an F-statistic close to 1. If the model captures real patterns in the data, the numerator will be much larger than the denominator, producing a large F-statistic.</span>
<span id="cb64-411"><a href="#cb64-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-412"><a href="#cb64-412" aria-hidden="true" tabindex="-1"></a>One might reasonably ask: why do we need the F-statistic at all when we already have individual t-tests for each coefficient? The answer lies in the multiple testing problem. When we have many predictors, each with its own t-test, the probability of finding at least one "significant" result by pure chance increases dramatically. For example, if we tested 100 completely useless predictors at the 5% significance level, we would expect about 5 of them to appear significant purely by chance. The F-statistic avoids this problem because it is a single, omnibus test that accounts for the total number of predictors. It maintains the correct overall error rate regardless of how many predictors are in the model. So the proper approach is to first check the F-statistic to determine whether the model as a whole is significant, and only then examine the individual t-statistics to identify which specific predictors are contributing.</span>
<span id="cb64-413"><a href="#cb64-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-416"><a href="#cb64-416" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb64-417"><a href="#cb64-417" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(simple_model)<span class="sc">$</span>fstatistic</span>
<span id="cb64-418"><a href="#cb64-418" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb64-419"><a href="#cb64-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-420"><a href="#cb64-420" aria-hidden="true" tabindex="-1"></a>In our simple regression model, the F-statistic is 1,635 with 1 and 6,605 degrees of freedom, and the associated p-value is less than $2.2 \times 10^{16}$. Since we have only one predictor in the simple model, the F-test is equivalent to the t-test for that predictor. In fact, the F-statistic in simple regression is exactly the square of the t-statistic: $40.44^2 \approx 1,635$. The overwhelming magnitude of this F-statistic and its essentially zero p-value tell us that the model is highly significant - "hours_studied" is unquestionably related to "exam_score".</span>
<span id="cb64-421"><a href="#cb64-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-424"><a href="#cb64-424" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb64-425"><a href="#cb64-425" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(multiple_model)<span class="sc">$</span>fstatistic</span>
<span id="cb64-426"><a href="#cb64-426" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb64-427"><a href="#cb64-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-428"><a href="#cb64-428" aria-hidden="true" tabindex="-1"></a>In our multiple regression model, the F-statistic is 1,638 with 6 and 6,600 degrees of freedom, and the p-value is again less than $2.2 \times 10^{-16}$. This tests whether at least one of the six predictors is related to exam scores. The result decisively rejects the null hypothesis - the model as a whole is highly significant, and at least one (and as we saw from the individual t-tests, five out of six) predictors are genuinely related to student performance. The fact that the F-statistic is 1,638 rather than close to 1 tells us that the model explains vastly more variance than we would expect by chance alone.</span>
<span id="cb64-429"><a href="#cb64-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-430"><a href="#cb64-430" aria-hidden="true" tabindex="-1"></a>It is worth pausing to note an interesting detail. Even though "sleep_hours" was not individually significant (p = 0.384), the overall F-test is still overwhelmingly significant. This is entirely consistent: the F-test only requires that at least one predictor be related to the response, and the other five predictors more than satisfy this requirement. The F-test does not tell us which predictors are significant - that is the job of the individual t-tests. But it does tell us that the model as a whole is capturing real patterns.</span>
<span id="cb64-431"><a href="#cb64-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-432"><a href="#cb64-432" aria-hidden="true" tabindex="-1"></a><span class="fu">## Interaction Terms</span></span>
<span id="cb64-433"><a href="#cb64-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-434"><a href="#cb64-434" aria-hidden="true" tabindex="-1"></a>Up to this point, all the linear regression models we have discussed have assumed something that may seem natural but is in fact a very strong assumption: that the effect of each predictor on the response is independent of the values of the other predictors. This is called the **additivity assumption**, and it is built into the standard multiple linear regression model. When we write our model as $exam_score = \beta_0 + \beta_1 \times hours_studied + \beta_2 \times attendance + \epsilon$, we are implicitly saying that the effect of studying one additional hour is always the same - roughly 0.29 additional points - regardless of whether a student has 60% attendance or 100% attendance. Similarly, we are saying that the effect of one additional percentage point of attendance is always the same, regardless of how many hours the student studies. The model treats each predictor's contribution as completely separate and simply adds them together, which is where the term "additive" comes from.</span>
<span id="cb64-435"><a href="#cb64-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-436"><a href="#cb64-436" aria-hidden="true" tabindex="-1"></a>But is this assumption realistic? Consider the following scenario. A student who attends nearly all classes has been exposed to lectures, discussions, and in-class explanations throughout the course. When this student sits down to study at home, each hour of studying is highly productive, because the student is reinforcing and deepening material they have already encountered in the classroom. Now consider a student who has very low attendance and has missed most of the lectures. When this student tries to study, each hour of studying may be less productive, because the student must learn the material from scratch rather than building on what was covered in class. In this scenario, the effect of study hours on exam scores depends on attendance - studying is more effective for students who also attend class regularly. This is exactly the kind of phenomenon that the additive model cannot capture, and it is what we call an **interaction effect**. In statistics, an interaction effect occurs when the relationship between one predictor and the response changes depending on the value of another predictor.</span>
<span id="cb64-437"><a href="#cb64-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-438"><a href="#cb64-438" aria-hidden="true" tabindex="-1"></a>To incorporate an interaction effect into a linear regression model, we create a new predictor variable that is the product of two existing predictors. Standard additive model with two predictors assumes that the effect of $X_1$ on Y is $\beta_1$, regardless of the value of $X_2$. To relax this assumption, we add a third term - the interaction term - which is simply the product $X_1 \times X_2$:</span>
<span id="cb64-439"><a href="#cb64-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-440"><a href="#cb64-440" aria-hidden="true" tabindex="-1"></a>$Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3(X_1 \times X_2) + \epsilon$</span>
<span id="cb64-441"><a href="#cb64-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-442"><a href="#cb64-442" aria-hidden="true" tabindex="-1"></a>The key to understanding how this works is to rearrange the equation algebraically. We can rewrite it as:</span>
<span id="cb64-443"><a href="#cb64-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-444"><a href="#cb64-444" aria-hidden="true" tabindex="-1"></a>$Y = \beta_0 + (\beta_1 + \beta_3X_2)X_1 + \beta_2X_2 + \epsilon$</span>
<span id="cb64-445"><a href="#cb64-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-446"><a href="#cb64-446" aria-hidden="true" tabindex="-1"></a>Written in this form, it becomes clear that the effective slope of $X_1$ is no longer a fixed constant $\beta_1$ - it is now $(\beta_1 + \beta_3X_2)$, which depends on the value of $X_2$. In other words, the effect of $X_1$ on Y changes as $X_2$ changes. If $\beta_3$ is positive, then higher values of $X_2$ amplify the effect of $X_1$. If $\beta_3$ is negative, then higher values of $X_2$ diminish the effect of $X_1$. And if $\beta_3$ is zero, then the interaction is absent and we are back to the simple additive model.</span>
<span id="cb64-447"><a href="#cb64-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-448"><a href="#cb64-448" aria-hidden="true" tabindex="-1"></a>The same rearrangement works in the other direction. We can also write the model as:</span>
<span id="cb64-449"><a href="#cb64-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-450"><a href="#cb64-450" aria-hidden="true" tabindex="-1"></a>$Y = \beta_0 + \beta_1X_1 + (\beta_2 + \beta_3X_1)X_2 + \epsilon$</span>
<span id="cb64-451"><a href="#cb64-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-452"><a href="#cb64-452" aria-hidden="true" tabindex="-1"></a>This shows that the effect of $X_2$ on Y is $(\beta_2 + \beta_3X_1)$, which depends on $X_1$. The interaction is symmetric: if the effect of study hours depends on attendance, then equally the effect of attendance depends on study hours. The interaction coefficient $\beta_3$ captures this mutual dependence.</span>
<span id="cb64-453"><a href="#cb64-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-454"><a href="#cb64-454" aria-hidden="true" tabindex="-1"></a>In our Student Performance example, an interaction between Hours_Studied and Attendance would be expressed as:</span>
<span id="cb64-455"><a href="#cb64-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-456"><a href="#cb64-456" aria-hidden="true" tabindex="-1"></a>exam_score = $\beta_0$ + $\beta_1$ $\times$ hours_studied + $\beta_2$ $\times$ attendance + $\beta_3$ $\times$ (hours_studied $\times$ attendance) + $\epsilon$</span>
<span id="cb64-457"><a href="#cb64-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-458"><a href="#cb64-458" aria-hidden="true" tabindex="-1"></a>The coefficient $\beta_3$ would tell us how the effectiveness of studying changes as attendance increases (or equivalently, how the effectiveness of attendance changes as study hours increase). If $\beta_3$ is positive and significant, it would confirm our intuition that studying and attending class work together synergistically - each one makes the other more effective.</span>
<span id="cb64-459"><a href="#cb64-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-460"><a href="#cb64-460" aria-hidden="true" tabindex="-1"></a>In R, there are two convenient ways to specify interaction terms. The syntax <span class="in">`hours_studied:attendance`</span> includes only the interaction term itself, while the syntax <span class="in">`hours_studied * attendance`</span> is a shorthand that automatically includes both the individual predictors (called main effects) and their interaction. In other words, <span class="in">`hours_studied * attendance`</span> is equivalent to writing <span class="in">`hours_studied + attendance + hours_studied:attendance`</span>.</span>
<span id="cb64-461"><a href="#cb64-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-462"><a href="#cb64-462" aria-hidden="true" tabindex="-1"></a>We have now fitted three models that allow us to progressively examine whether an interaction exists between "hours_studied" and "attendance" in predicting "exam_score". The results tell a clear and instructive story - one that is just as valuable for what it does not find as for what it does find.</span>
<span id="cb64-463"><a href="#cb64-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-466"><a href="#cb64-466" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb64-467"><a href="#cb64-467" aria-hidden="true" tabindex="-1"></a>additive_model <span class="ot">&lt;-</span> <span class="fu">lm</span>(</span>
<span id="cb64-468"><a href="#cb64-468" aria-hidden="true" tabindex="-1"></a>    exam_score <span class="sc">~</span> hours_studied <span class="sc">+</span> attendance,</span>
<span id="cb64-469"><a href="#cb64-469" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> student_performance</span>
<span id="cb64-470"><a href="#cb64-470" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb64-471"><a href="#cb64-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-472"><a href="#cb64-472" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(additive_model)</span>
<span id="cb64-473"><a href="#cb64-473" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb64-474"><a href="#cb64-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-475"><a href="#cb64-475" aria-hidden="true" tabindex="-1"></a>Our additive model serves as the baseline. It estimates the following equation:</span>
<span id="cb64-476"><a href="#cb64-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-477"><a href="#cb64-477" aria-hidden="true" tabindex="-1"></a>exam_score ≈ 45.60 + 0.293 $\times$ hours_studied + 0.197 $\times$ attendance</span>
<span id="cb64-478"><a href="#cb64-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-479"><a href="#cb64-479" aria-hidden="true" tabindex="-1"></a>In this model, the effect of each additional hour of study is always 0.293 points, regardless of how often the student attends class. And the effect of each additional percentage point of attendance is always 0.197 points, regardless of how many hours the student studies. The two predictors operate independently - their contributions are simply added together. The model explains 54.13% of the variance in exam scores ($R^2$ = 0.5413), with a residual standard error of 2.635. This additive interpretation may or may not reflect reality. It is possible that studying and attending class reinforce each other - that the benefit of studying is amplified when a student has also been attending lectures regularly. The interaction model tests this possibility directly.</span>
<span id="cb64-480"><a href="#cb64-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-483"><a href="#cb64-483" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb64-484"><a href="#cb64-484" aria-hidden="true" tabindex="-1"></a>interaction_model <span class="ot">&lt;-</span> <span class="fu">lm</span>(</span>
<span id="cb64-485"><a href="#cb64-485" aria-hidden="true" tabindex="-1"></a>    exam_score <span class="sc">~</span> hours_studied <span class="sc">*</span> attendance,</span>
<span id="cb64-486"><a href="#cb64-486" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> student_performance</span>
<span id="cb64-487"><a href="#cb64-487" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb64-488"><a href="#cb64-488" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(interaction_model)</span>
<span id="cb64-489"><a href="#cb64-489" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(interaction_model)</span>
<span id="cb64-490"><a href="#cb64-490" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb64-491"><a href="#cb64-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-492"><a href="#cb64-492" aria-hidden="true" tabindex="-1"></a>The interaction model adds the product of Hours_Studied and Attendance as a new predictor. The estimated equation is:</span>
<span id="cb64-493"><a href="#cb64-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-494"><a href="#cb64-494" aria-hidden="true" tabindex="-1"></a>exam_score $\approx$ 46.93 + 0.227 $\times$ hours_studied + 0.181 $\times$ attendance + 0.000831 $\times$ (hours_studied $\times$ attendance)</span>
<span id="cb64-495"><a href="#cb64-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-496"><a href="#cb64-496" aria-hidden="true" tabindex="-1"></a>Let us carefully examine each coefficient and what it means in the context of this model, because the interpretation of coefficients changes fundamentally when an interaction term is present. The intercept of 46.93 represents the predicted exam score when both "hours_studied" and "attendance" are zero. As always, this is a mathematical anchor point for the model rather than a substantively meaningful quantity, since no real student has zero hours of study and zero percent attendance. The coefficient for "hours_studied" is now 0.227, which is noticeably different from its value in the additive model (0.293). However, the interpretation of this coefficient has changed. In the additive model, 0.293 represented the effect of study hours at any level of attendance.</span>
<span id="cb64-497"><a href="#cb64-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-498"><a href="#cb64-498" aria-hidden="true" tabindex="-1"></a>In the interaction model, 0.227 represents the effect of study hours specifically when "attendance" equals zero. This is because when we rearrange the interaction model as exam_score = 46.93 + (0.227 + 0.000831 $\times$ attendance) $\times$ hours_studied + 0.181 $\times$ attendance we can see that the effective slope for "hours_studied" is (0.227 + 0.000831 $\times$ attendance). When "attendance" is zero, this reduces to 0.227. When "attendance" is at its mean of about 80, the effective slope becomes 0.227 + 0.000831 × 80 = 0.227 + 0.066 = 0.293 - which is almost exactly the slope we found in the additive model. This is reassuring and makes intuitive sense: the additive model's coefficient represents a kind of average effect across all attendance levels, and that average coincides with the effective slope at the mean level of attendance.</span>
<span id="cb64-499"><a href="#cb64-499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-500"><a href="#cb64-500" aria-hidden="true" tabindex="-1"></a>Similarly, the coefficient for "attendance" is 0.181, which represents the effect of attendance when "hours_studied" equals zero. The effective slope for "attendance" at the mean study hours of about 20 is 0.181 + 0.000831 $\times$ 20 = 0.181 + 0.017 = 0.198 - again, essentially the same as in the additive model.</span>
<span id="cb64-501"><a href="#cb64-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-502"><a href="#cb64-502" aria-hidden="true" tabindex="-1"></a>The interaction coefficient itself is 0.000831. This is the critical number. It tells us how the effect of one predictor changes for each one-unit increase in the other. Specifically, for each additional percentage point of attendance, the effect of one hour of study increases by 0.000831 points. Conversely, for each additional hour of study, the effect of one percentage point of attendance increases by 0.000831 points. In concrete terms, this means that a student with 90% attendance gains 0.227 + 0.000831 $\times$ 90 = 0.302 points per hour of study, while a student with 70% attendance gains 0.227 + 0.000831 $\times$ 70 = 0.285 points per hour of study. The difference is 0.017 points per hour - a very small amount.</span>
<span id="cb64-503"><a href="#cb64-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-504"><a href="#cb64-504" aria-hidden="true" tabindex="-1"></a>Now, the crucial question is whether this interaction effect is statistically significant. The t-statistic for the interaction term is 1.795, and the p-value is 0.0727. This p-value is above the conventional 0.05 significance threshold, although it is below 0.10. The 95% confidence interval for the interaction coefficient is <span class="co">[</span><span class="ot">-0.0000766, 0.001738</span><span class="co">]</span>, which includes zero. This means that at the 5% significance level, we cannot reject the null hypothesis that the interaction coefficient is zero. The evidence for an interaction is suggestive but not strong enough to meet the conventional standard of statistical significance.</span>
<span id="cb64-505"><a href="#cb64-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-506"><a href="#cb64-506" aria-hidden="true" tabindex="-1"></a>Looking at the model-level statistics reinforces this conclusion. The R² of the interaction model is 0.5415, compared to 0.5413 for the additive model. The increase is only 0.0002 - an almost imperceptible improvement. The residual standard error remains at 2.635, completely unchanged. Adding the interaction term has contributed virtually nothing to the model's explanatory power.</span>
<span id="cb64-507"><a href="#cb64-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-510"><a href="#cb64-510" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb64-511"><a href="#cb64-511" aria-hidden="true" tabindex="-1"></a>full_interaction_model <span class="ot">&lt;-</span> <span class="fu">lm</span>(</span>
<span id="cb64-512"><a href="#cb64-512" aria-hidden="true" tabindex="-1"></a>    exam_score <span class="sc">~</span> hours_studied <span class="sc">*</span> attendance <span class="sc">+</span> previous_scores <span class="sc">+</span> tutoring_sessions <span class="sc">+</span> physical_activity,</span>
<span id="cb64-513"><a href="#cb64-513" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> student_performance</span>
<span id="cb64-514"><a href="#cb64-514" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb64-515"><a href="#cb64-515" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(full_interaction_model)</span>
<span id="cb64-516"><a href="#cb64-516" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(full_interaction_model)</span>
<span id="cb64-517"><a href="#cb64-517" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb64-518"><a href="#cb64-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-519"><a href="#cb64-519" aria-hidden="true" tabindex="-1"></a>Our third model includes the interaction between Hours_Studied and Attendance alongside three additional predictors: "previous_scores", "tutoring_sessions", and "physical_activity". The estimated equation is:</span>
<span id="cb64-520"><a href="#cb64-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-521"><a href="#cb64-521" aria-hidden="true" tabindex="-1"></a>exam_score $\approx$ 41.81 + 0.241 $\times$ hours_studied + 0.185 $\times$ attendance + 0.048 $\times$ previous_scores + 0.494 $\times$ tutoring_sessions + 0.144 $\times$ physical_activity + 0.000636 $\times$ (hours_studied $\times$ attendance)</span>
<span id="cb64-522"><a href="#cb64-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-523"><a href="#cb64-523" aria-hidden="true" tabindex="-1"></a>In this richer model, the interaction coefficient has shrunk further to 0.000636, the t-statistic has decreased to 1.468, and the p-value has increased to 0.142. The 95% confidence interval is <span class="co">[</span><span class="ot">-0.000213, 0.001486</span><span class="co">]</span>, which now spans zero even more broadly than before. The interaction effect is clearly not statistically significant in this model, and its magnitude is even smaller than in Model 2.</span>
<span id="cb64-524"><a href="#cb64-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-525"><a href="#cb64-525" aria-hidden="true" tabindex="-1"></a>The $R^2$ of this full interaction model is 0.5983, compared to 0.5982 for the multiple regression model without the interaction term that we fitted earlier. The difference is 0.0001 — the interaction term adds essentially no explanatory power beyond what is already captured by the main effects and the other predictors. The residual standard error is 2.467, identical to the model without the interaction.</span>
<span id="cb64-526"><a href="#cb64-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-527"><a href="#cb64-527" aria-hidden="true" tabindex="-1"></a>The <span class="in">`anova()`</span> function at the end performs a formal comparison between the additive model and the interaction model. It tests whether the addition of the interaction term leads to a statistically significant improvement in model fit, using an F-test that compares the RSS of the two models. If the interaction term significantly reduces the RSS, the F-statistic will be large and the p-value will be small, indicating that the interaction is a meaningful addition to the model.</span>
<span id="cb64-528"><a href="#cb64-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-531"><a href="#cb64-531" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb64-532"><a href="#cb64-532" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(additive_model, interaction_model)</span>
<span id="cb64-533"><a href="#cb64-533" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb64-534"><a href="#cb64-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-535"><a href="#cb64-535" aria-hidden="true" tabindex="-1"></a>The ANOVA output provides a formal statistical test that directly compares our two nested models: Model 1, the additive model with only "hours_studied" and "attendance" as separate predictors, and Model 2, the interaction model that adds the hours_studied $\times$ "attendance" interaction term. The word "nested" is important here - it means that Model 1 is a special case of Model 2, obtained by setting the interaction coefficient to zero. The ANOVA test asks whether allowing that coefficient to be non-zero produces a meaningfully better fit to the data.</span>
<span id="cb64-536"><a href="#cb64-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-537"><a href="#cb64-537" aria-hidden="true" tabindex="-1"></a>The output shows us the residual degrees of freedom and the residual sum of squares (RSS) for each model. Model 1 has 6,604 residual degrees of freedom and an RSS of 45,868. Model 2 has 6,603 residual degrees of freedom and an RSS of 45,846. The difference in RSS between the two models is 22.369, which represents the amount of additional variance in exam scores that is explained by including the interaction term. The column labeled "Df" shows that the difference is 1 degree of freedom, confirming that exactly one additional parameter was added.</span>
<span id="cb64-538"><a href="#cb64-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-539"><a href="#cb64-539" aria-hidden="true" tabindex="-1"></a>The F-statistic for this comparison is 3.2217. Recall from our earlier discussion that the F-statistic is constructed by comparing the improvement in fit (the reduction in RSS) to the amount of variance that remains unexplained. Specifically, it takes the reduction in RSS per additional parameter (22.369 / 1 = 22.369) and divides it by the residual mean square of the fuller model (45,846 / 6,603 $\approx$ 6.943). The resulting F-value of 3.22 tells us that the interaction term reduced the RSS by about 3.22 times more than what a single random, useless predictor would be expected to reduce it. This is a modest improvement - certainly not trivial, but not overwhelming either.</span>
<span id="cb64-540"><a href="#cb64-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-541"><a href="#cb64-541" aria-hidden="true" tabindex="-1"></a>The p-value is 0.07271, which is shown with a single dot (.) next to it in R's significance coding system, indicating that it falls between 0.05 and 0.10. This p-value means that if the true interaction coefficient were zero (that is, if there were truly no interaction between studying and attendance in the population), there would be approximately a 7.3% probability of observing an improvement in fit as large as or larger than the one we found. At the conventional 5% significance level, we do not reject the null hypothesis - the interaction term does not provide a statistically significant improvement in model fit. The evidence is suggestive, sitting in that ambiguous zone between 0.05 and 0.10, but it does not meet the standard threshold for statistical significance.</span>
<span id="cb64-542"><a href="#cb64-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-543"><a href="#cb64-543" aria-hidden="true" tabindex="-1"></a>It is worth noting how perfectly consistent this ANOVA result is with the individual coefficient test for the interaction term that we saw in Model 2's summary output. The t-statistic for the interaction coefficient was 1.795, and its p-value was 0.0727 - virtually identical to the ANOVA p-value. This is not a coincidence. When we compare two models that differ by exactly one predictor, the ANOVA F-statistic is exactly the square of the t-statistic for that predictor ($1.795^2$ $\approx$ 3.222), and the p-values are identical. The two tests are mathematically equivalent in this case. The ANOVA approach becomes particularly valuable when we want to compare models that differ by more than one predictor - for instance, if we wanted to test whether a whole set of interaction terms simultaneously improves the model, we could not do this with individual t-tests, but we could do it with a single ANOVA F-test.</span>
<span id="cb64-544"><a href="#cb64-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-545"><a href="#cb64-545" aria-hidden="true" tabindex="-1"></a>To put the practical magnitude of this result in perspective, the interaction term reduced the RSS from 45,868 to 45,846 - a reduction of just 22.369 out of a total RSS of 45,868, which amounts to a 0.049% reduction in unexplained variance. The $R^2$ increased from 0.5413 to 0.5415, a gain of 0.0002. These numbers confirm what the p-value already suggested: even if the interaction were real, its practical importance would be negligible. The additive model, which is simpler and easier to interpret, captures the relationships between study hours, attendance, and exam scores just as effectively as the interaction model does. For this reason, both on statistical grounds and on grounds of parsimony, we would choose to proceed with the additive model and conclude that "hours_studied" and "attendance" contribute independently to student performance in this dataset.</span>
<span id="cb64-546"><a href="#cb64-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-547"><a href="#cb64-547" aria-hidden="true" tabindex="-1"></a>The decision to include interaction terms should be guided by both theory and evidence. From a theoretical standpoint, we should include interactions when we have a substantive reason to believe that the effect of one predictor depends on the value of another. In educational research, for example, we might hypothesize that the effect of tutoring depends on motivation level (highly motivated students may benefit more from tutoring), or that the effect of parental involvement depends on family income (parental involvement may matter more in lower-income families where other resources are scarce). These are theoretically grounded hypotheses that deserve to be tested.</span>
<span id="cb64-548"><a href="#cb64-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-549"><a href="#cb64-549" aria-hidden="true" tabindex="-1"></a>From an empirical standpoint, we should look for evidence in the data - specifically, a significant interaction coefficient with a meaningful magnitude. In our case, the evidence does not support the interaction between "hours_studied" and "attendance": the coefficient is tiny, the p-value exceeds 0.05, the confidence interval includes zero, and the $R^2$ improvement is negligible. The additive model is therefore preferred on grounds of parsimony - it is simpler, easier to interpret, and fits the data just as well. Including a non-significant interaction term would add unnecessary complexity to the model without any compensating benefit in explanatory power or predictive accuracy.</span>
<span id="cb64-550"><a href="#cb64-550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-551"><a href="#cb64-551" aria-hidden="true" tabindex="-1"></a><span class="fu">## Hierarchical Linear Regression</span></span>
<span id="cb64-552"><a href="#cb64-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-553"><a href="#cb64-553" aria-hidden="true" tabindex="-1"></a>In all the models we have fitted so far, we have made decisions about which predictors to include based on theoretical reasoning and then examined the results as a single, complete model. But in social science research, we are often interested in something more nuanced than simply knowing which predictors are significant. We want to understand how different groups of factors contribute to the outcome, and specifically, whether adding a new group of factors improves our ability to explain the response above and beyond what was already explained by previously entered factors. This is the core logic behind hierarchical regression, also known as sequential regression or blockwise regression.</span>
<span id="cb64-554"><a href="#cb64-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-555"><a href="#cb64-555" aria-hidden="true" tabindex="-1"></a>**Hierarchical regression** or sequential regression is not a fundamentally different statistical technique from the multiple linear regression we have already discussed. It uses exactly the same least squares estimation, the same coefficient estimates, the same standard errors, and the same t-statistics. What makes it distinctive is the strategy for entering predictors into the model. Rather than entering all predictors at once, the researcher builds the model in a series of deliberate steps - called blocks or stages - adding one group of theoretically related predictors at each step. After each block is added, the researcher examines how the model's explanatory power changes, paying particular attention to the change in $R^2$ (denoted $\Delta R^2$). This approach allows us to ask questions like: how much additional variance in exam scores do study habits explain, after we have already accounted for students' baseline academic ability? Or: does the school environment contribute anything meaningful once we already know about students' personal characteristics and study behavior?</span>
<span id="cb64-556"><a href="#cb64-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-557"><a href="#cb64-557" aria-hidden="true" tabindex="-1"></a>The order in which blocks are entered is not arbitrary - it should be guided by theory, prior research, or the specific research questions being investigated. Typically, researchers enter more fundamental, stable, or demographic variables first, and then add variables that are more proximal, malleable, or of primary theoretical interest in subsequent blocks. The rationale is that by entering background variables first, we establish a baseline, and then we can see whether the variables we are most interested in contribute explanatory power beyond what those background factors already provide.</span>
<span id="cb64-558"><a href="#cb64-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-559"><a href="#cb64-559" aria-hidden="true" tabindex="-1"></a>For our <span class="in">`student_performance`</span> dataset, a theoretically motivated hierarchical regression might proceed as follows. In the first block, we would enter a variable that captures students' baseline academic ability - "previous_scores". This is the most fundamental predictor, as it reflects everything a student brings to the table before the current course even begins: their prior knowledge, their learning capacity, and their historical academic trajectory. By entering this first, we establish how much of the variation in exam scores is explained simply by pre-existing differences in ability.</span>
<span id="cb64-560"><a href="#cb64-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-561"><a href="#cb64-561" aria-hidden="true" tabindex="-1"></a>In the second block, we would add variables related to study behavior and engagement - "hours_studied" and "attendance". These represent the deliberate efforts a student makes during the course. The key question at this stage is: do study habits and class attendance explain additional variance in exam scores above and beyond what is already explained by baseline ability? If $\Delta R^2$ is large and significant at this step, it tells us that what students do during the course matters over and above what they could do coming in.</span>
<span id="cb64-562"><a href="#cb64-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-563"><a href="#cb64-563" aria-hidden="true" tabindex="-1"></a>In the third block, we would add variables related to academic support - "tutoring_sessions". This captures the additional help a student receives outside of regular classes and personal study. The question here is whether external academic support adds anything once we already know about baseline ability and personal effort.</span>
<span id="cb64-564"><a href="#cb64-564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-565"><a href="#cb64-565" aria-hidden="true" tabindex="-1"></a>In the fourth and final block, we would add variables related to lifestyle and well-being - "sleep_hours" and "physical_activity". These are factors that are somewhat more distal from academic performance, and we are interested in whether they contribute any additional explanatory power after the more directly academic factors have already been accounted for.</span>
<span id="cb64-566"><a href="#cb64-566" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-567"><a href="#cb64-567" aria-hidden="true" tabindex="-1"></a>In R, hierarchical regression is implemented by fitting a series of separate <span class="in">`lm()`</span> models, each adding a new block of predictors to the previous model. We then compare the models using the <span class="in">`anova()`</span> function, which performs an F-test for the significance of the improvement at each step, and we manually compute the change in $R^2$ between steps.</span>
<span id="cb64-568"><a href="#cb64-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-571"><a href="#cb64-571" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb64-572"><a href="#cb64-572" aria-hidden="true" tabindex="-1"></a>block1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(</span>
<span id="cb64-573"><a href="#cb64-573" aria-hidden="true" tabindex="-1"></a>    exam_score <span class="sc">~</span> previous_scores,</span>
<span id="cb64-574"><a href="#cb64-574" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> student_performance</span>
<span id="cb64-575"><a href="#cb64-575" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb64-576"><a href="#cb64-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-577"><a href="#cb64-577" aria-hidden="true" tabindex="-1"></a>block2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(</span>
<span id="cb64-578"><a href="#cb64-578" aria-hidden="true" tabindex="-1"></a>    exam_score <span class="sc">~</span> previous_scores <span class="sc">+</span> hours_studied <span class="sc">+</span> attendance,</span>
<span id="cb64-579"><a href="#cb64-579" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> student_performance</span>
<span id="cb64-580"><a href="#cb64-580" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb64-581"><a href="#cb64-581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-582"><a href="#cb64-582" aria-hidden="true" tabindex="-1"></a>block3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(</span>
<span id="cb64-583"><a href="#cb64-583" aria-hidden="true" tabindex="-1"></a>    exam_score <span class="sc">~</span> previous_scores <span class="sc">+</span> hours_studied <span class="sc">+</span> attendance <span class="sc">+</span> tutoring_sessions,</span>
<span id="cb64-584"><a href="#cb64-584" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> student_performance</span>
<span id="cb64-585"><a href="#cb64-585" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb64-586"><a href="#cb64-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-587"><a href="#cb64-587" aria-hidden="true" tabindex="-1"></a>block4 <span class="ot">&lt;-</span> <span class="fu">lm</span>(</span>
<span id="cb64-588"><a href="#cb64-588" aria-hidden="true" tabindex="-1"></a>    exam_score <span class="sc">~</span> previous_scores <span class="sc">+</span> hours_studied <span class="sc">+</span> attendance <span class="sc">+</span> tutoring_sessions <span class="sc">+</span> sleep_hours <span class="sc">+</span> physical_activity,</span>
<span id="cb64-589"><a href="#cb64-589" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> student_performance</span>
<span id="cb64-590"><a href="#cb64-590" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb64-591"><a href="#cb64-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-592"><a href="#cb64-592" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(block1, block2, block3, block4)</span>
<span id="cb64-593"><a href="#cb64-593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-594"><a href="#cb64-594" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(block1)<span class="sc">$</span>r.squared</span>
<span id="cb64-595"><a href="#cb64-595" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(block2)<span class="sc">$</span>r.squared</span>
<span id="cb64-596"><a href="#cb64-596" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(block3)<span class="sc">$</span>r.squared</span>
<span id="cb64-597"><a href="#cb64-597" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(block4)<span class="sc">$</span>r.squared</span>
<span id="cb64-598"><a href="#cb64-598" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb64-599"><a href="#cb64-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-600"><a href="#cb64-600" aria-hidden="true" tabindex="-1"></a>The first block contains only "previous_scores". This variable serves as our baseline, capturing the academic ability and preparation that each student brings into the current course before any of the other factors come into play. The $R^2$ for this model is 0.0307, meaning that previous academic performance explains only about 3.07% of the variation in current exam scores. This is a notably small number, and it deserves careful interpretation. One might have expected prior scores to be a powerful predictor of current performance - after all, students who did well in the past tend to do well again. And indeed, if we look back at our earlier multiple regression results, the coefficient for "previous_scores" was statistically significant with a t-value of 22.81. But statistical significance and explanatory power are different things. The coefficient for "previous_scores" was 0.048, meaning that a 10-point advantage in prior scores translates to less than half a point on the current exam. The relationship is real but modest in magnitude, and the variable by itself leaves approximately 97% of the variation in exam scores unexplained. This tells us something substantively important: in this dataset, what a student has done before is a relatively weak predictor of what they will achieve now. The current course's exam score is primarily determined by factors other than historical performance.</span>
<span id="cb64-601"><a href="#cb64-601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-602"><a href="#cb64-602" aria-hidden="true" tabindex="-1"></a>In the second block, we add "hours_studied" and "attendance". The $R^2$ jumps dramatically from 0.0307 to 0.5722. The change in $R^2$, $\Delta R^2$ = 0.5722 − 0.0307 = 0.5415, is enormous. Adding study hours and attendance to the model increases the explained variance by 54.15 percentage points. This is by far the largest improvement at any step in our hierarchical analysis, and it fundamentally transforms the model from one that explains virtually nothing to one that explains more than half of the total variation. This result carries a profound substantive message. It tells us that the single most important category of factors for predicting exam scores is not what students were capable of before the course, but what they actively do during it - how many hours they invest in studying and how consistently they show up to class. The effort and engagement variables dwarf baseline ability in their explanatory contribution. For educators and policymakers, this is an optimistic finding: it suggests that student performance is more about current behavior than about fixed, pre-existing ability. Students who attend regularly and study diligently tend to perform well, regardless of their prior academic record. It is worth reflecting on why the $\Delta R^2$ is so much larger than the Block 1 $R^2$. One reason is that "hours_studied" and "attendance" have relatively large coefficients (approximately 0.29 and 0.20 per unit, respectively) and substantial variability across students. Another reason is that these two variables together capture two distinct and complementary dimensions of student effort - in-class engagement and out-of-class preparation - which together account for a broad swath of the academic experience.</span>
<span id="cb64-603"><a href="#cb64-603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-604"><a href="#cb64-604" aria-hidden="true" tabindex="-1"></a>In the third block, we add "tutoring_sessions". The $R^2$ increases from 0.5722 to 0.5967, yielding a $\Delta R^2$ of 0.0246. This means that tutoring sessions explain an additional 2.46 percentage points of variance in exam scores, above and beyond what is already explained by prior scores, study hours, and attendance. While 2.46% may seem small compared to the massive 54.15% gain in Block 2, it is important to interpret this number in context. By Block 3, we have already accounted for the major sources of variation - the "low-hanging fruit" has been picked, so to speak. The remaining unexplained variance at the end of Block 2 was about 42.78% (since 100% − 57.22% = 42.78%). Of this remaining unexplained variance, tutoring sessions account for 2.46 / 42.78 = about 5.75%. So among the factors not yet captured by the model, tutoring makes a meaningful - though not dominant - contribution. This makes theoretical sense: tutoring provides targeted academic support that goes beyond what students gain from attending lectures and studying on their own. It represents an additional layer of help that can address specific gaps in understanding, provide personalized feedback, and reinforce difficult concepts.</span>
<span id="cb64-605"><a href="#cb64-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-606"><a href="#cb64-606" aria-hidden="true" tabindex="-1"></a>In the fourth and final block, we add "sleep_hours" and "physical_activity". The $R^2$ increases from 0.5967 to 0.5982, yielding a $\Delta R^2$ of 0.0015. This is a very small increment - lifestyle factors explain only about 0.15 additional percentage points of variance in exam scores after all the academic variables have been accounted for. This finding tells us that once we know how a student performed previously, how much they study, how often they attend class, and how many tutoring sessions they receive, knowing about their sleep habits and physical exercise patterns adds almost nothing to our ability to predict their exam score. This does not necessarily mean that sleep and exercise are unimportant for well-being or general cognitive function - there is extensive research suggesting they are. But in terms of their incremental contribution to predicting exam scores specifically, over and above the academic and effort variables, their contribution is negligible. It is also worth recalling that in our earlier multiple regression analysis, "sleep_hours" was not statistically significant (p = 0.384), while "physical_activity" was significant but had a small coefficient (0.144). The hierarchical analysis confirms that these lifestyle variables are the least important block in our model.</span>
<span id="cb64-607"><a href="#cb64-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-608"><a href="#cb64-608" aria-hidden="true" tabindex="-1"></a>The ANOVA table provides formal F-tests for the significance of each block's contribution. Let me explain each row. The first row shows Model 1 (Block 1) with 6,605 residual degrees of freedom and an RSS of 96,921. This serves as the starting point. No comparison is made yet because there is no preceding model. The second row compares Model 2 to Model 1. The difference is 2 degrees of freedom (because we added two predictors: "hours_studied" and "attendance"), and the reduction in RSS is 54,143. This is a massive reduction - the unexplained variance was cut by more than half. The F-statistic is 4,447.92 and the p-value is less than $2.2 \times 10{-16}$. This F-statistic is extraordinarily large, providing overwhelming evidence that adding study behavior and engagement variables produces a significant improvement in the model. To understand the F-statistic intuitively, the numerator of the F-test takes the reduction in RSS per added predictor (54,143 / 2 = 27,071.5) and divides it by the residual mean square of the fuller model (approximately 6.09). The ratio of 4,448 means that each of the two new predictors reduced the RSS by roughly 4,448 times more than what a useless random predictor would be expected to contribute. This is exceptionally strong evidence. The third row compares Model 3 to Model 2. Adding Tutoring_Sessions (1 degree of freedom) reduced the RSS by 2,459. The F-statistic is 404.02 with a p-value less than $2.2 \times 10^{-16}$. This is also highly significant, confirming that tutoring sessions provide a meaningful improvement in the model even after study hours and attendance are already included. The F-statistic of 404 is smaller than the 4,448 for Block 2, which makes sense - tutoring contributes less than study behavior, but its contribution is still far beyond what chance alone could produce. The fourth row compares Model 4 to Model 3. Adding "sleep_hours" and "physical_activity" (2 degrees of freedom) reduced the RSS by only 150. The F-statistic is 12.34 with a p-value of $4.484 \times 10^{10-6}$. Although this p-value is well below 0.05 and therefore statistically significant, the magnitude of the improvement is tiny compared to the earlier blocks. The F-statistic of 12.34, while significant, is orders of magnitude smaller than the F-statistics for Blocks 2 and 3. The RSS decreased from 40,320 to 40,169 - a reduction of less than 0.4%. This confirms that lifestyle factors achieve statistical significance (largely thanks to Physical_Activity, as we know Sleep_Hours alone is not significant), but their practical contribution to explaining exam scores is minimal.</span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>