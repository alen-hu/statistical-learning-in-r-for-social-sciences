<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>1&nbsp; Introduction to statistical learning – Statistical Learning in R for Social Sciences</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./index.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-364982630eef5352dd1537128a8ed5cb.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./introduction_to_statistical_learning.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction to statistical learning</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Statistical Learning in R for Social Sciences</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./introduction_to_statistical_learning.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction to statistical learning</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
        
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction to statistical learning</span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><strong>Statistical learning</strong> refers to a broad set of approaches and techniques for estimating the function that connects input variables to an output variable. At its core, statistical learning is concerned with understanding the relationship between variables and using that understanding either to make predictions about future observations or to gain insight into how different factors influence an outcome of interest.</p>
<p>The fundamental idea of statistical learning can be expressed through a simple formula:</p>
<p><span class="math display">\[
Y = f(X) + ϵ
\]</span></p>
<p>This formula tells us that any outcome we wish to study or predict can be understood as the result of some systematic relationship between inputs and outputs, plus some random variation that we cannot fully explain or control. The goal of statistical learning is to estimate the function <span class="math inline">\(f\)</span> based on observed data, so that we can either predict Y for new observations or understand how changes in X are associated with changes in Y.</p>
<p>Let me now explain each component of this formula in detail.</p>
<ul>
<li><p>The dependent variable, denoted by Y, represents the response that we are trying to understand, explain, or predict. This is the variable whose variation we want to account for using other available information, so we refer to Y as the <strong>dependent variable</strong>. It is called “<em>dependent</em>” precisely because its values depend on, or are influenced by, other variables in the system we are studying.</p></li>
<li><p>The <strong>independent variable</strong> or predictor, denoted by X, represents the input information that we use to explain or predict the outcome Y. In most realistic situations, we have multiple predictors rather than just one, so X typically represents a collection of variables written as <span class="math inline">\(X = (X_1, X_2, ..., X_p)\)</span>, where <span class="math inline">\(_p\)</span> indicates the total number of predictors available. The key characteristic of these variables is that they provide information that helps us understand or anticipate the values of the dependent variable.</p></li>
<li><p>The function <span class="math inline">\(f\)</span> represents the <strong>systematic relationship between the dependent variable and the indipendent variable</strong>. This function captures all the information that the input variables collectively provide about the output variable. In other words, <span class="math inline">\(f\)</span> describes the pattern or rule that connects inputs to outputs in a consistent, reproducible way. The crucial point is that in real-world applications, the true form of <span class="math inline">\(f\)</span> is almost always unknown to us. We never directly observe this function; instead, we must estimate it based on the data we have collected. The entire enterprise of statistical learning revolves around developing methods to estimate <span class="math inline">\(f\)</span> as accurately as possible, given the constraints of our data and our analytical goals.</p></li>
<li><p>The <strong>error term</strong>, denoted by <span class="math inline">\(ϵ\)</span>, represents the random component of the relationship between dependent and independent. This term captures all the variation in Y that cannot be explained by the predictors X. The error term is assumed to be independent of X and to have a mean of zero, which means that on average, the errors cancel out and do not systematically bias our predictions in one direction or another. The error term exists for several important reasons.</p>
<ul>
<li><p>First, there may be variables that influence Y but that we have not measured or included in our analysis.</p></li>
<li><p>Second, even if we could measure every relevant variable, there might be inherent randomness or unpredictability in the phenomenon we are studying.</p></li>
<li><p>Third, our measurements themselves may contain some degree of imprecision or noise.</p></li>
</ul></li>
</ul>
<p>To make these concepts concrete, let me illustrate them with the example drawn from sociological research.</p>
<p>Consider a sociologist studying income inequality and social mobility. The researcher might want to understand what determines a person’s income in adulthood. The dependent variable Y would be adult income, measured in monetary units. The predictors X might encompass the person’s own educational credentials, their occupation, the region where they live, their parents’ socioeconomic status, their race and gender, and the number of years of work experience they have accumulated. The function <span class="math inline">\(f\)</span> would capture the systematic relationships between these characteristics and income, revealing how the labor market rewards different attributes and how social background continues to influence economic outcomes across generations. The error term <span class="math inline">\(ϵ\)</span> would account for all the variation in income that these measured factors cannot explain. This residual variation might stem from unmeasured differences in job performance, luck in finding particularly good or bad employment matches, health shocks that affect earning capacity, or discrimination that varies in ways not captured by the measured variables.</p>
<p>We can write this relationship as:</p>
<p><span class="math display">\[
Y\ =\ f(X_1,\ X_2,\ X_3,\ X_4,\ X_5,\ X_6,\ X_7)\ +\ ϵ
\]</span></p>
<p><span class="math display">\[
adult\ income = f(educational\ credentials,\ occupation,\ geographic\ region,\ parents'\ socioeconomic\ status,\ race,\ gender,\ gender,\ years\ of\ work\ experience) + ϵ
\]</span></p>
<p>In this formula, Y represents adult income measured in monetary units such as annual earnings in dollars or euros. This is the outcome we are trying to understand and potentially predict. The predictors are defined as follows. <span class="math inline">\(X_1\)</span> represents the person’s educational credentials, which might be measured as years of schooling completed or as the highest degree obtained. <span class="math inline">\(X_2\)</span> represents occupation, which could be coded as occupational prestige scores or as categorical indicators for different types of jobs. <span class="math inline">\(X_3\)</span> represents the geographic region where the person lives and works, capturing spatial variation in labor markets and cost of living. <span class="math inline">\(X_4\)</span> represents parents’ socioeconomic status, which might be measured through parental income, parental education, or a composite index combining multiple indicators of family background. <span class="math inline">\(X_5\)</span> represents race, coded as categorical indicators for different racial or ethnic groups. <span class="math inline">\(X_6\)</span> represents gender, typically coded as a binary or categorical variable. <span class="math inline">\(X_7\)</span> represents years of work experience, measuring how long the person has been participating in the labor force. The function <span class="math inline">\(f\)</span> captures the systematic relationship between all these predictors and adult income. This function describes how the labor market values different combinations of education, occupation, location, background, and demographic characteristics. The precise form of <span class="math inline">\(f\)</span> is unknown to us and must be estimated from data. It might be relatively simple, such as a linear combination of the predictors, or it might be quite complex, involving interactions between variables and nonlinear relationships. The error term <span class="math inline">\(ϵ\)</span> represents all the variation in adult income that cannot be explained by the seven predictors we have included. This encompasses unmeasured factors such as individual differences in productivity, motivation, and interpersonal skills, as well as random events like fortunate or unfortunate timing in job searches, health events that affect earning capacity, and idiosyncratic experiences of discrimination or favoritism in the workplace.</p>
<p>The function <span class="math inline">\(f\)</span> is the central object of interest in statistical learning. It represents the systematic relationship between the independent variable X and the dependent variable Y, capturing all the information that the independent variables provide about the dependent variable. When we say that <span class="math inline">\(Y = f(X) + ϵ\)</span>, we are asserting that the outcome Y can be decomposed into two parts: a predictable component <span class="math inline">\(f(X)\)</span> that depends on the values of the predictors, and an unpredictable component <span class="math inline">\(ϵ\)</span> that represents random variation. The function <span class="math inline">\(f\)</span> is what connects the world of inputs to the world of predictors in a consistent, reproducible manner.</p>
<p>Understanding the nature of <span class="math inline">\(f\)</span> is crucial because it embodies the underlying pattern that governs how changes in the independent variable translate into changes in the dependent variable. If we knew <span class="math inline">\(f\)</span> perfectly, we would understand exactly how each predictor influences the response, how predictors interact with one another, and what outcome to expect for any given combination of input values. However, in virtually all real-world applications, <span class="math inline">\(f\)</span> is unknown. We never observe <span class="math inline">\(f\)</span> directly; we only observe data points consisting of predictor values and corresponding outcomes. The entire purpose of statistical learning is to use these observed data points to construct an estimate of <span class="math inline">\(f\)</span>, which we denote as <span class="math inline">\(f̂\)</span> (read as “f-hat”). This estimate allows us to either make predictions about future outcomes or draw inferences about the relationships between variables.</p>
<p>The reasons we might want to estimate <span class="math inline">\(f\)</span> fall into two broad categories: prediction and inference. These two goals are conceptually distinct, and they often lead us to prefer different types of statistical learning methods.</p>
<p><strong>Prediction</strong> is concerned with accurately anticipating the value of Y for new observations where we know the predictors X but do not yet know the outcome. In prediction tasks, we treat <span class="math inline">\(f̂\)</span> as a kind of black box. We do not necessarily care about the internal workings of our estimated function or about which specific predictors matter most. What we care about is whether our estimate <span class="math inline">\(f̂\)</span> produces accurate predictions when applied to new data. The quality of our predictions depends on two sources of error. The first is <strong>reducible error</strong>, which arises because our estimate <span class="math inline">\(f̂\)</span> is imperfect and does not exactly match the true f.&nbsp;We can potentially reduce this error by using better statistical learning methods or by collecting more data. The second is <strong>irreducible error</strong>, which corresponds to the variance of <span class="math inline">\(ϵ\)</span>. Even if we had a perfect estimate of <span class="math inline">\(f\)</span>, our predictions would still contain some error because Y is inherently influenced by random factors that cannot be predicted from X alone.</p>
<p>To illustrate prediction using our income example, imagine that a government agency wants to identify individuals who are at risk of falling into poverty so that it can target social assistance programs more effectively. The agency has access to administrative data containing information about people’s education, occupation, geographic location, family background, race, gender, and work experience. The goal is to predict each person’s income based on these characteristics. In this context, the agency does not need to understand precisely why certain combinations of predictors lead to low income. What matters is that the predictive model accurately identifies individuals whose incomes are likely to fall below some threshold. The function <span class="math inline">\(f̂\)</span> serves as a tool for sorting people into risk categories, and its value is judged entirely by how well it predicts actual incomes.</p>
<p><strong>Inference</strong>, by contrast, is concerned with understanding the relationship between the predictors and the outcome. When our goal is inference, we cannot treat <span class="math inline">\(f̂\)</span> as a black box because we need to know its exact form. We want to answer questions such as which predictors are associated with the response, what is the direction and magnitude of each predictor’s effect, and whether the relationships are linear or more complex. Inference requires that our estimate <span class="math inline">\(f̂\)</span> be interpretable, meaning that we can examine it and draw substantive conclusions about how the world works.</p>
<p>Returning to the income example, suppose a sociologist wants to understand the mechanisms of income inequality. The researcher might ask questions such as: <em>How much does an additional year of education increase expected income? Does the effect of education differ by race or gender? How large is the income penalty associated with being female, after controlling for education, occupation, and experience?</em> These are inferential questions because they seek to illuminate the structure of <span class="math inline">\(f\)</span> itself, not merely to use <span class="math inline">\(f\)</span> for prediction. Answering these questions requires a model that allows the researcher to isolate the contribution of each predictor and to interpret coefficients or other parameters in substantively meaningful ways. A model that predicts income very accurately but does not reveal anything about how individual predictors matter would be useless for this purpose.</p>
<p>In practice, many research projects involve elements of both prediction and inference. A sociologist studying income might want to understand the determinants of earnings while also developing a model that can predict incomes for new individuals. However, there is often tension between these goals because the methods that produce the most accurate predictions are not always the most interpretable, and the most interpretable methods do not always produce the best predictions.</p>
<p>Having established why we want to estimate <span class="math inline">\(f\)</span>, let me now turn to the question of how we estimate <span class="math inline">\(f\)</span>. Statistical learning methods for estimating <span class="math inline">\(f\)</span> can be broadly divided into two categories: parametric methods and non-parametric methods. These two approaches differ fundamentally in the assumptions they make about the form of <span class="math inline">\(f\)</span> and in the way they use data to construct an estimate.</p>
<p>Parametric methods proceed in two steps. In the first step, we make an assumption about the functional form of <span class="math inline">\(f\)</span>. That is, we specify in advance what kind of mathematical relationship we believe connects the predictors to the outcome. The most common assumption is that <span class="math inline">\(f\)</span> is linear, meaning that we assume the relationship can be written as <span class="math inline">\(f(X) = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p\)</span>. This linear model asserts that the outcome is a weighted sum of the predictors, where the weights <span class="math inline">\(\beta_1, \beta_2, ..., \beta_p\)</span> are unknown coefficients that quantify the contribution of each predictor, and <span class="math inline">\(\beta_0\)</span> is an intercept term representing the expected outcome when all predictors equal zero. By assuming a linear form, we have dramatically simplified the problem. Instead of having to estimate an arbitrary, potentially very complex function <span class="math inline">\(f\)</span>, we only need to estimate the intercept and the p coefficients. In the second step of the parametric approach, we use the observed data to fit or train the model. This means finding values of the parameters that make the model match the data as closely as possible. For the linear model, the most common fitting procedure is ordinary least squares, which chooses the parameter values that minimize the sum of squared differences between the observed outcomes and the outcomes predicted by the model. Once we have estimated the parameters, our estimate <span class="math inline">\(f̂\)</span> is fully specified, and we can use it for prediction or inference.</p>
<p>The parametric approach has several important advantages. Because we have reduced the problem to estimating a fixed number of parameters, parametric methods are computationally efficient and can work well even with relatively small samples. Furthermore, the resulting models are typically easy to interpret. In a linear model, each coefficient tells us how much the expected outcome changes when the corresponding predictor increases by one unit, holding all other predictors constant. This interpretability makes parametric models particularly valuable for inference. However, parametric methods also have a significant limitation. The assumption we make about the form of <span class="math inline">\(f\)</span> may be wrong. If the true relationship between the predictors and the outcome is nonlinear or involves complex interactions, a linear model will fail to capture these features and will provide a poor approximation to <span class="math inline">\(f\)</span>. We can try to address this problem by using more flexible parametric models that include polynomial terms, interaction effects, or other elaborations of the basic linear form. But as we make our parametric model more flexible, we must estimate more parameters, which requires more data and increases the risk of a phenomenon called overfitting. Overfitting occurs when a model fits the training data very well but performs poorly on new data because it has captured random noise rather than genuine patterns. The model essentially memorizes the idiosyncrasies of the particular sample rather than learning the underlying relationship.</p>
<p><strong>Non-parametric methods</strong> take a fundamentally different approach. Instead of assuming a specific functional form for <span class="math inline">\(f\)</span>, non-parametric methods seek an estimate that gets close to the data points without imposing strong prior assumptions about the shape of the relationship. The idea is to let the data speak for themselves and to allow <span class="math inline">\(f̂\)</span> to take whatever form best fits the observed patterns. One example of a non-parametric method is the thin-plate spline, which estimates <span class="math inline">\(f\)</span> as a smooth surface that passes near the observed data points. The analyst does not specify in advance that <span class="math inline">\(f\)</span> should be linear or quadratic or any other particular form. Instead, the method finds a smooth function that fits the data well, subject to some constraint on how wiggly or rough the function is allowed to be. Another example is the k-nearest neighbors method, which predicts the outcome for a new observation by averaging the outcomes of the k training observations that are most similar to it in terms of the predictor values.</p>
<p>The main advantage of non-parametric methods is their flexibility. Because they do not assume a particular form for <span class="math inline">\(f\)</span>, they can potentially capture a much wider range of relationships, including highly nonlinear patterns and complex interactions that would be missed by a simple parametric model. If the true <span class="math inline">\(f\)</span> has an unusual or complicated shape, a non-parametric method has a better chance of approximating it accurately. However, non-parametric methods also have important disadvantages. Because they do not reduce the problem to estimating a small number of parameters, they typically require much larger samples to produce accurate estimates. The flexibility that allows non-parametric methods to fit complex patterns also makes them prone to overfitting, especially when sample sizes are limited. Furthermore, the estimates produced by non-parametric methods are often difficult to interpret. A thin-plate spline or a k-nearest neighbors prediction does not come with coefficients that tell us how each predictor contributes to the outcome. This lack of interpretability makes non-parametric methods less useful for inference, even when they excel at prediction.</p>
<p>The choice between parametric and non-parametric methods involves a fundamental trade-off. Parametric methods impose structure on the problem, which makes estimation easier and results more interpretable, but at the cost of potentially misspecifying the true form of <span class="math inline">\(f\)</span>. Non-parametric methods avoid this misspecification risk by staying flexible, but they require more data and produce less interpretable results. In practice, the best choice depends on the goals of the analysis, the amount of data available, and how much prior knowledge we have about the likely form of the relationship.</p>
<p>This brings us to a closely related issue: the trade-off between prediction accuracy and model interpretability. In statistical learning, there is often an inverse relationship between how flexible a method is and how interpretable its results are. Methods that impose strong restrictions on the form of <span class="math inline">\(f\)</span> tend to be highly interpretable but may not fit complex patterns very well. Methods that are highly flexible can capture intricate relationships but produce results that are difficult for humans to understand.</p>
<p>At one end of the spectrum, we have highly restrictive methods like linear regression and its close relatives. Linear regression assumes that <span class="math inline">\(f\)</span> is a linear combination of the predictors, which is a very strong restriction. This restriction means that linear regression can only produce straight lines in one dimension, flat planes in two dimensions, and hyperplanes in higher dimensions. The advantage is that the results are extremely interpretable. Each coefficient has a clear meaning: it tells us the expected change in Y associated with a one-unit increase in the corresponding predictor, holding other predictors constant. We can examine the coefficients and immediately understand which predictors matter, how large their effects are, and in which direction they operate. For inference purposes, this interpretability is invaluable.</p>
<p>Moving along the spectrum toward greater flexibility, we encounter methods like generalized additive models, which relax the linearity assumption by allowing each predictor to have a potentially nonlinear effect on the outcome, while still maintaining an additive structure. These models are more flexible than linear regression and can capture curved relationships, but they remain reasonably interpretable because we can plot and examine the estimated effect of each predictor separately. Further along the spectrum, we find decision trees, which partition the predictor space into regions and assign a predicted outcome to each region. Trees are moderately flexible and can capture interactions and nonlinearities, but they remain somewhat interpretable because we can visualize the tree structure and see which predictors are used to make splits and at what values. At the far end of the spectrum, we have highly flexible methods such as bagging, boosting, support vector machines with nonlinear kernels, and deep neural networks. These methods can approximate extremely complex functions and often achieve superior predictive accuracy on difficult problems. However, their results are very hard to interpret. A neural network with thousands of parameters does not yield simple statements about how each predictor influences the outcome. The model operates as a black box: we can feed in predictors and obtain predictions, but we cannot easily understand why the model makes the predictions it does.</p>
<p>One might think that we should always prefer the most flexible method available, reasoning that greater flexibility means better ability to capture the true <span class="math inline">\(f\)</span>. Surprisingly, this is not the case. More flexible methods are not always better, even when our sole goal is prediction. The reason is overfitting. A highly flexible method can fit the training data very closely, including the random noise in that particular sample. When we apply the model to new data, the noise patterns will be different, and the overfitted model will perform poorly. In many situations, a simpler, more restrictive model that does not fit the training data as closely will actually generalize better to new observations.</p>
<p>This phenomenon is especially important when sample sizes are limited. With a small sample, there is not enough information to reliably estimate a complex, flexible model, and the risk of overfitting is high. In such cases, imposing structure through a parametric model can actually improve predictive performance by preventing the model from chasing noise. As sample sizes grow larger, we can afford to use more flexible methods because there is enough information to distinguish genuine patterns from random variation.</p>
<p>The choice of method therefore depends on our goals and our circumstances. If our primary goal is inference, we typically prefer more restrictive, interpretable methods like linear regression, even if they sacrifice some predictive accuracy. The interpretability allows us to draw substantive conclusions about how the world works. If our primary goal is prediction and we have ample data, we might prefer more flexible methods that can capture complex patterns, accepting that we will not be able to easily interpret the results. If we want prediction but have limited data, we might still prefer restrictive methods to avoid overfitting.</p>
<p>Let me now bring together these various dimensions by summarizing how different statistical learning methods can be characterized according to their suitability for prediction versus inference, their parametric versus non-parametric nature, and their position on the interpretability-flexibility spectrum.</p>
<p><strong>Linear regression</strong> and its extensions such as ridge regression and the lasso are parametric methods that assume a linear relationship between predictors and outcome. They occupy the high-interpretability, low-flexibility end of the spectrum. Each predictor has a coefficient that directly quantifies its contribution to the outcome, making these methods ideal for inference. Their predictive accuracy is good when the true relationship is approximately linear, but they will miss nonlinear patterns. Because they estimate relatively few parameters, they work well with moderate sample sizes and are resistant to overfitting.</p>
<p><strong>Logistic regression</strong> is the classification analog of linear regression. It is a parametric method that models the probability of belonging to a particular class as a function of the predictors. Like linear regression, it is highly interpretable because the coefficients can be transformed into odds ratios that describe how each predictor affects the odds of the outcome. It is well-suited for inference in classification problems but assumes a particular functional form that may not hold in all situations.</p>
<p><strong>Generalized additive models</strong> are parametric in the sense that they assume an additive structure, but they allow nonlinear effects for individual predictors. They occupy a middle position on the flexibility-interpretability spectrum. The additive structure maintains considerable interpretability because we can examine the effect of each predictor separately, but the nonlinear components provide more flexibility than standard linear regression. These models are useful when we suspect that relationships are nonlinear but still want to understand how each predictor contributes to the outcome.</p>
<p><strong>Decision trees</strong> are non-parametric methods that recursively partition the predictor space. They are moderately flexible and can capture interactions and nonlinearities. Their interpretability is moderate: small trees are easy to understand and visualize, but large, complex trees become difficult to interpret. Trees are prone to overfitting, especially when grown deep, but they provide a foundation for more sophisticated ensemble methods.</p>
<p><strong>Bagging</strong> and <strong>random forests</strong> are ensemble methods that combine many decision trees to improve predictive accuracy and reduce overfitting. They are non-parametric and highly flexible, capable of capturing complex patterns in the data. However, by aggregating many trees, they sacrifice the interpretability of individual trees. These methods are primarily useful for prediction rather than inference, particularly when sample sizes are large enough to support their flexibility.</p>
<p><strong>Boosting methods</strong> build ensembles of weak learners, typically small decision trees, in a sequential manner that focuses on observations that previous models predicted poorly. Like bagging and random forests, boosting methods are highly flexible and achieve excellent predictive performance on many problems. They are non-parametric and operate toward the low-interpretability end of the spectrum, making them suitable for prediction but less useful for inference.</p>
<p><strong>Support vector machines</strong> are methods that find optimal boundaries between classes in classification problems or fit regression functions in a way that ignores errors smaller than some threshold. With linear kernels, they are similar to linear methods and retain interpretability. With nonlinear kernels, they become highly flexible and can capture complex decision boundaries, but they lose interpretability. The flexibility of support vector machines can be tuned by choosing different kernels and adjusting regularization parameters.</p>
<p><strong>Neural networks</strong> and <strong>deep learning</strong> represent the extreme of flexibility. These methods can approximate virtually any function given enough data and computational resources. They have achieved remarkable success in applications such as image recognition, natural language processing, and game playing. However, their complexity makes them essentially uninterpretable. A deep neural network with millions of parameters cannot be summarized in a way that humans can understand. These methods are purely predictive tools, unsuitable for inference, and they require very large datasets to train effectively without overfitting.</p>
<p><strong>K-nearest neighbors</strong> is a simple non-parametric method that makes predictions by averaging the outcomes of similar observations in the training data. It is highly flexible and makes no assumptions about the form of f.&nbsp;However, it is not very interpretable because it does not produce coefficients or other summaries of how predictors relate to outcomes. Its predictive accuracy depends strongly on the choice of k and on having a sufficiently large and representative training sample.</p>
<p>In summary, the landscape of statistical learning methods can be understood through several interconnected dimensions. The parametric versus non-parametric distinction concerns whether a method assumes a specific functional form for <span class="math inline">\(f\)</span> or lets the data determine the shape of the relationship. The interpretability versus flexibility trade-off reflects the tension between methods that produce easily understood results and methods that can capture complex patterns. The prediction versus inference distinction concerns whether our goal is to forecast future outcomes accurately or to understand the mechanisms connecting predictors to responses. These dimensions are interrelated: parametric methods tend to be more interpretable and better suited for inference, while non-parametric methods tend to be more flexible and better suited for prediction when interpretability is not required. The choice among methods depends on the specific goals of an analysis, the nature of the data, the sample size available, and the analyst’s prior beliefs about the likely form of the relationship being studied.</p>
<p>To complete our overview of the foundational concepts in statistical learning, we need to understand additional distinction between supervised and unsupervised learning that help us categorize different types of learning problems.</p>
<p><strong>Supervised learning</strong> refers to situations where for each observation in our dataset, we have both predictor measurements and a corresponding outcome measurement. The term “<em>supervised</em>” reflects the idea that the learning process is guided or supervised by the known outcomes. We observe what actually happened for each case in our training data, and we use this information to learn the relationship between predictors and outcomes. All the methods we have discussed so far, including linear regression, logistic regression, decision trees, random forests, and neural networks, fall into the supervised learning category when applied to problems where outcomes are observed. The fundamental goal of supervised learning is to build a model that can predict the outcome for new observations based on their predictor values, or to understand how the predictors relate to the outcome. In sociological research, most studies involve supervised learning because we typically have data on both the explanatory variables and the outcome of interest. For example, when we study the relationship between education and income, we observe both variables for the individuals in our sample, which allows us to estimate how education influences earnings.</p>
<p><strong>Unsupervised learning</strong> describes a fundamentally different situation where we observe predictor measurements for each observation but have no corresponding outcome variable. Without an outcome to predict or explain, we cannot fit a regression model or train a classifier. Instead, unsupervised learning seeks to discover patterns, structures, or groupings within the data itself. The most common unsupervised learning task is cluster analysis, which attempts to identify subgroups of observations that are similar to one another. For instance, a sociologist might have survey data containing many variables about people’s attitudes, behaviors, and demographic characteristics, but no predefined categorization of people into types. Cluster analysis could reveal that the respondents naturally fall into distinct groups based on their patterns of responses, perhaps identifying clusters that correspond to different lifestyles, political orientations, or consumption patterns. The key feature of unsupervised learning is that there is no “correct answer” to supervise the learning process. We are not trying to predict a known outcome but rather to uncover hidden structure in the data. This makes unsupervised learning more exploratory and somewhat more subjective than supervised learning, since there is no objective criterion like prediction accuracy to evaluate whether we have found the right structure.</p>
<table class="caption-top table">
<caption><strong>Table 1.1</strong> Summary table of statistical learning methods</caption>
<colgroup>
<col style="width: 5%">
<col style="width: 20%">
<col style="width: 9%">
<col style="width: 13%">
<col style="width: 12%">
<col style="width: 20%">
<col style="width: 18%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Unsupervised / Supervised</th>
<th>Parametric / Non-parametric</th>
<th>Flexibility</th>
<th>Interpretability</th>
<th>Best Suited For</th>
<th>Regression / Classification</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Linear Regression</td>
<td>Supervised</td>
<td>Parametric</td>
<td>Low</td>
<td>High</td>
<td>Inference</td>
<td>Regression</td>
</tr>
<tr class="even">
<td>Ridge Regression</td>
<td>Supervised</td>
<td>Parametric</td>
<td>Low</td>
<td>High</td>
<td>Inference &amp; Prediction</td>
<td>Regression</td>
</tr>
<tr class="odd">
<td>Lasso</td>
<td>Supervised</td>
<td>Parametric</td>
<td>Low</td>
<td>High</td>
<td>Inference &amp; Prediction</td>
<td>Regression</td>
</tr>
<tr class="even">
<td>Logistic Regression</td>
<td>Supervised</td>
<td>Parametric</td>
<td>Low</td>
<td>High</td>
<td>Inference</td>
<td>Classification</td>
</tr>
<tr class="odd">
<td>Generalized Additive Models (GAMs)</td>
<td>Supervised</td>
<td>Parametric (additive structure)</td>
<td>Medium</td>
<td>Medium-High</td>
<td>Inference &amp; Prediction</td>
<td>Both</td>
</tr>
<tr class="even">
<td>Decision Trees</td>
<td>Supervised</td>
<td>Non-parametric</td>
<td>Medium</td>
<td>Medium</td>
<td>Inference &amp; Prediction</td>
<td>Both</td>
</tr>
<tr class="odd">
<td>Bagging</td>
<td>Supervised</td>
<td>Non-parametric</td>
<td>High</td>
<td>Low</td>
<td>Prediction</td>
<td>Both</td>
</tr>
<tr class="even">
<td>Random Forests</td>
<td>Supervised</td>
<td>Non-parametric</td>
<td>High</td>
<td>Low</td>
<td>Prediction</td>
<td>Both</td>
</tr>
<tr class="odd">
<td>Boosting</td>
<td>Supervised</td>
<td>Non-parametric</td>
<td>High</td>
<td>Low</td>
<td>Prediction</td>
<td>Both</td>
</tr>
<tr class="even">
<td>Support Vector Machines (Linear Kernel)</td>
<td>Supervised</td>
<td>Parametric</td>
<td>Low-Medium</td>
<td>Medium</td>
<td>Prediction &amp; Inference</td>
<td>Both</td>
</tr>
<tr class="odd">
<td>Support Vector Machines (Nonlinear Kernel)</td>
<td>Supervised</td>
<td>Non-parametric</td>
<td>High</td>
<td>Low</td>
<td>Prediction</td>
<td>Both</td>
</tr>
<tr class="even">
<td>K-Nearest Neighbors</td>
<td>Supervised</td>
<td>Non-parametric</td>
<td>High</td>
<td>Low</td>
<td>Prediction</td>
<td>Both</td>
</tr>
<tr class="odd">
<td>Neural Networks / Deep Learning</td>
<td>Supervised</td>
<td>Non-parametric</td>
<td>Very High</td>
<td>Very Low</td>
<td>Prediction</td>
<td>Both</td>
</tr>
<tr class="even">
<td>K-Means Clustering</td>
<td>Unsupervised</td>
<td>Non-parametric</td>
<td>Medium</td>
<td>Medium</td>
<td>Discovering groups in data</td>
<td>Moderate to Large</td>
</tr>
<tr class="odd">
<td>Hierarchical Clustering</td>
<td>Unsupervised</td>
<td>Non-parametric</td>
<td>Medium</td>
<td>Medium-High</td>
<td>Discovering nested group structures</td>
<td>Moderate</td>
</tr>
<tr class="even">
<td>Principal Component Analysis (PCA)</td>
<td>Unsupervised</td>
<td>Parametric</td>
<td>Low</td>
<td>Medium-High</td>
<td>Dimensionality reduction</td>
<td>Moderate</td>
</tr>
<tr class="odd">
<td>Factor Analysis</td>
<td>Unsupervised</td>
<td>Parametric</td>
<td>Low</td>
<td>High</td>
<td>Identifying latent constructs</td>
<td>Moderate</td>
</tr>
</tbody>
</table>


<!-- -->


</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./index.html" class="pagination-link" aria-label="Preface">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Preface</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Introduction to statistical learning"</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>**Statistical learning** refers to a broad set of approaches and techniques for estimating the function that connects input variables to an output variable. At its core, statistical learning is concerned with understanding the relationship between variables and using that understanding either to make predictions about future observations or to gain insight into how different factors influence an outcome of interest.</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>The fundamental idea of statistical learning can be expressed through a simple formula:</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>Y = f(X) + ϵ</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>This formula tells us that any outcome we wish to study or predict can be understood as the result of some systematic relationship between inputs and outputs, plus some random variation that we cannot fully explain or control. The goal of statistical learning is to estimate the function $f$ based on observed data, so that we can either predict Y for new observations or understand how changes in X are associated with changes in Y.</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>Let me now explain each component of this formula in detail.</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The dependent variable, denoted by Y, represents the response that we are trying to understand, explain, or predict. This is the variable whose variation we want to account for using other available information, so we refer to Y as the **dependent variable**. It is called "*dependent*" precisely because its values depend on, or are influenced by, other variables in the system we are studying.</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The **independent variable** or predictor, denoted by X, represents the input information that we use to explain or predict the outcome Y. In most realistic situations, we have multiple predictors rather than just one, so X typically represents a collection of variables written as $X = (X_1, X_2, ..., X_p)$, where $_p$ indicates the total number of predictors available. The key characteristic of these variables is that they provide information that helps us understand or anticipate the values of the dependent variable.</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The function $f$ represents the **systematic relationship between the dependent variable and the indipendent variable**. This function captures all the information that the input variables collectively provide about the output variable. In other words, $f$ describes the pattern or rule that connects inputs to outputs in a consistent, reproducible way. The crucial point is that in real-world applications, the true form of $f$ is almost always unknown to us. We never directly observe this function; instead, we must estimate it based on the data we have collected. The entire enterprise of statistical learning revolves around developing methods to estimate $f$ as accurately as possible, given the constraints of our data and our analytical goals.</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The **error term**, denoted by $ϵ$, represents the random component of the relationship between dependent and independent. This term captures all the variation in Y that cannot be explained by the predictors X. The error term is assumed to be independent of X and to have a mean of zero, which means that on average, the errors cancel out and do not systematically bias our predictions in one direction or another. The error term exists for several important reasons.</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>First, there may be variables that influence Y but that we have not measured or included in our analysis.</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Second, even if we could measure every relevant variable, there might be inherent randomness or unpredictability in the phenomenon we are studying.</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Third, our measurements themselves may contain some degree of imprecision or noise.</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>To make these concepts concrete, let me illustrate them with the example drawn from sociological research.</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>Consider a sociologist studying income inequality and social mobility. The researcher might want to understand what determines a person's income in adulthood. The dependent variable Y would be adult income, measured in monetary units. The predictors X might encompass the person's own educational credentials, their occupation, the region where they live, their parents' socioeconomic status, their race and gender, and the number of years of work experience they have accumulated. The function $f$ would capture the systematic relationships between these characteristics and income, revealing how the labor market rewards different attributes and how social background continues to influence economic outcomes across generations. The error term $ϵ$ would account for all the variation in income that these measured factors cannot explain. This residual variation might stem from unmeasured differences in job performance, luck in finding particularly good or bad employment matches, health shocks that affect earning capacity, or discrimination that varies in ways not captured by the measured variables.</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>We can write this relationship as:</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>Y\ =\ f(X_1,\ X_2,\ X_3,\ X_4,\ X_5,\ X_6,\ X_7)\ +\ ϵ</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>adult\ income = f(educational\ credentials,\ occupation,\ geographic\ region,\ parents'\ socioeconomic\ status,\ race,\ gender,\ gender,\ years\ of\ work\ experience) + ϵ</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>In this formula, Y represents adult income measured in monetary units such as annual earnings in dollars or euros. This is the outcome we are trying to understand and potentially predict. The predictors are defined as follows. $X_1$ represents the person's educational credentials, which might be measured as years of schooling completed or as the highest degree obtained. $X_2$ represents occupation, which could be coded as occupational prestige scores or as categorical indicators for different types of jobs. $X_3$ represents the geographic region where the person lives and works, capturing spatial variation in labor markets and cost of living. $X_4$ represents parents' socioeconomic status, which might be measured through parental income, parental education, or a composite index combining multiple indicators of family background. $X_5$ represents race, coded as categorical indicators for different racial or ethnic groups. $X_6$ represents gender, typically coded as a binary or categorical variable. $X_7$ represents years of work experience, measuring how long the person has been participating in the labor force. The function $f$ captures the systematic relationship between all these predictors and adult income. This function describes how the labor market values different combinations of education, occupation, location, background, and demographic characteristics. The precise form of $f$ is unknown to us and must be estimated from data. It might be relatively simple, such as a linear combination of the predictors, or it might be quite complex, involving interactions between variables and nonlinear relationships. The error term $ϵ$ represents all the variation in adult income that cannot be explained by the seven predictors we have included. This encompasses unmeasured factors such as individual differences in productivity, motivation, and interpersonal skills, as well as random events like fortunate or unfortunate timing in job searches, health events that affect earning capacity, and idiosyncratic experiences of discrimination or favoritism in the workplace.</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>The function $f$ is the central object of interest in statistical learning. It represents the systematic relationship between the independent variable X and the dependent variable Y, capturing all the information that the independent variables provide about the dependent variable. When we say that $Y = f(X) + ϵ$, we are asserting that the outcome Y can be decomposed into two parts: a predictable component $f(X)$ that depends on the values of the predictors, and an unpredictable component $ϵ$ that represents random variation. The function $f$ is what connects the world of inputs to the world of predictors in a consistent, reproducible manner.</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>Understanding the nature of $f$ is crucial because it embodies the underlying pattern that governs how changes in the independent variable translate into changes in the dependent variable. If we knew $f$ perfectly, we would understand exactly how each predictor influences the response, how predictors interact with one another, and what outcome to expect for any given combination of input values. However, in virtually all real-world applications, $f$ is unknown. We never observe $f$ directly; we only observe data points consisting of predictor values and corresponding outcomes. The entire purpose of statistical learning is to use these observed data points to construct an estimate of $f$, which we denote as $f̂$ (read as "f-hat"). This estimate allows us to either make predictions about future outcomes or draw inferences about the relationships between variables.</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>The reasons we might want to estimate $f$ fall into two broad categories: prediction and inference. These two goals are conceptually distinct, and they often lead us to prefer different types of statistical learning methods.</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>**Prediction** is concerned with accurately anticipating the value of Y for new observations where we know the predictors X but do not yet know the outcome. In prediction tasks, we treat $f̂$ as a kind of black box. We do not necessarily care about the internal workings of our estimated function or about which specific predictors matter most. What we care about is whether our estimate $f̂$ produces accurate predictions when applied to new data. The quality of our predictions depends on two sources of error. The first is **reducible error**, which arises because our estimate $f̂$ is imperfect and does not exactly match the true f. We can potentially reduce this error by using better statistical learning methods or by collecting more data. The second is **irreducible error**, which corresponds to the variance of $ϵ$. Even if we had a perfect estimate of $f$, our predictions would still contain some error because Y is inherently influenced by random factors that cannot be predicted from X alone.</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>To illustrate prediction using our income example, imagine that a government agency wants to identify individuals who are at risk of falling into poverty so that it can target social assistance programs more effectively. The agency has access to administrative data containing information about people's education, occupation, geographic location, family background, race, gender, and work experience. The goal is to predict each person's income based on these characteristics. In this context, the agency does not need to understand precisely why certain combinations of predictors lead to low income. What matters is that the predictive model accurately identifies individuals whose incomes are likely to fall below some threshold. The function $f̂$ serves as a tool for sorting people into risk categories, and its value is judged entirely by how well it predicts actual incomes.</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>**Inference**, by contrast, is concerned with understanding the relationship between the predictors and the outcome. When our goal is inference, we cannot treat $f̂$ as a black box because we need to know its exact form. We want to answer questions such as which predictors are associated with the response, what is the direction and magnitude of each predictor's effect, and whether the relationships are linear or more complex. Inference requires that our estimate $f̂$ be interpretable, meaning that we can examine it and draw substantive conclusions about how the world works.</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>Returning to the income example, suppose a sociologist wants to understand the mechanisms of income inequality. The researcher might ask questions such as: *How much does an additional year of education increase expected income? Does the effect of education differ by race or gender? How large is the income penalty associated with being female, after controlling for education, occupation, and experience?* These are inferential questions because they seek to illuminate the structure of $f$ itself, not merely to use $f$ for prediction. Answering these questions requires a model that allows the researcher to isolate the contribution of each predictor and to interpret coefficients or other parameters in substantively meaningful ways. A model that predicts income very accurately but does not reveal anything about how individual predictors matter would be useless for this purpose.</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>In practice, many research projects involve elements of both prediction and inference. A sociologist studying income might want to understand the determinants of earnings while also developing a model that can predict incomes for new individuals. However, there is often tension between these goals because the methods that produce the most accurate predictions are not always the most interpretable, and the most interpretable methods do not always produce the best predictions.</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>Having established why we want to estimate $f$, let me now turn to the question of how we estimate $f$. Statistical learning methods for estimating $f$ can be broadly divided into two categories: parametric methods and non-parametric methods. These two approaches differ fundamentally in the assumptions they make about the form of $f$ and in the way they use data to construct an estimate.</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>Parametric methods proceed in two steps. In the first step, we make an assumption about the functional form of $f$. That is, we specify in advance what kind of mathematical relationship we believe connects the predictors to the outcome. The most common assumption is that $f$ is linear, meaning that we assume the relationship can be written as $f(X) = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p$. This linear model asserts that the outcome is a weighted sum of the predictors, where the weights $\beta_1, \beta_2, ..., \beta_p$ are unknown coefficients that quantify the contribution of each predictor, and $\beta_0$ is an intercept term representing the expected outcome when all predictors equal zero. By assuming a linear form, we have dramatically simplified the problem. Instead of having to estimate an arbitrary, potentially very complex function $f$, we only need to estimate the intercept and the p coefficients. In the second step of the parametric approach, we use the observed data to fit or train the model. This means finding values of the parameters that make the model match the data as closely as possible. For the linear model, the most common fitting procedure is ordinary least squares, which chooses the parameter values that minimize the sum of squared differences between the observed outcomes and the outcomes predicted by the model. Once we have estimated the parameters, our estimate $f̂$ is fully specified, and we can use it for prediction or inference.</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>The parametric approach has several important advantages. Because we have reduced the problem to estimating a fixed number of parameters, parametric methods are computationally efficient and can work well even with relatively small samples. Furthermore, the resulting models are typically easy to interpret. In a linear model, each coefficient tells us how much the expected outcome changes when the corresponding predictor increases by one unit, holding all other predictors constant. This interpretability makes parametric models particularly valuable for inference. However, parametric methods also have a significant limitation. The assumption we make about the form of $f$ may be wrong. If the true relationship between the predictors and the outcome is nonlinear or involves complex interactions, a linear model will fail to capture these features and will provide a poor approximation to $f$. We can try to address this problem by using more flexible parametric models that include polynomial terms, interaction effects, or other elaborations of the basic linear form. But as we make our parametric model more flexible, we must estimate more parameters, which requires more data and increases the risk of a phenomenon called overfitting. Overfitting occurs when a model fits the training data very well but performs poorly on new data because it has captured random noise rather than genuine patterns. The model essentially memorizes the idiosyncrasies of the particular sample rather than learning the underlying relationship.</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>**Non-parametric methods** take a fundamentally different approach. Instead of assuming a specific functional form for $f$, non-parametric methods seek an estimate that gets close to the data points without imposing strong prior assumptions about the shape of the relationship. The idea is to let the data speak for themselves and to allow $f̂$ to take whatever form best fits the observed patterns. One example of a non-parametric method is the thin-plate spline, which estimates $f$ as a smooth surface that passes near the observed data points. The analyst does not specify in advance that $f$ should be linear or quadratic or any other particular form. Instead, the method finds a smooth function that fits the data well, subject to some constraint on how wiggly or rough the function is allowed to be. Another example is the k-nearest neighbors method, which predicts the outcome for a new observation by averaging the outcomes of the k training observations that are most similar to it in terms of the predictor values.</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>The main advantage of non-parametric methods is their flexibility. Because they do not assume a particular form for $f$, they can potentially capture a much wider range of relationships, including highly nonlinear patterns and complex interactions that would be missed by a simple parametric model. If the true $f$ has an unusual or complicated shape, a non-parametric method has a better chance of approximating it accurately. However, non-parametric methods also have important disadvantages. Because they do not reduce the problem to estimating a small number of parameters, they typically require much larger samples to produce accurate estimates. The flexibility that allows non-parametric methods to fit complex patterns also makes them prone to overfitting, especially when sample sizes are limited. Furthermore, the estimates produced by non-parametric methods are often difficult to interpret. A thin-plate spline or a k-nearest neighbors prediction does not come with coefficients that tell us how each predictor contributes to the outcome. This lack of interpretability makes non-parametric methods less useful for inference, even when they excel at prediction.</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>The choice between parametric and non-parametric methods involves a fundamental trade-off. Parametric methods impose structure on the problem, which makes estimation easier and results more interpretable, but at the cost of potentially misspecifying the true form of $f$. Non-parametric methods avoid this misspecification risk by staying flexible, but they require more data and produce less interpretable results. In practice, the best choice depends on the goals of the analysis, the amount of data available, and how much prior knowledge we have about the likely form of the relationship.</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>This brings us to a closely related issue: the trade-off between prediction accuracy and model interpretability. In statistical learning, there is often an inverse relationship between how flexible a method is and how interpretable its results are. Methods that impose strong restrictions on the form of $f$ tend to be highly interpretable but may not fit complex patterns very well. Methods that are highly flexible can capture intricate relationships but produce results that are difficult for humans to understand.</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>At one end of the spectrum, we have highly restrictive methods like linear regression and its close relatives. Linear regression assumes that $f$ is a linear combination of the predictors, which is a very strong restriction. This restriction means that linear regression can only produce straight lines in one dimension, flat planes in two dimensions, and hyperplanes in higher dimensions. The advantage is that the results are extremely interpretable. Each coefficient has a clear meaning: it tells us the expected change in Y associated with a one-unit increase in the corresponding predictor, holding other predictors constant. We can examine the coefficients and immediately understand which predictors matter, how large their effects are, and in which direction they operate. For inference purposes, this interpretability is invaluable.</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>Moving along the spectrum toward greater flexibility, we encounter methods like generalized additive models, which relax the linearity assumption by allowing each predictor to have a potentially nonlinear effect on the outcome, while still maintaining an additive structure. These models are more flexible than linear regression and can capture curved relationships, but they remain reasonably interpretable because we can plot and examine the estimated effect of each predictor separately. Further along the spectrum, we find decision trees, which partition the predictor space into regions and assign a predicted outcome to each region. Trees are moderately flexible and can capture interactions and nonlinearities, but they remain somewhat interpretable because we can visualize the tree structure and see which predictors are used to make splits and at what values. At the far end of the spectrum, we have highly flexible methods such as bagging, boosting, support vector machines with nonlinear kernels, and deep neural networks. These methods can approximate extremely complex functions and often achieve superior predictive accuracy on difficult problems. However, their results are very hard to interpret. A neural network with thousands of parameters does not yield simple statements about how each predictor influences the outcome. The model operates as a black box: we can feed in predictors and obtain predictions, but we cannot easily understand why the model makes the predictions it does.</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>One might think that we should always prefer the most flexible method available, reasoning that greater flexibility means better ability to capture the true $f$. Surprisingly, this is not the case. More flexible methods are not always better, even when our sole goal is prediction. The reason is overfitting. A highly flexible method can fit the training data very closely, including the random noise in that particular sample. When we apply the model to new data, the noise patterns will be different, and the overfitted model will perform poorly. In many situations, a simpler, more restrictive model that does not fit the training data as closely will actually generalize better to new observations.</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>This phenomenon is especially important when sample sizes are limited. With a small sample, there is not enough information to reliably estimate a complex, flexible model, and the risk of overfitting is high. In such cases, imposing structure through a parametric model can actually improve predictive performance by preventing the model from chasing noise. As sample sizes grow larger, we can afford to use more flexible methods because there is enough information to distinguish genuine patterns from random variation.</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>The choice of method therefore depends on our goals and our circumstances. If our primary goal is inference, we typically prefer more restrictive, interpretable methods like linear regression, even if they sacrifice some predictive accuracy. The interpretability allows us to draw substantive conclusions about how the world works. If our primary goal is prediction and we have ample data, we might prefer more flexible methods that can capture complex patterns, accepting that we will not be able to easily interpret the results. If we want prediction but have limited data, we might still prefer restrictive methods to avoid overfitting.</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>Let me now bring together these various dimensions by summarizing how different statistical learning methods can be characterized according to their suitability for prediction versus inference, their parametric versus non-parametric nature, and their position on the interpretability-flexibility spectrum.</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>**Linear regression** and its extensions such as ridge regression and the lasso are parametric methods that assume a linear relationship between predictors and outcome. They occupy the high-interpretability, low-flexibility end of the spectrum. Each predictor has a coefficient that directly quantifies its contribution to the outcome, making these methods ideal for inference. Their predictive accuracy is good when the true relationship is approximately linear, but they will miss nonlinear patterns. Because they estimate relatively few parameters, they work well with moderate sample sizes and are resistant to overfitting.</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>**Logistic regression** is the classification analog of linear regression. It is a parametric method that models the probability of belonging to a particular class as a function of the predictors. Like linear regression, it is highly interpretable because the coefficients can be transformed into odds ratios that describe how each predictor affects the odds of the outcome. It is well-suited for inference in classification problems but assumes a particular functional form that may not hold in all situations.</span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>**Generalized additive models** are parametric in the sense that they assume an additive structure, but they allow nonlinear effects for individual predictors. They occupy a middle position on the flexibility-interpretability spectrum. The additive structure maintains considerable interpretability because we can examine the effect of each predictor separately, but the nonlinear components provide more flexibility than standard linear regression. These models are useful when we suspect that relationships are nonlinear but still want to understand how each predictor contributes to the outcome.</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>**Decision trees** are non-parametric methods that recursively partition the predictor space. They are moderately flexible and can capture interactions and nonlinearities. Their interpretability is moderate: small trees are easy to understand and visualize, but large, complex trees become difficult to interpret. Trees are prone to overfitting, especially when grown deep, but they provide a foundation for more sophisticated ensemble methods.</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>**Bagging** and **random forests** are ensemble methods that combine many decision trees to improve predictive accuracy and reduce overfitting. They are non-parametric and highly flexible, capable of capturing complex patterns in the data. However, by aggregating many trees, they sacrifice the interpretability of individual trees. These methods are primarily useful for prediction rather than inference, particularly when sample sizes are large enough to support their flexibility.</span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>**Boosting methods** build ensembles of weak learners, typically small decision trees, in a sequential manner that focuses on observations that previous models predicted poorly. Like bagging and random forests, boosting methods are highly flexible and achieve excellent predictive performance on many problems. They are non-parametric and operate toward the low-interpretability end of the spectrum, making them suitable for prediction but less useful for inference.</span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a>**Support vector machines** are methods that find optimal boundaries between classes in classification problems or fit regression functions in a way that ignores errors smaller than some threshold. With linear kernels, they are similar to linear methods and retain interpretability. With nonlinear kernels, they become highly flexible and can capture complex decision boundaries, but they lose interpretability. The flexibility of support vector machines can be tuned by choosing different kernels and adjusting regularization parameters.</span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>**Neural networks** and **deep learning** represent the extreme of flexibility. These methods can approximate virtually any function given enough data and computational resources. They have achieved remarkable success in applications such as image recognition, natural language processing, and game playing. However, their complexity makes them essentially uninterpretable. A deep neural network with millions of parameters cannot be summarized in a way that humans can understand. These methods are purely predictive tools, unsuitable for inference, and they require very large datasets to train effectively without overfitting.</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a>**K-nearest neighbors** is a simple non-parametric method that makes predictions by averaging the outcomes of similar observations in the training data. It is highly flexible and makes no assumptions about the form of f. However, it is not very interpretable because it does not produce coefficients or other summaries of how predictors relate to outcomes. Its predictive accuracy depends strongly on the choice of k and on having a sufficiently large and representative training sample.</span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a>In summary, the landscape of statistical learning methods can be understood through several interconnected dimensions. The parametric versus non-parametric distinction concerns whether a method assumes a specific functional form for $f$ or lets the data determine the shape of the relationship. The interpretability versus flexibility trade-off reflects the tension between methods that produce easily understood results and methods that can capture complex patterns. The prediction versus inference distinction concerns whether our goal is to forecast future outcomes accurately or to understand the mechanisms connecting predictors to responses. These dimensions are interrelated: parametric methods tend to be more interpretable and better suited for inference, while non-parametric methods tend to be more flexible and better suited for prediction when interpretability is not required. The choice among methods depends on the specific goals of an analysis, the nature of the data, the sample size available, and the analyst's prior beliefs about the likely form of the relationship being studied.</span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a>To complete our overview of the foundational concepts in statistical learning, we need to understand additional distinction between supervised and unsupervised learning that help us categorize different types of learning problems.</span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a>**Supervised learning** refers to situations where for each observation in our dataset, we have both predictor measurements and a corresponding outcome measurement. The term "*supervised*" reflects the idea that the learning process is guided or supervised by the known outcomes. We observe what actually happened for each case in our training data, and we use this information to learn the relationship between predictors and outcomes. All the methods we have discussed so far, including linear regression, logistic regression, decision trees, random forests, and neural networks, fall into the supervised learning category when applied to problems where outcomes are observed. The fundamental goal of supervised learning is to build a model that can predict the outcome for new observations based on their predictor values, or to understand how the predictors relate to the outcome. In sociological research, most studies involve supervised learning because we typically have data on both the explanatory variables and the outcome of interest. For example, when we study the relationship between education and income, we observe both variables for the individuals in our sample, which allows us to estimate how education influences earnings.</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>**Unsupervised learning** describes a fundamentally different situation where we observe predictor measurements for each observation but have no corresponding outcome variable. Without an outcome to predict or explain, we cannot fit a regression model or train a classifier. Instead, unsupervised learning seeks to discover patterns, structures, or groupings within the data itself. The most common unsupervised learning task is cluster analysis, which attempts to identify subgroups of observations that are similar to one another. For instance, a sociologist might have survey data containing many variables about people's attitudes, behaviors, and demographic characteristics, but no predefined categorization of people into types. Cluster analysis could reveal that the respondents naturally fall into distinct groups based on their patterns of responses, perhaps identifying clusters that correspond to different lifestyles, political orientations, or consumption patterns. The key feature of unsupervised learning is that there is no "correct answer" to supervise the learning process. We are not trying to predict a known outcome but rather to uncover hidden structure in the data. This makes unsupervised learning more exploratory and somewhat more subjective than supervised learning, since there is no objective criterion like prediction accuracy to evaluate whether we have found the right structure.</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Method <span class="pp">|</span> Unsupervised / Supervised <span class="pp">|</span> Parametric / Non-parametric <span class="pp">|</span> Flexibility <span class="pp">|</span> Interpretability <span class="pp">|</span> Best Suited For <span class="pp">|</span> Regression / Classification <span class="pp">|</span> Sample Size Requirements <span class="pp">|</span></span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a><span class="pp">|--------|---------------------------|-------------|------------------|-----------------|---------------------------|-------------------------|</span></span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Linear Regression <span class="pp">|</span> Supervised <span class="pp">|</span> Parametric <span class="pp">|</span> Low <span class="pp">|</span> High <span class="pp">|</span> Inference <span class="pp">|</span> Regression <span class="pp">|</span> Moderate <span class="pp">|</span></span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Ridge Regression <span class="pp">|</span> Supervised <span class="pp">|</span> Parametric <span class="pp">|</span> Low <span class="pp">|</span> High <span class="pp">|</span> Inference &amp; Prediction <span class="pp">|</span> Regression <span class="pp">|</span> Moderate <span class="pp">|</span></span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Lasso <span class="pp">|</span> Supervised <span class="pp">|</span> Parametric <span class="pp">|</span> Low <span class="pp">|</span> High <span class="pp">|</span> Inference &amp; Prediction <span class="pp">|</span> Regression <span class="pp">|</span> Moderate <span class="pp">|</span></span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Logistic Regression <span class="pp">|</span> Supervised <span class="pp">|</span> Parametric <span class="pp">|</span> Low <span class="pp">|</span> High <span class="pp">|</span> Inference <span class="pp">|</span> Classification <span class="pp">|</span> Moderate <span class="pp">|</span></span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Generalized Additive Models (GAMs) <span class="pp">|</span> Supervised <span class="pp">|</span> Parametric (additive structure) <span class="pp">|</span> Medium <span class="pp">|</span> Medium-High <span class="pp">|</span> Inference &amp; Prediction <span class="pp">|</span> Both <span class="pp">|</span> Moderate to Large <span class="pp">|</span></span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Decision Trees <span class="pp">|</span> Supervised <span class="pp">|</span> Non-parametric <span class="pp">|</span> Medium <span class="pp">|</span> Medium <span class="pp">|</span> Inference &amp; Prediction <span class="pp">|</span> Both <span class="pp">|</span> Moderate <span class="pp">|</span></span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Bagging <span class="pp">|</span> Supervised <span class="pp">|</span> Non-parametric <span class="pp">|</span> High <span class="pp">|</span> Low <span class="pp">|</span> Prediction <span class="pp">|</span> Both <span class="pp">|</span> Large <span class="pp">|</span></span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Random Forests <span class="pp">|</span> Supervised <span class="pp">|</span> Non-parametric <span class="pp">|</span> High <span class="pp">|</span> Low <span class="pp">|</span> Prediction <span class="pp">|</span> Both <span class="pp">|</span> Large <span class="pp">|</span></span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Boosting <span class="pp">|</span> Supervised <span class="pp">|</span> Non-parametric <span class="pp">|</span> High <span class="pp">|</span> Low <span class="pp">|</span> Prediction <span class="pp">|</span> Both <span class="pp">|</span> Large <span class="pp">|</span></span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Support Vector Machines (Linear Kernel) <span class="pp">|</span> Supervised <span class="pp">|</span> Parametric <span class="pp">|</span> Low-Medium <span class="pp">|</span> Medium <span class="pp">|</span> Prediction &amp; Inference <span class="pp">|</span> Both <span class="pp">|</span> Moderate <span class="pp">|</span></span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Support Vector Machines (Nonlinear Kernel) <span class="pp">|</span> Supervised <span class="pp">|</span> Non-parametric <span class="pp">|</span> High <span class="pp">|</span> Low <span class="pp">|</span> Prediction <span class="pp">|</span> Both <span class="pp">|</span> Large <span class="pp">|</span></span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> K-Nearest Neighbors <span class="pp">|</span> Supervised <span class="pp">|</span> Non-parametric <span class="pp">|</span> High <span class="pp">|</span> Low <span class="pp">|</span> Prediction <span class="pp">|</span> Both <span class="pp">|</span> Large <span class="pp">|</span></span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Neural Networks / Deep Learning <span class="pp">|</span> Supervised <span class="pp">|</span> Non-parametric <span class="pp">|</span> Very High <span class="pp">|</span> Very Low <span class="pp">|</span> Prediction <span class="pp">|</span> Both <span class="pp">|</span> Very Large <span class="pp">|</span></span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> K-Means Clustering <span class="pp">|</span> Unsupervised <span class="pp">|</span> Non-parametric <span class="pp">|</span> Medium <span class="pp">|</span> Medium <span class="pp">|</span> Discovering groups in data <span class="pp">|</span> Moderate to Large <span class="pp">|</span></span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Hierarchical Clustering <span class="pp">|</span> Unsupervised <span class="pp">|</span> Non-parametric <span class="pp">|</span> Medium <span class="pp">|</span> Medium-High <span class="pp">|</span> Discovering nested group structures <span class="pp">|</span> Moderate <span class="pp">|</span></span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Principal Component Analysis (PCA) <span class="pp">|</span> Unsupervised <span class="pp">|</span> Parametric <span class="pp">|</span> Low <span class="pp">|</span> Medium-High <span class="pp">|</span> Dimensionality reduction <span class="pp">|</span> Moderate <span class="pp">|</span></span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Factor Analysis <span class="pp">|</span> Unsupervised <span class="pp">|</span> Parametric <span class="pp">|</span> Low <span class="pp">|</span> High <span class="pp">|</span> Identifying latent constructs <span class="pp">|</span> Moderate <span class="pp">|</span></span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a>: **Table 1.1** Summary table of statistical learning methods</span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>