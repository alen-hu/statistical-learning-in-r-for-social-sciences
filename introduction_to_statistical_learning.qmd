---
title: "Introduction to Statistical Learning"
description: ""
number-sections: true
title-block-banner: "#00868B"
title-block-banner-color: "white"
---

**Statistical learning** refers to a broad set of approaches and techniques for estimating the function that connects independent variables to an dependent variable. At its core, statistical learning is concerned with understanding the relationship between variables and using that understanding either to make predictions about future observations or to gain insight into how different factors influence an outcome of interest.

## Statistical Learning Formula

The fundamental idea of statistical learning can be expressed through a simple formula:

$$
Y = f(X) + ϵ
$$

This formula tells us that any outcome we wish to study or predict can be understood as the result of some systematic relationship between independent and dependent variables, plus some random variation that we cannot fully explain or control. The goal of statistical learning is to estimate the function $f$ based on observed data, so that we can either predict Y for new observations or understand how changes in X are associated with changes in Y.

Let me now explain each component of this formula in detail.

- The **dependent variable** or response, denoted by Y, represents the response that we are trying to understand, explain, or predict. This is the variable whose variation we want to account for using other available information. It is called "*dependent*" precisely because its values depend on, or are influenced by, other variables in the system we are studying.

- The **independent variable** or predictor, denoted by X, represents the input information that we use to explain or predict the outcome Y. In most realistic situations, we have multiple predictors rather than just one, so X typically represents a collection of variables written as $X = (X_1, X_2, ..., X_p)$, where $_p$ indicates the total number of predictors available. The key characteristic of these variables is that they provide information that helps us understand or anticipate the values of the dependent variable.

- The function $f$ represents the **systematic relationship between the dependent variable and the indipendent variable**. This function captures all the information that the independent variables collectively provide about the dependent variable. In other words, $f$ describes the pattern or rule that connects predictors to response in a consistent, reproducible way. The crucial point is that in real-world applications, the true form of $f$ is almost always unknown to us. We never directly observe this function; instead, we must estimate it based on the data we have collected. The entire enterprise of statistical learning revolves around developing methods to estimate $f$ as accurately as possible, given the constraints of our data and our analytical goals.

- The **error term**, denoted by $ϵ$, represents the random component of the relationship between dependent and independent variables. This term captures all the variation in Y that cannot be explained by the predictors X. The error term is assumed to be independent of X and to have a mean of zero, which means that on average, the errors cancel out and do not systematically bias our predictions in one direction or another. The error term exists for several important reasons.

  - First, there may be variables that influence Y but that we have not measured or included in our analysis.
  
  - Second, even if we could measure every relevant variable, there might be inherent randomness or unpredictability in the phenomenon we are studying.
  
  - Third, our measurements themselves may contain some degree of imprecision or noise.

To make these concepts concrete, let me illustrate them with the example drawn from sociological research. Consider a sociologist studying income inequality and social mobility. The researcher might want to understand what determines a person's income in adulthood. The dependent variable Y would be adult income, measured in monetary units. The predictors X might encompass the person's own educational credentials, their occupation, the region where they live, their parents' socioeconomic status, their race and gender, and the number of years of work experience they have accumulated. The function $f$ would capture the systematic relationships between these characteristics and income, revealing how the labor market rewards different attributes and how social background continues to influence economic outcomes across generations. The error term $ϵ$ would account for all the variation in income that these measured factors cannot explain. This residual variation might stem from unmeasured differences in job performance, luck in finding particularly good or bad employment matches, health shocks that affect earning capacity, or discrimination that varies in ways not captured by the measured variables.

We can write this relationship as:

$$
Y\ =\ f(X_1,\ X_2,\ X_3,\ X_4,\ X_5,\ X_6,\ X_7)\ +\ ϵ
$$

In this formula, Y represents adult income measured in monetary units such as annual earnings in euros. This is the response we are trying to understand or predict. The predictors are defined as follows.

- $X_1$ represents the person's educational credentials, which might be measured as years of schooling completed or as the highest degree obtained.

- $X_2$ represents occupation, which could be coded as occupational prestige scores or as categorical indicators for different types of jobs.

- $X_3$ represents the geographic region where the person lives and works, capturing spatial variation in labor markets and cost of living.

- $X_4$ represents parents' socioeconomic status, which might be measured through parental income, parental education, or a composite index combining multiple indicators of family background.

- $X_5$ represents race, coded as categorical indicators for different racial or ethnic groups.

- $X_6$ represents gender, typically coded as a binary or categorical variable.

- $X_7$ represents years of work experience, measuring how long the person has been participating in the labor force.

The function $f$ captures the systematic relationship between all these predictors and adult income. This function describes how the labor market values different combinations of education, occupation, location, background, and demographic characteristics. The precise form of $f$ is unknown to us and must be estimated from data. It might be relatively simple, such as a linear combination of the predictors, or it might be quite complex, involving interactions between variables and nonlinear relationships.

The error term $ϵ$ represents all the variation in adult income that cannot be explained by the seven predictors we have included. This encompasses unmeasured factors such as individual differences in productivity, motivation, and interpersonal skills, as well as random events like fortunate or unfortunate timing in job searches, health events that affect earning capacity, and idiosyncratic experiences of discrimination or favoritism in the workplace.

## Relationship between Independent and Dependent Variable

### $f$ thing

The function $f$ is the central object of interest in statistical learning. It represents the systematic relationship between the independent variable X and the dependent variable Y, capturing all the information that the independent variables provide about the dependent variable. When we say that $Y = f(X) + ϵ$, we are asserting that the response Y can be decomposed into two parts: a predictable component $f(X)$ that depends on the values of the predictors, and an unpredictable component $ϵ$ that represents random variation. The function $f$ is what connects the world of responses to the world of predictors in a consistent, reproducible manner.

Understanding the nature of $f$ is crucial because it embodies the underlying pattern that governs how changes in the independent variable translate into changes in the dependent variable. If we knew $f$ perfectly, we would understand exactly how each predictor influences the response, how predictors interact with one another, and what response to expect for any given combination of predictor values. However, in virtually all real-world applications, $f$ is unknown. We never observe $f$ directly; we only observe data points consisting of predictor values and corresponding responses. The entire purpose of statistical learning is to use these observed data points to construct an estimate of $f$, which we denote as $\hat{f}$. This estimate allows us to either make predictions about future outcomes or draw inferences about the relationships between variables.

The reasons we might want to estimate $f$ fall into two broad categories: prediction and inference. These two goals are conceptually distinct, and they often lead us to prefer different types of statistical learning methods.

**Prediction** is concerned with accurately anticipating the value of Y for new observations where we know the predictors X but do not yet know the response of the predictors. In prediction tasks, we treat $f̂$ as a kind of black box. We do not necessarily care about the internal workings of our estimated function or about which specific predictors matter most. What we care about is whether our estimate $f̂$ produces accurate predictions when applied to new data. The quality of our predictions depends on two sources of error. The first is **reducible error**, which arises because our estimate $f̂$ is imperfect and does not exactly match the true $f$. We can potentially reduce this error by using better statistical learning methods or by collecting more data. The second is **irreducible error**, which corresponds to the variance of $ϵ$. Even if we had a perfect estimate of $f$, our predictions would still contain some error because Y is inherently influenced by random factors that cannot be predicted from X alone.

To illustrate prediction using our income example, imagine that a government agency wants to identify individuals who are at risk of falling into poverty so that it can target social assistance programs more effectively. The agency has access to administrative data containing information about people's education, occupation, geographic location, family background, race, gender, and work experience. The goal is to predict each person's income based on these characteristics. In this context, the agency does not need to understand precisely why certain combinations of predictors lead to low income. What matters is that the predictive model accurately identifies individuals whose incomes are likely to fall below some threshold. The function $f̂$ serves as a tool for sorting people into risk categories, and its value is judged entirely by how well it predicts actual incomes.

**Inference**, by contrast, is concerned with understanding the relationship between the predictors and the outcome. When our goal is inference, we cannot treat $f̂$ as a black box because we need to know its exact form. We want to answer questions such as which predictors are associated with the response, what is the direction and magnitude of each predictor's effect, and whether the relationships are linear or more complex. Inference requires that our estimate $f̂$ be interpretable, meaning that we can examine it and draw substantive conclusions about how the world works.

Returning to the income example, suppose a sociologist wants to understand the mechanisms of income inequality. The researcher might ask questions such as: *How much does an additional year of education increase expected income? Does the effect of education differ by race or gender? How large is the income penalty associated with being female, after controlling for education, occupation, and experience?* These are inferential questions because they seek to illuminate the structure of $f$ itself, not merely to use $f$ for prediction. Answering these questions requires a model that allows the researcher to isolate the contribution of each predictor and to interpret coefficients or other parameters in substantively meaningful ways. A model that predicts income very accurately but does not reveal anything about how individual predictors matter would be useless for this purpose.

In practice, many research projects involve elements of both prediction and inference. A sociologist studying income might want to understand the determinants of earnings while also developing a model that can predict incomes for new individuals. However, there is often tension between these goals because the methods that produce the most accurate predictions are not always the most interpretable, and the most interpretable methods do not always produce the best predictions.

#### Parametric vs non-parametric methods

Having established why we want to estimate $f$, let me now turn to the question of how we estimate $f$. Statistical learning methods for estimating $f$ can be broadly divided into two categories: parametric methods and non-parametric methods. These two approaches differ fundamentally in the assumptions they make about the form of $f$ and in the way they use data to construct an estimate.

**Parametric methods** proceed in two steps. In the first step, we make an assumption about the functional form of $f$. That is, we specify in advance what kind of mathematical relationship we believe connects the predictors to the outcome. The most common assumption is that $f$ is linear, meaning that we assume the relationship can be written as $f(X) = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p$. This linear model asserts that the response is a weighted sum of the predictors, where the weights $\beta_1, \beta_2, ..., \beta_p$ are unknown coefficients that quantify the contribution of each predictor, and $\beta_0$ is an intercept term representing the expected response when all predictors equal zero. By assuming a linear form, we have dramatically simplified the problem. Instead of having to estimate an arbitrary, potentially very complex function $f$, we only need to estimate the intercept and the p coefficients. In the second step of the parametric approach, we use the observed data to fit or train the model. This means finding values of the parameters that make the model match the data as closely as possible. For the linear model, the most common fitting procedure is ordinary least squares, which chooses the parameter values that minimize the sum of squared differences between the observed responses and the responses predicted by the model. Once we have estimated the parameters, our estimate $f̂$ is fully specified, and we can use it for prediction or inference.

The parametric approach has several important advantages. Because we have reduced the problem to estimating a fixed number of parameters, parametric methods are computationally efficient and can work well even with relatively small samples. Furthermore, the resulting models are typically easy to interpret. In a linear model, each coefficient tells us how much the expected responses changes when the corresponding predictor increases by one unit, holding all other predictors constant. This interpretability makes parametric models particularly valuable for inference. However, parametric methods also have a significant limitation. The assumption we make about the form of $f$ may be wrong. If the true relationship between the predictors and the response is nonlinear or involves complex interactions, a linear model will fail to capture these features and will provide a poor approximation to $f$. We can try to address this problem by using more flexible parametric models that include polynomial terms, interaction effects, or other elaborations of the basic linear form. But as we make our parametric model more flexible, we must estimate more parameters, which requires more data and increases the risk of a phenomenon called overfitting. **Overfitting** occurs when a model fits the training data very well but performs poorly on new data because it has captured random noise rather than genuine patterns. The model essentially memorizes the idiosyncrasies of the particular sample rather than learning the underlying relationship.

**Non-parametric methods** take a fundamentally different approach. Instead of assuming a specific functional form for $f$, non-parametric methods seek an estimate that gets close to the data points without imposing strong prior assumptions about the shape of the relationship. The idea is to let the data speak for themselves and to allow $\hat{f}$ to take whatever form best fits the observed patterns. One example of a non-parametric method is the thin-plate spline, which estimates $f$ as a smooth surface that passes near the observed data points. The analyst does not specify in advance that $f$ should be linear or quadratic or any other particular form. Instead, the method finds a smooth function that fits the data well, subject to some constraint on how wiggly or rough the function is allowed to be. Another example is the k-nearest neighbors method, which predicts the outcome for a new observation by averaging the outcomes of the k training observations that are most similar to it in terms of the predictor values.

The main advantage of non-parametric methods is their flexibility. Because they do not assume a particular form for $f$, they can potentially capture a much wider range of relationships, including highly nonlinear patterns and complex interactions that would be missed by a simple parametric model. If the true $f$ has an unusual or complicated shape, a non-parametric method has a better chance of approximating it accurately. However, non-parametric methods also have important disadvantages. Because they do not reduce the problem to estimating a small number of parameters, they typically require much larger samples to produce accurate estimates. The flexibility that allows non-parametric methods to fit complex patterns also makes them prone to overfitting, especially when sample sizes are limited. Furthermore, the estimates produced by non-parametric methods are often difficult to interpret. A thin-plate spline or a k-nearest neighbors prediction does not come with coefficients that tell us how each predictor contributes to the outcome. This lack of interpretability makes non-parametric methods less useful for inference, even when they excel at prediction.

The choice between parametric and non-parametric methods involves a fundamental trade-off. Parametric methods impose structure on the problem, which makes estimation easier and results more interpretable, but at the cost of potentially misspecifying the true form of $f$. Non-parametric methods avoid this misspecification risk by staying flexible, but they require more data and produce less interpretable results. In practice, the best choice depends on the goals of the analysis, the amount of data available, and how much prior knowledge we have about the likely form of the relationship.

This brings us to a closely related issue: the trade-off between prediction accuracy and model interpretability. In statistical learning, there is often an inverse relationship between how flexible a method is and how interpretable its results are. Methods that impose strong restrictions on the form of $f$ tend to be highly interpretable but may not fit complex patterns very well. Methods that are highly flexible can capture intricate relationships but produce results that are difficult for humans to understand.

At one end of the spectrum, we have highly restrictive methods like linear regression and its close relatives. Linear regression assumes that $f$ is a linear combination of the predictors, which is a very strong restriction. This restriction means that linear regression can only produce straight lines in one dimension, flat planes in two dimensions, and hyperplanes in higher dimensions. The advantage is that the results are extremely interpretable. Each coefficient has a clear meaning: it tells us the expected change in Y associated with a one-unit increase in the corresponding predictor, holding other predictors constant. We can examine the coefficients and immediately understand which predictors matter, how large their effects are, and in which direction they operate. For inference purposes, this interpretability is invaluable.

Moving along the spectrum toward greater flexibility, we encounter methods like generalized additive models, which relax the linearity assumption by allowing each predictor to have a potentially nonlinear effect on the response, while still maintaining an additive structure. These models are more flexible than linear regression and can capture curved relationships, but they remain reasonably interpretable because we can plot and examine the estimated effect of each predictor separately. Further along the spectrum, we find decision trees, which partition the predictor space into regions and assign a predicted response to each region. Trees are moderately flexible and can capture interactions and nonlinearities, but they remain somewhat interpretable because we can visualize the tree structure and see which predictors are used to make splits and at what values. These methods can approximate extremely complex functions and often achieve superior predictive accuracy on difficult problems. However, their results are very hard to interpret. A neural network with thousands of parameters does not yield simple statements about how each predictor influences the response. The model operates as a black box: we can feed in predictors and obtain predictions, but we cannot easily understand why the model makes the predictions it does.

One might think that we should always prefer the most flexible method available, reasoning that greater flexibility means better ability to capture the true $f$. Surprisingly, this is not the case. More flexible methods are not always better, even when our sole goal is prediction. The reason is overfitting. A highly flexible method can fit the training data very closely, including the random noise in that particular sample. When we apply the model to new data, the noise patterns will be different, and the overfitted model will perform poorly. In many situations, a simpler, more restrictive model that does not fit the training data as closely will actually generalize better to new observations.

This phenomenon is especially important when sample sizes are limited. With a small sample, there is not enough information to reliably estimate a complex, flexible model, and the risk of overfitting is high. In such cases, imposing structure through a parametric model can actually improve predictive performance by preventing the model from chasing noise. As sample sizes grow larger, we can afford to use more flexible methods because there is enough information to distinguish genuine patterns from random variation.

The choice of method therefore depends on our goals and our circumstances. If our primary goal is inference, we typically prefer more restrictive, interpretable methods like linear regression, even if they sacrifice some predictive accuracy. If our primary goal is prediction and we have ample data, we might prefer more flexible methods that can capture complex patterns, accepting that we will not be able to easily interpret the results. If we want prediction but have limited data, we might still prefer restrictive methods to avoid overfitting.

#### Supervised vs unsupervised learning

To complete our overview of the foundational concepts in statistical learning, we need to understand additional distinction between supervised and unsupervised learning that help us categorize different types of learning problems.

**Supervised learning** refers to situations where for each observation in our dataset, we have both predictor measurements and a corresponding response measurement. The term "*supervised*" reflects the idea that the learning process is guided or supervised by the known dependent variables. We observe what actually happened for each case in our training data, and we use this information to learn the relationship between independent variables and dependent variable. All the methods we have discussed so far, including linear regression, logistic regression, decision trees, random forests, and neural networks, fall into the supervised learning category when applied to problems where outcomes are observed. The fundamental goal of supervised learning is to build a model that can predict the response for new observations based on their predictor values, or to understand how the predictors relate to the response. In sociological research, most studies involve supervised learning because we typically have data on both the explanatory variables and the outcome of interest. For example, when we study the relationship between education and income, we observe both variables for the individuals in our sample, which allows us to estimate how education influences earnings.

**Unsupervised learning** describes a fundamentally different situation where we observe predictor measurements for each observation but have no corresponding response variable. Without a dependent variable to predict or explain, we cannot fit a regression model or train a classifier. Instead, unsupervised learning seeks to discover patterns, structures, or groupings within the data itself. The most common unsupervised learning task is cluster analysis, which attempts to identify subgroups of observations that are similar to one another. For instance, a sociologist might have survey data containing many variables about people's attitudes, behaviors, and demographic characteristics, but no predefined categorization of people into types. Cluster analysis could reveal that the respondents naturally fall into distinct groups based on their patterns of responses, perhaps identifying clusters that correspond to different lifestyles, political orientations, or consumption patterns. The key feature of unsupervised learning is that there is no "correct answer" to supervise the learning process. We are not trying to predict a known outcome but rather to uncover hidden structure in the data. This makes unsupervised learning more exploratory and somewhat more subjective than supervised learning, since there is no objective criterion like prediction accuracy to evaluate whether we have found the right structure.

| Method | Unsupervised / Supervised | Parametric / Non-parametric | Flexibility | Interpretability | Best Suited For |
|--------------|---------------------|-------------------|------------------|-----------------|
| Linear Regression | Supervised | Parametric | Low | High | Inference |
| Ridge Regression | Supervised | Parametric | Low | High | Inference & Prediction |
| Lasso | Supervised | Parametric | Low | High | Inference & Prediction |
| Logistic Regression | Supervised | Parametric | Low | High | Inference |
| Generalized Additive Models (GAMs) | Supervised | Parametric (additive structure) | Medium | Medium-High | Inference & Prediction |
| Decision Trees | Supervised | Non-parametric | Medium | Medium | Inference & Prediction |
| Bagging | Supervised | Non-parametric | High | Low | Prediction |
| Random Forests | Supervised | Non-parametric | High | Low | Prediction |
| Boosting | Supervised | Non-parametric | High | Low | Prediction |
| Support Vector Machines (Linear Kernel) | Supervised | Parametric | Low-Medium | Medium | Prediction & Inference |
| Support Vector Machines (Nonlinear Kernel) | Supervised | Non-parametric | High | Low | Prediction |
| K-Nearest Neighbors | Supervised | Non-parametric | High | Low | Prediction |
| Neural Networks / Deep Learning | Supervised | Non-parametric | Very High | Very Low | Prediction |
| K-Means Clustering | Unsupervised | Non-parametric | Medium | Medium | Discovering groups in data |
| Hierarchical Clustering | Unsupervised | Non-parametric | Medium | Medium-High | Discovering nested group structures |
| Principal Component Analysis (PCA) | Unsupervised | Parametric | Low | Medium-High | Dimensionality reduction |
| Factor Analysis | Unsupervised | Parametric | Low | High | Identifying latent constructs |

: **Table 1.1** Summary table of statistical learning methods

## Assessing Model Accuracy

$$
MSE = \frac{1}{n} \times \sum_{i=1}^n(y_i - \hat{f}(x_i))^2
$$
